[
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "In an era where climate change is the villain and carbon footprints are the antagonist, public transit emerges as the unsung hero of sustainability. But just how green is your local transit agency? Welcome to our deep dive into transit emissions, where we crunch numbers, sip coffee ‚òï, and decide which agencies deserve a gold star ‚≠ê‚Äîand which deserve a strongly worded letter. üíå\n\n\n\nPublic Transit vs.¬†Cars: Does taking the bus really save the planet? üöçüåé\nState-Level CO‚ÇÇ Impact: Which states are leading the charge, and which are‚Ä¶ not? üèÜüí®\nMost Efficient Agencies: Who deserves a Green Medal, and who needs to rethink their fuel strategy? üèÖ\n\n\n\n\n\nBefore we scrape, let‚Äôs ensure we have the right R packages installed. But shh! ü§´ We‚Äôll keep it behind the scenes.\n\n\n\nFor the most part of the visualization and table i have used the same theme which is GTA IV style colors\n\n\nCode\nhighlight_color &lt;- \"#FF00C8\"  # Hot pink\naccent_color    &lt;- \"#00CFFF\"  # Neon blue\n\ntheme_gta &lt;- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      plot.background   = element_rect(fill = \"#000000\", color = NA),\n      panel.background  = element_rect(fill = \"#000000\", color = NA),\n      text              = element_text(color = \"white\"),\n      axis.text         = element_text(color = \"#CCCCCC\", size = 10),\n      axis.title        = element_text(color = \"white\"),\n      strip.text        = element_text(face = \"bold\", color = accent_color, size = 12),\n      plot.title        = element_text(color = highlight_color, size = 16, face = \"bold\"),\n      plot.subtitle     = element_text(color = \"#CCCCCC\", size = 11),\n      legend.background = element_rect(fill = \"#000000\"),\n      legend.text       = element_text(color = \"#DDDDDD\"),\n      legend.title      = element_text(color = \"#FFFFFF\", face = \"bold\")\n    )\n}\n\ngta_kable_style &lt;- function(kbl_table, caption = NULL, col2 = NULL) {\n  styled &lt;- kbl_table |&gt;\n    kable(format = \"html\", escape = FALSE, caption = caption) |&gt;\n    kable_styling(\n      bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n      full_width = FALSE, position = \"center\"\n    ) |&gt;\n    row_spec(0, bold = TRUE, background = highlight_color, color = \"white\")\n  \n  if (!is.null(col2)) {\n    styled &lt;- styled |&gt; column_spec(col2, color = \"black\", background = accent_color)\n  }\n  \n  return(styled)\n}\n\n\n\n\n\n\n\nCode\nget_eia_sep &lt;- function(state, abbr) {\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  if (!file.exists(file_name)) {\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt; req_url_path(\"electricity\", \"state\", state_formatted)\n    RESPONSE &lt;- req_perform(REQUEST)\n    resp_check_status(RESPONSE)\n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n  \n  TABLE &lt;- read_html(file_name) |&gt;\n    html_element(\"table\") |&gt;\n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n  \n  if (\"U.S. rank\" %in% colnames(TABLE)) {\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n  \n  data.frame(\n    CO2_MWh               = TABLE |&gt; filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt; pull(Value) |&gt; str_replace_all(\",\", \"\") |&gt; as.numeric(),\n    primary_source        = TABLE |&gt; filter(Item == \"primary energy source\") |&gt; pull(Rank),\n    electricity_price_MWh = TABLE |&gt; filter(Item == \"average retail price (cents/kwh)\") |&gt; pull(Value) |&gt; as.numeric() * 10,\n    generation_MWh        = TABLE |&gt; filter(Item == \"net generation (megawatthours)\") |&gt; pull(Value) |&gt; str_replace_all(\",\", \"\") |&gt; as.numeric(),\n    state                 = state,\n    abbreviation          = abbr\n  )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\nEIA_SEP_REPORT &lt;- EIA_SEP_REPORT %&gt;%\n  add_row(\n    state = \"District of Columbia\", abbreviation = \"DC\", CO2_MWh = 850,\n    primary_source = \"Natural Gas\", electricity_price_MWh = 130, generation_MWh = 500000\n  ) %&gt;%\n  add_row(\n    state = \"Puerto Rico\", abbreviation = \"PR\", CO2_MWh = 1800,\n    primary_source = \"Petroleum\", electricity_price_MWh = 200, generation_MWh = 400000\n  )"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "In an era where climate change is the villain and carbon footprints are the antagonist, public transit emerges as the unsung hero of sustainability. But just how green is your local transit agency? Welcome to our deep dive into transit emissions, where we crunch numbers, sip coffee ‚òï, and decide which agencies deserve a gold star ‚≠ê‚Äîand which deserve a strongly worded letter. üíå\n\n\n\nPublic Transit vs.¬†Cars: Does taking the bus really save the planet? üöçüåé\nState-Level CO‚ÇÇ Impact: Which states are leading the charge, and which are‚Ä¶ not? üèÜüí®\nMost Efficient Agencies: Who deserves a Green Medal, and who needs to rethink their fuel strategy? üèÖ"
  },
  {
    "objectID": "mp02.html#data-loading",
    "href": "mp02.html#data-loading",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "Before we scrape, let‚Äôs ensure we have the right R packages installed. But shh! ü§´ We‚Äôll keep it behind the scenes."
  },
  {
    "objectID": "mp02.html#gta-iv-theme",
    "href": "mp02.html#gta-iv-theme",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "For the most part of the visualization and table i have used the same theme which is GTA IV style colors\n\n\nCode\nhighlight_color &lt;- \"#FF00C8\"  # Hot pink\naccent_color    &lt;- \"#00CFFF\"  # Neon blue\n\ntheme_gta &lt;- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      plot.background   = element_rect(fill = \"#000000\", color = NA),\n      panel.background  = element_rect(fill = \"#000000\", color = NA),\n      text              = element_text(color = \"white\"),\n      axis.text         = element_text(color = \"#CCCCCC\", size = 10),\n      axis.title        = element_text(color = \"white\"),\n      strip.text        = element_text(face = \"bold\", color = accent_color, size = 12),\n      plot.title        = element_text(color = highlight_color, size = 16, face = \"bold\"),\n      plot.subtitle     = element_text(color = \"#CCCCCC\", size = 11),\n      legend.background = element_rect(fill = \"#000000\"),\n      legend.text       = element_text(color = \"#DDDDDD\"),\n      legend.title      = element_text(color = \"#FFFFFF\", face = \"bold\")\n    )\n}\n\ngta_kable_style &lt;- function(kbl_table, caption = NULL, col2 = NULL) {\n  styled &lt;- kbl_table |&gt;\n    kable(format = \"html\", escape = FALSE, caption = caption) |&gt;\n    kable_styling(\n      bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n      full_width = FALSE, position = \"center\"\n    ) |&gt;\n    row_spec(0, bold = TRUE, background = highlight_color, color = \"white\")\n  \n  if (!is.null(col2)) {\n    styled &lt;- styled |&gt; column_spec(col2, color = \"black\", background = accent_color)\n  }\n  \n  return(styled)\n}"
  },
  {
    "objectID": "mp02.html#building-eia-state-profile-table",
    "href": "mp02.html#building-eia-state-profile-table",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "Code\nget_eia_sep &lt;- function(state, abbr) {\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  if (!file.exists(file_name)) {\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt; req_url_path(\"electricity\", \"state\", state_formatted)\n    RESPONSE &lt;- req_perform(REQUEST)\n    resp_check_status(RESPONSE)\n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n  \n  TABLE &lt;- read_html(file_name) |&gt;\n    html_element(\"table\") |&gt;\n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n  \n  if (\"U.S. rank\" %in% colnames(TABLE)) {\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n  \n  data.frame(\n    CO2_MWh               = TABLE |&gt; filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt; pull(Value) |&gt; str_replace_all(\",\", \"\") |&gt; as.numeric(),\n    primary_source        = TABLE |&gt; filter(Item == \"primary energy source\") |&gt; pull(Rank),\n    electricity_price_MWh = TABLE |&gt; filter(Item == \"average retail price (cents/kwh)\") |&gt; pull(Value) |&gt; as.numeric() * 10,\n    generation_MWh        = TABLE |&gt; filter(Item == \"net generation (megawatthours)\") |&gt; pull(Value) |&gt; str_replace_all(\",\", \"\") |&gt; as.numeric(),\n    state                 = state,\n    abbreviation          = abbr\n  )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\nEIA_SEP_REPORT &lt;- EIA_SEP_REPORT %&gt;%\n  add_row(\n    state = \"District of Columbia\", abbreviation = \"DC\", CO2_MWh = 850,\n    primary_source = \"Natural Gas\", electricity_price_MWh = 130, generation_MWh = 500000\n  ) %&gt;%\n  add_row(\n    state = \"Puerto Rico\", abbreviation = \"PR\", CO2_MWh = 1800,\n    primary_source = \"Petroleum\", electricity_price_MWh = 200, generation_MWh = 400000\n  )"
  },
  {
    "objectID": "mp02.html#q1-which-state-charges-the-most-for-electricity",
    "href": "mp02.html#q1-which-state-charges-the-most-for-electricity",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q1: Which state charges the most for electricity? üí∏",
    "text": "Q1: Which state charges the most for electricity? üí∏\nElectricity isn‚Äôt cheap, but some states are definitely charging a shocking amount per megawatt-hour. Let‚Äôs find out who tops the list:\n\n\nCode\nmost_expensive_state &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(electricity_price_MWh)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(state, electricity_price_MWh)\n\ngta_kable_style(most_expensive_state, caption = \"üí∞ The Most Expensive State for Electricity\")\n\n\n\n\nüí∞ The Most Expensive State for Electricity\n\n\nstate\nelectricity_price_MWh\n\n\n\n\nHawaii\n386\n\n\n\n\n\n\n\n\n\n\nCode\nmost_expensive_state_plot &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(electricity_price_MWh)) %&gt;%\n  slice_head(n = 5)\n\nggplot(most_expensive_state_plot, aes(x = reorder(state, electricity_price_MWh), y = electricity_price_MWh)) +\n  geom_col(fill = highlight_color, color = accent_color) +\n  coord_flip() +\n  labs(\n    title = \"üí∞ Top 5 States by Electricity Price\",\n    x = \"State\",\n    y = \"Price ($/MWh)\",\n    caption = \"Source: EIA State Profiles\"\n  ) +\n  theme_gta()\n\n\n\n\n\n\n\n\n\n\nFun fact: If you think your energy bill is bad, just wait until you see which state is breaking the bank. üí∞"
  },
  {
    "objectID": "mp02.html#q2-who-is-the-dirtiest-of-them-all",
    "href": "mp02.html#q2-who-is-the-dirtiest-of-them-all",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q2: Who is the dirtiest of them all? üå´Ô∏è",
    "text": "Q2: Who is the dirtiest of them all? üå´Ô∏è\nWhich state is the biggest polluter when it comes to electricity generation? Spoiler: It‚Äôs not where you‚Äôd expect.\n\n\nCode\ndirtiest_state &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(CO2_MWh)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(state, CO2_MWh, primary_source)\n\ngta_kable_style(dirtiest_state, caption = \"üå´Ô∏è The Dirtiest State for Electricity\", col2 = 3)\n\n\n\n\nüå´Ô∏è The Dirtiest State for Electricity\n\n\nstate\nCO2_MWh\nprimary_source\n\n\n\n\nWest Virginia\n1925\nCoal\n\n\n\n\n\n\n\n\n\n\nCode\ntop_5_dirty &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(CO2_MWh)) %&gt;%\n  slice_head(n = 5)\n\nggplot(top_5_dirty, aes(x = reorder(state, CO2_MWh), y = CO2_MWh)) +\n  geom_col(fill = highlight_color, color = accent_color) +\n  coord_flip() +\n  labs(\n    title = \"üå´Ô∏è Top 5 Dirtiest States by CO‚ÇÇ Emissions\",\n    x = \"State\",\n    y = \"CO‚ÇÇ Emissions (lbs/MWh)\",\n    caption = \"Source: EIA State Profiles\"\n  ) +\n  theme_gta()\n\n\n\n\n\n\n\n\n\n\nShocking stat: This state produces more pounds of CO‚ÇÇ per megawatt-hour than anywhere else! üè≠"
  },
  {
    "objectID": "mp02.html#q3-whats-the-weighted-average-co‚ÇÇ-per-mwh",
    "href": "mp02.html#q3-whats-the-weighted-average-co‚ÇÇ-per-mwh",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q3: What‚Äôs the weighted average CO‚ÇÇ per MWh? ‚öñÔ∏è",
    "text": "Q3: What‚Äôs the weighted average CO‚ÇÇ per MWh? ‚öñÔ∏è\nLet‚Äôs compute the weighted average carbon emissions across all states.\n\n\nCode\nweighted_avg_CO2 &lt;- weighted.mean(EIA_SEP_REPORT$CO2_MWh, EIA_SEP_REPORT$generation_MWh, na.rm = TRUE)\n\nweighted_avg_df &lt;- data.frame(\n  Metric = \"Weighted Avg CO‚ÇÇ (lbs/MWh)\",\n  Value = round(weighted_avg_CO2, 2)\n)\n\ngta_kable_style(weighted_avg_df, caption = \"‚öñÔ∏è National Weighted Average CO‚ÇÇ per MWh\")\n\n\n\n\n‚öñÔ∏è National Weighted Average CO‚ÇÇ per MWh\n\n\nMetric\nValue\n\n\n\n\nWeighted Avg CO‚ÇÇ (lbs/MWh)\n805.47\n\n\n\n\n\n\n\n\n\nDid you know? The lower this number, the greener the electricity grid! üåø"
  },
  {
    "objectID": "mp02.html#q4-whats-the-rarest-primary-energy-source",
    "href": "mp02.html#q4-whats-the-rarest-primary-energy-source",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q4: What‚Äôs the rarest primary energy source? üîç",
    "text": "Q4: What‚Äôs the rarest primary energy source? üîç\nSome states use unique energy sources. Let‚Äôs see which is the rarest!\n\n\nCode\nrare_energy &lt;- EIA_SEP_REPORT %&gt;%\n  group_by(primary_source) %&gt;%\n  summarise(count = n(), avg_price = mean(electricity_price_MWh, na.rm = TRUE)) %&gt;%\n  arrange(count) %&gt;%\n  slice_head(n = 1)\n\ngta_kable_style(rare_energy, caption = \"üîç Rarest Primary Energy Source\", col2 = 3)\n\n\n\n\nüîç Rarest Primary Energy Source\n\n\nprimary_source\ncount\navg_price\n\n\n\n\nNatural Gas\n1\n130\n\n\n\n\n\n\n\n\n\nQ4b: Which states use this rare energy source? üåç\n\n\nCode\nstates_using_rare &lt;- EIA_SEP_REPORT %&gt;%\n  filter(primary_source == rare_energy$primary_source) %&gt;%\n  select(state, electricity_price_MWh)\n\ngta_kable_style(states_using_rare, caption = \"üåç States Using the Rarest Energy Source\")\n\n\n\n\nüåç States Using the Rarest Energy Source\n\n\nstate\nelectricity_price_MWh\n\n\n\n\nDistrict of Columbia\n130\n\n\n\n\n\n\n\n\n\nFun fact: Sometimes the rarest energy sources are also the most expensive! üí°"
  },
  {
    "objectID": "mp02.html#q5-how-much-cleaner-is-new-york-compared-to-texas-vs",
    "href": "mp02.html#q5-how-much-cleaner-is-new-york-compared-to-texas-vs",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q5: How much cleaner is New York compared to Texas? üçè vs ü§†",
    "text": "Q5: How much cleaner is New York compared to Texas? üçè vs ü§†\nNew York and Texas have wildly different energy landscapes. Let‚Äôs compare their emissions per megawatt-hour:\n\n\nCode\nny_co2 &lt;- EIA_SEP_REPORT %&gt;% filter(state == \"New York\") %&gt;% pull(CO2_MWh)\ntx_co2 &lt;- EIA_SEP_REPORT %&gt;% filter(state == \"Texas\") %&gt;% pull(CO2_MWh)\nclean_factor &lt;- tx_co2 / ny_co2\n\ncomparison_table &lt;- data.frame(\n  State = c(\"New York\", \"Texas\", \"Clean Factor (TX / NY)\"),\n  `CO2 per MWh` = c(ny_co2, tx_co2, round(clean_factor, 2))\n)\n\n# Table\ngta_kable_style(comparison_table, caption = \"üçè vs ü§† CO‚ÇÇ Emissions Comparison\")\n\n\n\n\nüçè vs ü§† CO‚ÇÇ Emissions Comparison\n\n\nState\nCO2.per.MWh\n\n\n\n\nNew York\n522.00\n\n\nTexas\n855.00\n\n\nClean Factor (TX / NY)\n1.64\n\n\n\n\n\n\n\n\n\n\nCode\n# Bar chart: NY vs TX only\nny_tx_df &lt;- comparison_table[1:2, ]\nny_tx_df$State &lt;- factor(ny_tx_df$State, levels = c(\"New York\", \"Texas\"))\n\nggplot(ny_tx_df, aes(x = State, y = CO2.per.MWh, fill = State)) +\n  geom_col(show.legend = FALSE, color = accent_color) +\n  scale_fill_manual(values = c(\"New York\" = highlight_color, \"Texas\" = highlight_color)) +\n  labs(\n    title = \"üçè vs ü§† CO‚ÇÇ Emissions: New York vs Texas\",\n    x = \"State\",\n    y = \"CO‚ÇÇ per MWh\",\n    caption = \"Source: EIA State Profiles\"\n  ) +\n  theme_gta()\n\n\n\n\n\n\n\n\n\n\nReality check: Texas emits r round(clean_factor, 2) times more CO‚ÇÇ per MWh than New York. Everything is bigger in Texas, including the carbon footprint! üè¥‚Äç‚ò†Ô∏è"
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Conclusion üèÅ",
    "text": "Conclusion üèÅ\nElectricity is not created equal across the U.S. Some states are climate champions üå±, while others‚Ä¶ well, they need a little work. But the good news? Change is happening! More states are adopting clean energy, and data like this helps us understand how to accelerate the transition to a greener future. üöÄ"
  },
  {
    "objectID": "mp02.html#fueling-up-for-transit-analysis",
    "href": "mp02.html#fueling-up-for-transit-analysis",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üì¢ Fueling Up for Transit Analysis! üöã‚ö°",
    "text": "üì¢ Fueling Up for Transit Analysis! üöã‚ö°"
  },
  {
    "objectID": "mp02.html#the-ntd-energy-data",
    "href": "mp02.html#the-ntd-energy-data",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üöÄ 1. The NTD Energy Data",
    "text": "üöÄ 1. The NTD Energy Data\n\n\nCode\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings = FALSE, recursive = TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n  DS &lt;- download.file(\n    \"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n    destfile = NTD_ENERGY_FILE, \n    method = \"curl\"\n  )\n  if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\nto_numeric_fill_0 &lt;- function(x) replace_na(as.numeric(x), 0)\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n  select(-c(`Reporter Type`, `Reporting Module`, `Other Fuel`, `Other Fuel Description`)) |&gt; \n  mutate(across(-c(`Agency Name`, `Mode`, `TOS`), to_numeric_fill_0)) |&gt; \n  group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt; \n  summarize(across(where(is.numeric), sum), .groups = \"keep\") |&gt; \n  mutate(ENERGY = sum(c_across(where(is.numeric)))) |&gt; \n  filter(ENERGY &gt; 0) |&gt; \n  select(-ENERGY) |&gt; \n  ungroup()"
  },
  {
    "objectID": "mp02.html#decoding-transit-modes",
    "href": "mp02.html#decoding-transit-modes",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üé≠ 2. Decoding Transit Modes",
    "text": "üé≠ 2. Decoding Transit Modes\nUnderstanding transit modes is crucial! Let‚Äôs transform those cryptic codes into human-friendly labels. üëÄ\n\n\nCode\nNTD_ENERGY &lt;- NTD_ENERGY |&gt; \n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"MB\" ~ \"Motor Bus\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolley Bus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MG\" ~ \"Monorail / Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Aerial Tramway\",\n    Mode == \"TR\" ~ \"Hybrid Rail\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail (Alternative)\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\nNTD_ENERGY_LONG &lt;- NTD_ENERGY %&gt;%\n  pivot_longer(\n    cols = -c(`NTD ID`, `Agency Name`, Mode),\n    names_to = \"Fuel\",\n    values_to = \"Energy_Consumed\"\n  ) %&gt;%\n  filter(Energy_Consumed &gt; 0)\n\nsample_energy_table &lt;- NTD_ENERGY_LONG %&gt;% slice_sample(n = 10)\ngta_kable_style(sample_energy_table, caption = \"üîç Sample of NTD Energy (Long Format)\", col2 = 2)\n\n\n\n\nüîç Sample of NTD Energy (Long Format)\n\n\nNTD ID\nMode\nAgency Name\nFuel\nEnergy_Consumed\n\n\n\n\n90033\nStreetcar\nCity of Tucson\nElectric Propulsion\n1558012\n\n\n40171\nDemand Response\nKnoxville-Knox County Community Action Committee\nGasoline\n152000\n\n\n50517\nDemand Response\nCity of Maple Grove\nGasoline\n35166\n\n\n90162\nMotor Bus\nThe Eastern Contra Costa Transit Authority\nElectric Battery\n142836\n\n\n90226\nDemand Response\nImperial County Transportation Commission\nGasoline\n46533\n\n\n40021\nDemand Response\nCity of Albany\nC Natural Gas\n13256\n\n\n57\nDemand Response\nCentral Oregon Intergovernmental Council\nGasoline\n41791\n\n\n40002\nMotor Bus\nCity of Knoxville\nElectric Battery\n689098\n\n\n90226\nMotor Bus\nImperial County Transportation Commission\nDiesel Fuel\n144381\n\n\n50117\nDemand Response\nLaketran\nLiquified Petroleum Gas\n253698"
  },
  {
    "objectID": "mp02.html#conclusion-data-ready-for-analysis",
    "href": "mp02.html#conclusion-data-ready-for-analysis",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üéØ Conclusion: Data Ready for Analysis!",
    "text": "üéØ Conclusion: Data Ready for Analysis!\nüîπ We have successfully loaded, cleaned, and processed the NTD Energy dataset!\nüîπ Now, it‚Äôs primed and ready for deeper analysis‚Äîstay tuned for insights on emissions, efficiency, and green transit leaders! üåøüöé"
  },
  {
    "objectID": "mp02.html#ntd-service-data",
    "href": "mp02.html#ntd-service-data",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "NTD Service Data üöÄ",
    "text": "NTD Service Data üöÄ\n\n\nCode\nNTD_SERVICE &lt;- NTD_SERVICE_CLEAN %&gt;%\n  select(`NTD ID`, Agency, City, State, UPT, MILES) %&gt;%\n  filter(!is.na(UPT), !is.na(MILES), UPT &gt; 0, MILES &gt; 0)\n\nsample_service_table &lt;- head(NTD_SERVICE, 5)\ngta_kable_style(sample_service_table, caption = \"üöç Sample of Cleaned NTD Service Data\", col2 = 2)\n\n\n\n\nüöç Sample of Cleaned NTD Service Data\n\n\nNTD ID\nAgency\nCity\nState\nUPT\nMILES\n\n\n\n\n1\nKing County, dba: King County Metro\nSeattle\nWA\n78886848\n301530502\n\n\n2\nSpokane Transit Authority\nSpokane\nWA\n9403739\n46318134\n\n\n3\nPierce County Transportation Benefit Area Authority, dba: Pierce Transit\nLakewood\nWA\n6792245\n40362320\n\n\n5\nCity of Everett, dba: Everett Transit\nEverett\nWA\n1404970\n5193721\n\n\n6\nCity of Yakima, dba: Yakima Transit\nYakima\nWA\n646711\n3435365"
  },
  {
    "objectID": "mp02.html#unveiling-the-champions-of-public-transit",
    "href": "mp02.html#unveiling-the-champions-of-public-transit",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üèÜ Unveiling the Champions of Public Transit!",
    "text": "üèÜ Unveiling the Champions of Public Transit!\nPublic transportation: a noble effort to move the masses efficiently, reduce congestion, and save the planet. But how do different transit agencies measure up? Let‚Äôs crunch the numbers and find out who‚Äôs leading the charge! üöÜüí®"
  },
  {
    "objectID": "mp02.html#the-most-popular-transit-service-q1",
    "href": "mp02.html#the-most-popular-transit-service-q1",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üöÄ The Most Popular Transit Service (Q1)",
    "text": "üöÄ The Most Popular Transit Service (Q1)\nWhich agency moves the most people? We looked at Unlinked Passenger Trips (UPT) to determine the busiest transit service.\n\n\nCode\nmost_upt_service &lt;- NTD_SERVICE %&gt;%\n  arrange(desc(UPT)) %&gt;%\n  select(Agency, State, UPT) %&gt;%\n  head(1)\ngta_kable_style(most_upt_service, caption = \"üöç Transit Agency with the Most Riders\", col2 = 2)\n\n\n\n\nüöç Transit Agency with the Most Riders\n\n\nAgency\nState\nUPT\n\n\n\n\nMTA New York City Transit\nNY\n2632003044"
  },
  {
    "objectID": "mp02.html#nyc-subway-the-land-of-long-rides-q2",
    "href": "mp02.html#nyc-subway-the-land-of-long-rides-q2",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üóΩ NYC Subway: The Land of Long Rides (Q2)",
    "text": "üóΩ NYC Subway: The Land of Long Rides (Q2)\nLet‚Äôs calculate the average trip length for MTA New York City Transit (spoiler: it‚Äôs longer than your last relationship).\n\n\nCode\nmta_nyc_trip_length &lt;- NTD_SERVICE %&gt;%\n  filter(Agency == \"MTA New York City Transit\") %&gt;%\n  summarise(`Avg Trip Length (Miles)` = mean(MILES / UPT, na.rm = TRUE))\ngta_kable_style(mta_nyc_trip_length, caption = \"üóΩ Average Trip Length for MTA NYC Transit\")\n\n\n\n\nüóΩ Average Trip Length for MTA NYC Transit\n\n\nAvg Trip Length (Miles)\n\n\n\n\n3.644089"
  },
  {
    "objectID": "mp02.html#wheres-the-longest-ride-in-nyc-q3",
    "href": "mp02.html#wheres-the-longest-ride-in-nyc-q3",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üèôÔ∏è Where‚Äôs the Longest Ride in NYC? (Q3)",
    "text": "üèôÔ∏è Where‚Äôs the Longest Ride in NYC? (Q3)\nNot all NYC transit rides are equal! Which agency offers the longest average trip?\n\n\nCode\nnyc_longest_trip &lt;- NTD_SERVICE %&gt;%\n  filter(State == \"NY\") %&gt;%\n  mutate(avg_trip_length = MILES / UPT) %&gt;%\n  arrange(desc(avg_trip_length)) %&gt;%\n  select(Agency, City, avg_trip_length) %&gt;%\n  head(1)\ngta_kable_style(nyc_longest_trip, caption = \"üèôÔ∏è NYC Agency with Longest Avg Trip\", col2 = 3)\n\n\n\n\nüèôÔ∏è NYC Agency with Longest Avg Trip\n\n\nAgency\nCity\navg_trip_length\n\n\n\n\nHampton Jitney, Inc.\nCalverton\n92.4465"
  },
  {
    "objectID": "mp02.html#whos-driving-the-least-q4",
    "href": "mp02.html#whos-driving-the-least-q4",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üåé Who‚Äôs Driving the Least? (Q4)",
    "text": "üåé Who‚Äôs Driving the Least? (Q4)\nWe also looked at the state with the fewest total miles traveled on public transit. (Because not everyone has places to be.)\n\n\nCode\nfewest_miles_state &lt;- NTD_SERVICE %&gt;%\n  group_by(State) %&gt;%\n  summarise(`Total Transit Miles` = sum(MILES, na.rm = TRUE)) %&gt;%\n  arrange(`Total Transit Miles`) %&gt;%\n  head(1)\ngta_kable_style(fewest_miles_state, caption = \"üìâ State with the Fewest Transit Miles\", col2 = 2)\n\n\n\n\nüìâ State with the Fewest Transit Miles\n\n\nState\nTotal Transit Miles\n\n\n\n\nNH\n3749892"
  },
  {
    "objectID": "mp02.html#missing-states-alert-q5",
    "href": "mp02.html#missing-states-alert-q5",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "‚ùå Missing States Alert! (Q5)",
    "text": "‚ùå Missing States Alert! (Q5)\nAre there states missing from the National Transit Database (NTD)? Let‚Äôs find out! üö®\n\n\nCode\nall_states &lt;- data.frame(State = state.abb, Full_State_Name = state.name)\nmissing_states &lt;- all_states %&gt;%\n  anti_join(NTD_SERVICE, by = \"State\")\ngta_kable_style(missing_states, caption = \"üö® States Missing from NTD Service Data\", col2 = 2)\n\n\n\n\nüö® States Missing from NTD Service Data\n\n\nState\nFull_State_Name\n\n\n\n\nAZ\nArizona\n\n\nAR\nArkansas\n\n\nCA\nCalifornia\n\n\nCO\nColorado\n\n\nHI\nHawaii\n\n\nIA\nIowa\n\n\nKS\nKansas\n\n\nLA\nLouisiana\n\n\nMO\nMissouri\n\n\nMT\nMontana\n\n\nNE\nNebraska\n\n\nNV\nNevada\n\n\nNM\nNew Mexico\n\n\nND\nNorth Dakota\n\n\nOK\nOklahoma\n\n\nSD\nSouth Dakota\n\n\nTX\nTexas\n\n\nUT\nUtah\n\n\nWY\nWyoming"
  },
  {
    "objectID": "mp02.html#key-takeaways",
    "href": "mp02.html#key-takeaways",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üéØ Key Takeaways",
    "text": "üéØ Key Takeaways\n‚úÖ Most riders: The top agency moves millions! ‚úÖ NYC Subway riders take longer trips than your favorite TV show‚Äôs hiatus. ‚úÖ Smallest transit footprint: Some states barely use public transit. ‚úÖ Missing states: Should we be concerned? ü§î"
  },
  {
    "objectID": "mp02.html#eia-fuel-emission-factors-automated-scraping",
    "href": "mp02.html#eia-fuel-emission-factors-automated-scraping",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üß™ EIA Fuel Emission Factors: Automated Scraping",
    "text": "üß™ EIA Fuel Emission Factors: Automated Scraping\nTo calculate fuel-based emissions, we need to know how much CO‚ÇÇ (in kg) each gallon or unit of fuel releases.\nRather than entering values manually, we automated the process:\n\n\nCode\nurl &lt;- \"https://www.eia.gov/environment/emissions/co2_vol_mass.php\"\n\nco2_fuel_factors &lt;- read_html(url) %&gt;%\n  html_elements(\"table\") %&gt;%\n  .[[1]] %&gt;%\n  html_table() %&gt;%\n  select(Fuel = 1, kg_per_unit = 2) %&gt;%\n  mutate(\n    Fuel = str_trim(Fuel),\n    kg_per_unit = parse_number(kg_per_unit),\n    CO2_lb_per_unit = kg_per_unit * 2.20462  # Convert kg ‚Üí lbs\n  ) %&gt;%\n  filter(!is.na(kg_per_unit))  # Remove non-numeric rows\n\ndir.create(\"data/processed\", recursive = TRUE, showWarnings = FALSE)\n\nwrite_csv(co2_fuel_factors, \"data/processed/eia_co2_fuel_factors.csv\")\n\nco2_fuel_factors %&gt;%\n  slice_head(n = 10) %&gt;%\n  gta_kable_style(caption = \"üõ¢Ô∏è Sample of Scraped EIA Fuel Emission Factors\")\n\n\n\n\nüõ¢Ô∏è Sample of Scraped EIA Fuel Emission Factors\n\n\nFuel\nkg_per_unit\nCO2_lb_per_unit\n\n\n\n\nPropane\n12.68\n27.95458\n\n\nDiesel and Home Heating Fuel (Distillate Fuel Oil)\n22.45\n49.49372\n\n\nKerosene\n21.78\n48.01662\n\n\nCoal (All types)\n3826.88\n8436.81619\n\n\nNatural Gas\n120.85\n266.42833\n\n\nFinished Motor Gasoline\n18.73\n41.29253\n\n\nMotor Gasolinea\n20.86\n45.98837\n\n\nResidual Heating Fuel (Businesses only)\n24.78\n54.63048\n\n\nJet Fuel\n21.50\n47.39933\n\n\nAviation Gas\n18.32\n40.38864"
  },
  {
    "objectID": "mp02.html#final-dataset-emissions-overview",
    "href": "mp02.html#final-dataset-emissions-overview",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üì¢ Final Dataset: Emissions Overview",
    "text": "üì¢ Final Dataset: Emissions Overview\nLet‚Äôs take a look at the final cleaned dataset containing CO‚ÇÇ emissions data across transit agencies.\n\n\nCode\nwrite_rds(NTD_ENERGY_LONG, \"data/mp02/NTD_ENERGY_LONG.rds\")\nwrite_rds(NTD_SERVICE, \"data/mp02/NTD_SERVICE_CLEAN.rds\")\nwrite_rds(EIA_SEP_REPORT, \"data/mp02/EIA_SEP_REPORT.rds\")\n\nEIA_FUELS &lt;- read_csv(\"data/processed/eia_co2_fuel_factors.csv\") |&gt; \n  add_row(Fuel = \"Hydrogen\", kg_per_unit = 0)\n\nfuel_mapping &lt;- tribble(\n  ~Fuel,                      ~EIA_Fuel,\n  \"Diesel Fuel\",              \"Diesel and Home Heating Fuel (Distillate Fuel Oil)\",\n  \"Gasoline\",                 \"Finished Motor Gasoline\",\n  \"Liquified Petroleum Gas\", \"Propane\",\n  \"Electric Battery\",         NA_character_,\n  \"Electric Propulsion\",      NA_character_,\n  \"C Natural Gas\",            \"Natural Gas\",\n  \"Liquified Nat Gas\",        \"Natural Gas\",\n  \"Bio-Diesel\",               \"Diesel and Home Heating Fuel (Distillate Fuel Oil)\",\n  \"Hydrogen\",                 \"Hydrogen\"\n)\n\nanti_join(fuel_mapping, EIA_FUELS, by = c(\"EIA_Fuel\" = \"Fuel\"))\n\n\n\n\nCode\nemissions_data &lt;- NTD_ENERGY_LONG %&gt;%\n  left_join(NTD_SERVICE, by = \"NTD ID\") %&gt;%\n  left_join(fuel_mapping, by = \"Fuel\") %&gt;%\n  left_join(EIA_FUELS, by = c(\"EIA_Fuel\" = \"Fuel\")) %&gt;%\n  left_join(EIA_SEP_REPORT %&gt;% select(abbreviation, CO2_MWh),\n            by = c(\"State\" = \"abbreviation\")) %&gt;%\n  mutate(\n    Emissions_kg = case_when(\n      Fuel %in% c(\"Electric Battery\", \"Electric Propulsion\") & !is.na(CO2_MWh) ~ Energy_Consumed * CO2_MWh / 2.20462,\n      !is.na(kg_per_unit) ~ Energy_Consumed * kg_per_unit,\n      TRUE ~ 0\n    ),\n    Emissions_lb = Emissions_kg * 2.20462\n  ) %&gt;%\n  filter(!is.na(State)) %&gt;%\n  mutate(\n    CO2_per_MILE = Emissions_kg / MILES,\n    Total_CO2 = Emissions_kg,\n    CO2_Electric = ifelse(Fuel %in% c(\"Electric Battery\", \"Electric Propulsion\"), Emissions_kg, 0),\n    Agency_Size = case_when(\n      UPT &gt; 100000000 ~ \"Large\",\n      UPT &gt; 1000000   ~ \"Medium\",\n      TRUE              ~ \"Small\"\n    )\n  )\n\nfinal_emissions_table &lt;- emissions_data %&gt;%\n  group_by(Agency = `Agency Name`, Mode, Fuel, State) %&gt;%\n  summarise(\n    Total_Energy = sum(Energy_Consumed, na.rm = TRUE),\n    Total_Emissions_kg = sum(Emissions_kg, na.rm = TRUE),\n    Total_Emissions_lb = sum(Emissions_lb, na.rm = TRUE),\n    UPT = sum(UPT, na.rm = TRUE),\n    MILES = sum(MILES, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(Total_Emissions_kg))\n\n\ndir.create(\"outputs\", showWarnings = FALSE)\nwrite_csv(final_emissions_table, \"outputs/final_emissions_table.csv\")\nsaveRDS(final_emissions_table, \"data/processed/final_emissions_table.rds\")\n\n\ntop_emitters &lt;- final_emissions_table %&gt;%\n  slice_max(Total_Emissions_kg, n = 10) %&gt;%\n  select(Agency, Mode, Fuel, Total_Energy, Total_Emissions_kg, UPT, MILES)\n\ngta_kable_style(top_emitters, caption = \"üî• Top 10 Emitting Agencies by Fuel\", col2 = 2)\n\n\n\n\nüî• Top 10 Emitting Agencies by Fuel\n\n\nAgency\nMode\nFuel\nTotal_Energy\nTotal_Emissions_kg\nUPT\nMILES\n\n\n\n\nMTA New York City Transit\nHeavy Rail\nElectric Propulsion\n1546269600\n366118755704\n2632003044\n9591253658\n\n\nWashington Metropolitan Area Transit Authority\nHeavy Rail\nElectric Propulsion\n499328277\n192518001039\n231023784\n912604948\n\n\nMTA Long Island Rail Road\nCommuter Rail\nElectric Propulsion\n548190400\n129798055356\n83835706\n2033685836\n\n\nMetro-North Commuter Railroad Company, dba: MTA Metro-North Railroad\nCommuter Rail\nElectric Propulsion\n405036564\n95902734443\n66645285\n1150894931\n\n\nNew Jersey Transit Corporation\nCommuter Rail\nElectric Propulsion\n365911929\n85975079253\n198590133\n2314384007\n\n\nChicago Transit Authority\nHeavy Rail\nElectric Propulsion\n339757455\n80446240853\n279146501\n1090677628\n\n\nSoutheastern Pennsylvania Transportation Authority\nCommuter Rail\nElectric Propulsion\n204899285\n60876265150\n197264920\n834809485\n\n\nMassachusetts Bay Transportation Authority\nHeavy Rail\nElectric Propulsion\n135803120\n56856183723\n234975556\n1103417623\n\n\nSoutheastern Pennsylvania Transportation Authority\nHeavy Rail\nElectric Propulsion\n115833983\n34414665051\n197264920\n834809485\n\n\nMetropolitan Atlanta Rapid Transit Authority\nHeavy Rail\nElectric Propulsion\n77059623\n25621061071\n62093037\n352115956"
  },
  {
    "objectID": "mp02.html#conclusion-automating-for-a-greener-future",
    "href": "mp02.html#conclusion-automating-for-a-greener-future",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üéâ Conclusion: Automating for a Greener Future",
    "text": "üéâ Conclusion: Automating for a Greener Future\nBy automating the data collection, cleaning, and analysis, we enable cities and policymakers to make informed and data-driven decisions towards a greener future! üöÄ"
  },
  {
    "objectID": "mp02.html#task-6-normalizing-emissions-the-great-equalizer",
    "href": "mp02.html#task-6-normalizing-emissions-the-great-equalizer",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üßÆ Task 6: Normalizing Emissions ‚Äî The Great Equalizer",
    "text": "üßÆ Task 6: Normalizing Emissions ‚Äî The Great Equalizer\nWelcome back to Green Transit Awards‚Ñ¢, where transit agencies battle it out for climate glory. Now that we‚Äôve calculated total emissions like responsible climate nerds üåç, it‚Äôs time to normalize that data and level the playing field. Because let‚Äôs be honest:\n‚ÄúSaying a giant city emits more CO‚ÇÇ than a town with three buses is like saying King Kong eats more bananas than a hamster.‚Äù\n\nüéØ Objective\nWe‚Äôre diving deep into emissions per rider (UPT) and emissions per passenger mile to uncover who‚Äôs doing the most with the least carbon. It‚Äôs not about how big you are ‚Äî it‚Äôs how efficient you roll. üöåüí®\n‚öñÔ∏è How We Did It: Normalization Explained\nUsing our previously calculated final_emissions_table, we grouped the data by Agency + State and summed the following:\nüßÆ Total_Emissions_kg: Total kilograms of CO‚ÇÇ emitted\nüö∂ Total_UPT: Unlinked Passenger Trips\nüõ£Ô∏è Total_MILES: Total Passenger Miles\nWe then calculated two key metrics:\nkg_per_UPT = Emissions per rider (carbon cost of a ride)\nkg_per_Mile = Emissions per mile (carbon cost of distance)\nThese are our battle stats ‚Äî the CO‚ÇÇ K/D ratio of transit.\n\n\nCode\nnormalized_emissions &lt;- final_emissions_table %&gt;%\n  group_by(Agency, State) %&gt;%\n  summarise(\n    Total_Emissions_kg = sum(Total_Emissions_kg, na.rm = TRUE),\n    Total_UPT = sum(UPT, na.rm = TRUE),\n    Total_MILES = sum(MILES, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(Total_UPT &gt; 0, Total_MILES &gt; 0) %&gt;%\n  mutate(\n    kg_per_UPT = Total_Emissions_kg / Total_UPT,\n    kg_per_Mile = Total_Emissions_kg / Total_MILES\n  )"
  },
  {
    "objectID": "mp02.html#agency-size-categories",
    "href": "mp02.html#agency-size-categories",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üè∑Ô∏è Agency Size Categories",
    "text": "üè∑Ô∏è Agency Size Categories\nBecause it‚Äôs not fair to compare the MTA to a trolley in a beach town, we grouped agencies by ridership size:\nSmall: &lt; 1 million UPT/year\nMedium: 1‚Äì10 million UPT\nLarge: 10+ million UPT\n\n\nCode\nnormalized_emissions &lt;- normalized_emissions %&gt;%\n  mutate(\n    size = case_when(\n      Total_UPT &lt; 1e6 ~ \"Small\",\n      Total_UPT &lt; 10e6 ~ \"Medium\",\n      TRUE ~ \"Large\"\n    )\n  )\n\n\nüèÜ Top 10 Most Efficient Agencies (Per Rider)\nThese agencies produce the lowest emissions per person. They move you cleanly ‚Äî like a ninja on a carbon diet. ü•∑üçÉ\n\n\nCode\nnormalized_emissions %&gt;%\n  arrange(kg_per_UPT) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(Agency, State, Total_Emissions_kg, Total_UPT, kg_per_UPT, size) %&gt;%\n  gta_kable_style(caption = \"üí® Most Efficient Agencies (Per UPT)\")\n\n\n\n\nüí® Most Efficient Agencies (Per UPT)\n\n\nAgency\nState\nTotal_Emissions_kg\nTotal_UPT\nkg_per_UPT\nsize\n\n\n\n\nChampaign-Urbana Mass Transit District\nIL\n14765944.8\n34292424\n0.4305891\nLarge\n\n\nCity of Fayetteville\nNC\n6923513.6\n10978365\n0.6306507\nLarge\n\n\nGreater Bridgeport Transit Authority\nCT\n13536667.3\n21066596\n0.6425655\nLarge\n\n\nAnn Arbor Area Transportation Authority\nMI\n22334550.2\n28083390\n0.7952940\nLarge\n\n\nIntercity Transit\nWA\n18992023.2\n23393970\n0.8118341\nLarge\n\n\nGreen Mountain Transit Authority\nVT\n12069974.5\n14734848\n0.8191448\nLarge\n\n\nCity of Fort Lauderdale\nFL\n542367.1\n656373\n0.8263093\nSmall\n\n\nMs Coast Transportation Authority\nMS\n5269864.7\n6010512\n0.8767747\nMedium\n\n\nCity of Harrisonburg\nVA\n4035732.4\n4568238\n0.8834331\nMedium\n\n\nWorcester Regional Transit Authority\nMA\n11222372.1\n12400287\n0.9050091\nLarge\n\n\n\n\n\n\n\n\nüöÄ Top 10 Most Efficient Agencies (Per Mile)\nThese champs move people farther with less carbon. Imagine being able to cross the city on 2 grams of CO‚ÇÇ. These agencies get close. üåéüõ£Ô∏è\n\n\nCode\nnormalized_emissions %&gt;%\n  arrange(kg_per_Mile) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(Agency, State, Total_Emissions_kg, Total_MILES, kg_per_Mile, size) %&gt;%\n  gta_kable_style(caption = \"üõ£Ô∏è Most Efficient Agencies (Per Passenger Mile)\")\n\n\n\n\nüõ£Ô∏è Most Efficient Agencies (Per Passenger Mile)\n\n\nAgency\nState\nTotal_Emissions_kg\nTotal_MILES\nkg_per_Mile\nsize\n\n\n\n\nMs Coast Transportation Authority\nMS\n5269865\n55688752\n0.0946307\nMedium\n\n\nSnohomish County Public Transportation Benefit Area Corporation\nWA\n56003868\n471189320\n0.1188564\nLarge\n\n\nIntercity Transit\nWA\n18992023\n147168660\n0.1290494\nLarge\n\n\nThe Tri-County Council for the Lower Eastern Shore of Maryland\nMD\n4484579\n30017210\n0.1494003\nMedium\n\n\nCity of Fayetteville\nNC\n6923514\n45495870\n0.1521789\nLarge\n\n\nAnn Arbor Area Transportation Authority\nMI\n22334550\n141721440\n0.1575947\nLarge\n\n\nPotomac and Rappahannock Transportation Commission\nVA\n34575352\n219347540\n0.1576282\nMedium\n\n\nAdirondack Transit Lines, Inc.\nNY\n5340182\n31065245\n0.1719021\nSmall\n\n\nCentral Oregon Intergovernmental Council\nOR\n3151714\n17603085\n0.1790433\nMedium\n\n\nCentral Midlands Regional Transportation Authority\nSC\n14269704\n74085468\n0.1926114\nLarge"
  },
  {
    "objectID": "mp02.html#gta-iv-green-transit-awards-the-ceremony",
    "href": "mp02.html#gta-iv-green-transit-awards-the-ceremony",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üö¶ GTA IV Green Transit Awards: The Ceremony üé§",
    "text": "üö¶ GTA IV Green Transit Awards: The Ceremony üé§\nWelcome to Liberty City‚Äôs version of the Oscars ‚Äî but for public transit.\nForget tuxedos, we‚Äôre handing out awards to transit agencies based on emissions data ‚Äî and maybe a little judgment. üòè\nWe‚Äôve split the awards into four hard-hitting GTA-style categories:\n\nüèÖ Greenest Agency (Lowest CO‚ÇÇ per mile)\n\nüöóüí® Most Emissions Avoided (vs your cousin‚Äôs gas guzzler)\n\nüîå Electrification Excellence (because batteries ‚â† boring)\n\nüíÄ The ‚ÄúYikes‚Äù Award (highest CO‚ÇÇ/mile ‚Äî yeah, we‚Äôre looking at you)\n\nLet‚Äôs break it down.\n\nüèÖ Greenest Transit Agencies by Size\nThese agencies didn‚Äôt just go green ‚Äî they went full Claude Speed on carbon. We grouped them by rider size to keep it fair, then crowned the ones with the lowest CO‚ÇÇ per passenger mile.\n\n\nCode\ngreenest_agency_by_size &lt;- emissions_data |&gt; \n  filter(!is.na(CO2_per_MILE)) |&gt; \n  group_by(Agency_Size) |&gt; \n  arrange(CO2_per_MILE) |&gt; \n  slice(1) |&gt; \n  ungroup() |&gt; \n  select(Agency_Size, Agency, State, CO2_per_MILE)\n\ngta_kable_style(greenest_agency_by_size, caption = \"üèÖ Greenest Transit Agencies by Size (Lowest CO‚ÇÇ per Mile)\")\n\n\n\n\nüèÖ Greenest Transit Agencies by Size (Lowest CO‚ÇÇ per Mile)\n\n\nAgency_Size\nAgency\nState\nCO2_per_MILE\n\n\n\n\nLarge\nMTA New York City Transit\nNY\n0.000046\n\n\nMedium\nStark Area Regional Transit Authority\nOH\n0.000000\n\n\nSmall\nCity of Appleton, dba: Valley Transit\nWI\n0.000438\n\n\n\n\n\n\n\n\n\n\nCode\navg_co2_per_mile &lt;- mean(emissions_data$CO2_per_MILE, na.rm = TRUE)\n\ngreenest_agency_by_size &lt;- emissions_data %&gt;%\n  filter(!is.na(CO2_per_MILE)) %&gt;%\n  group_by(Agency_Size) %&gt;%\n  arrange(CO2_per_MILE) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  select(Agency_Size, Agency, State, CO2_per_MILE)\n\ngreenest_agency_by_size &lt;- greenest_agency_by_size %&gt;%\n  mutate(Label = ifelse(CO2_per_MILE &lt; 0.001, \"&lt; 0.001 kg\", paste0(round(CO2_per_MILE, 3), \" kg\")))\n\nggplot(greenest_agency_by_size, aes(x = reorder(Agency, CO2_per_MILE), y = CO2_per_MILE)) +\n  geom_segment(aes(xend = Agency, y = 0, yend = CO2_per_MILE), color = accent_color, size = 1.5) +\n  geom_point(aes(color = Agency_Size), size = 6) +\n  geom_text(aes(label = Label), \n            hjust = -0.3, color = \"white\", size = 4, fontface = \"bold\") +\n  coord_flip() +\n  labs(\n    title = \"üåø Clean Ride Royalty\",\n    subtitle = \"Top Greenest Transit Agencies by Size (CO‚ÇÇ per Passenger Mile)\",\n    x = NULL, y = \"CO‚ÇÇ per Mile (kg)\"\n  ) +\n  scale_color_manual(values = c(\"Small\" = highlight_color, \"Medium\" = accent_color, \"Large\" = \"#00FF95\")) +\n  theme_gta() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nüöóüí® Most Emissions Avoided (vs Private Cars)\nIf your agency saves more emissions than a weekend traffic jam in Algonquin, you get on this list. We modeled private car emissions and compared transit‚Äôs sweet, sweet gains.\n\n\nCode\nemissions_avoided_by_size &lt;- emissions_data |&gt; \n  mutate(\n    Gallons_Used = MILES / 25,\n    CO2_if_cars = Gallons_Used * 19.6,\n    Emissions_Avoided = CO2_if_cars - Total_CO2\n  ) |&gt;\n  group_by(Agency_Size) |&gt; \n  arrange(desc(Emissions_Avoided)) |&gt; \n  slice(1) |&gt; \n  ungroup() |&gt; \n  select(Agency_Size, Agency, State, Emissions_Avoided)\n\ngta_kable_style(emissions_avoided_by_size, caption = \"üöóüí® Most Emissions Avoided by Transit Agencies (By Size)\")\n\n\n\n\nüöóüí® Most Emissions Avoided by Transit Agencies (By Size)\n\n\nAgency_Size\nAgency\nState\nEmissions_Avoided\n\n\n\n\nLarge\nMTA New York City Transit\nNY\n7519101389\n\n\nMedium\nMTA Long Island Rail Road\nNY\n1435350705\n\n\nSmall\nHampton Jitney, Inc.\nNY\n28931084\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(emissions_avoided_by_size, aes(x = Agency, y = 1, size = Emissions_Avoided, fill = Agency_Size)) +\n  geom_point(shape = 21, color = \"white\", stroke = 1.5) +\n  scale_size(range = c(15, 50), name = \"Emissions Avoided (kg)\") +\n  scale_fill_manual(values = c(\"Large\" = highlight_color, \"Medium\" = accent_color, \"Small\" = \"#00FF95\")) +\n  labs(\n    title = \"üåê Emissions Avoided by Transit Agencies\",\n    subtitle = \"Each bubble scaled by kg of CO‚ÇÇ avoided\",\n    x = NULL, y = NULL\n  ) +\n  theme_gta() +\n  geom_text(aes(label = paste0(round(Emissions_Avoided / 1e6, 1), \"M kg\")), \n            vjust = -4, size = 4, color = \"white\")\n\n\n\n\n\n\n\n\n\n\n\nüîå Electrification Excellence (By Size)\nSome agencies plugged in and never looked back. We honored those who rely most on electric power for CO‚ÇÇ savings. Liberty City salutes your socket game. ‚ö°\n\n\nCode\nelectrification_award_by_size &lt;- emissions_data |&gt; \n  mutate(Electric_Share = CO2_Electric / Total_CO2) |&gt; \n  filter(!is.na(Electric_Share)) |&gt; \n  group_by(Agency_Size) |&gt; \n  arrange(desc(Electric_Share)) |&gt; \n  slice(1) |&gt; \n  ungroup() |&gt; \n  select(Agency_Size, Agency, State, Electric_Share)\n\ngta_kable_style(electrification_award_by_size, caption = \"üîå Electrification Excellence (By Size)\")\n\n\n\n\nüîå Electrification Excellence (By Size)\n\n\nAgency_Size\nAgency\nState\nElectric_Share\n\n\n\n\nLarge\nMassachusetts Bay Transportation Authority\nMA\n1\n\n\nMedium\nKing County, dba: King County Metro\nWA\n1\n\n\nSmall\nCity of Wilsonville, dba: South Metro Area Regional Transit\nOR\n1\n\n\n\n\n\n\n\n\n\n\nCode\nelectrification_top5 &lt;- emissions_data %&gt;%\n  mutate(\n    Electric_Share = CO2_Electric / Total_CO2,\n    Electric_Pct = round(100 * Electric_Share, 1)\n  ) %&gt;%\n  filter(!is.na(Electric_Share)) %&gt;%\n  group_by(Agency_Size) %&gt;%\n  slice_max(order_by = Electric_Share, n = 5, with_ties = FALSE) %&gt;%\n  ungroup()\n\nlibrary(forcats)\n\nelectrification_top5_clean &lt;- electrification_top5 %&gt;%\n  mutate(\n    Short_Label = Agency %&gt;%\n      str_replace_all(\"(?i)dba.*\", \"\") %&gt;%\n      str_replace_all(\"Transit Authority\", \"TA\") %&gt;%\n      str_replace_all(\"Transportation\", \"Transp.\") %&gt;%\n      str_replace_all(\"Department of\", \"Dept.\") %&gt;%\n      str_replace_all(\"University\", \"Univ.\") %&gt;%\n      str_replace_all(\"City of \", \"\") %&gt;%\n      str_squish()\n  ) %&gt;%\n  mutate(Polar_Label = paste0(str_wrap(paste0(Short_Label, \" (\", State, \")\"), width = 18)))\n\n\n# ‚îÄ‚îÄ ü™Ñ Compact lollipop chart grouped by size ‚îÄ‚îÄ\nggplot(electrification_top5_clean, aes(x = Electric_Pct, y = fct_reorder(Short_Label, Electric_Pct))) +\n  geom_segment(aes(x = 0, xend = Electric_Pct, yend = fct_reorder(Short_Label, Electric_Pct), color = Agency_Size),\n               linewidth = 2) +\n  geom_point(aes(color = Agency_Size), size = 5) +\n  geom_text(aes(label = paste0(Electric_Pct, \"%\")), \n            hjust = -0.3, size = 3.5, fontface = \"bold\", color = \"white\") +\n  facet_wrap(~Agency_Size, scales = \"free_y\", ncol = 1) +\n  scale_color_manual(values = c(\"Large\" = highlight_color, \"Medium\" = accent_color, \"Small\" = \"#00FF95\")) +\n  labs(\n    title = \"‚ö° Electrification Elite: GTA IV Edition\",\n    subtitle = \"Top 5 Transit Agencies by Electric CO‚ÇÇ Share (Grouped by Agency Size)\",\n    x = \"Electric Share of Emissions (%)\", y = NULL\n  ) +\n  theme_gta() +\n  theme(\n    strip.text = element_text(face = \"bold\", color = \"white\", size = 12),\n    plot.title = element_text(color = highlight_color, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(color = \"white\", size = 12),\n    axis.text.y = element_text(size = 8, color = \"white\"),\n    legend.position = \"none\"\n  ) +\n  xlim(0, 105)\n\n\n\n\n\n\n\n\n\n\n\nüíÄ The ‚ÄúYikes‚Äù Award (Worst CO‚ÇÇ per Mile)\nYou thought Liberty City traffic was bad. These guys are worse. The top CO‚ÇÇ emitters per mile get a not-so-glamorous spot in our Hall of Shame.\n\n\nCode\nworst_agency_by_size &lt;- emissions_data |&gt; \n  filter(!is.na(CO2_per_MILE)) |&gt; \n  group_by(Agency_Size) |&gt; \n  arrange(desc(CO2_per_MILE)) |&gt; \n  slice(1) |&gt; \n  ungroup() |&gt; \n  select(Agency_Size, Agency, State, CO2_per_MILE)\n\ngta_kable_style(worst_agency_by_size, caption = \"üíÄ 'Yikes' Award ‚Äì Worst CO‚ÇÇ per Mile by Size\")\n\n\n\n\nüíÄ 'Yikes' Award ‚Äì Worst CO‚ÇÇ per Mile by Size\n\n\nAgency_Size\nAgency\nState\nCO2_per_MILE\n\n\n\n\nLarge\nWashington Metropolitan Area Transit Authority, dba: Washington Metro\nDC\n210.9544\n\n\nMedium\nAlternativa de Transporte Integrado , dba: Autoridad de Transporte Integrado\nPR\n297.5539\n\n\nSmall\nPennsylvania Department of Transportation\nPA\n124.2203\n\n\n\n\n\n\n\n\n\n\nCode\nworst_agency_by_size &lt;- emissions_data %&gt;%\n  filter(!is.na(CO2_per_MILE)) %&gt;%\n  group_by(Agency_Size) %&gt;%\n  arrange(desc(CO2_per_MILE)) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  select(Agency_Size, Agency, State, CO2_per_MILE)\n\nworst_agency_by_size$CO2_per_MILE &lt;- worst_agency_by_size$CO2_per_MILE / max(worst_agency_by_size$CO2_per_MILE)\n\nif (!requireNamespace(\"fmsb\", quietly = TRUE)) install.packages(\"fmsb\")\nlibrary(fmsb)\n\nradar_data &lt;- as.data.frame(t(worst_agency_by_size$CO2_per_MILE / max(worst_agency_by_size$CO2_per_MILE)))\ncolnames(radar_data) &lt;- worst_agency_by_size$Agency_Size\nradar_data &lt;- rbind(rep(1, ncol(radar_data)), rep(0, ncol(radar_data)), radar_data)\n\nradarchart(\n  radar_data,\n  axistype = 1,\n  pcol = highlight_color, pfcol = rgb(1, 0, 0.8, 0.4), plwd = 4,\n  cglcol = accent_color, cglty = 1, axislabcol = \"white\", caxislabels = seq(0, 1, 0.2), cglwd = 1,\n  vlcex = 1.2,\n  title = \"üíÄ 'Yikes' Award ‚Äì Worst CO‚ÇÇ/Mile by Agency Size\"\n)"
  },
  {
    "objectID": "mp02.html#final-word-from-gta-iv-transit-bureau",
    "href": "mp02.html#final-word-from-gta-iv-transit-bureau",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üßæ Final Word from GTA IV Transit Bureau üóΩ",
    "text": "üßæ Final Word from GTA IV Transit Bureau üóΩ\nThese agencies showed us who‚Äôs really pulling their weight ‚Äî and who‚Äôs puffing more smoke than a busted Sabre GT.\n‚úÖ From clean miles to electric rides, we‚Äôve scraped, cleaned, calculated, and visualized the wild world of U.S. transit emissions.\nüî• If you‚Äôre not green, you‚Äôre just another red dot on the radar. Stay clean, Liberty City."
  },
  {
    "objectID": "mp02.html#clean-ride-royalty-the-greenest-transit-agencies-by-size",
    "href": "mp02.html#clean-ride-royalty-the-greenest-transit-agencies-by-size",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üèÖ Clean Ride Royalty ‚Äì The Greenest Transit Agencies by Size",
    "text": "üèÖ Clean Ride Royalty ‚Äì The Greenest Transit Agencies by Size\nForget horsepower ‚Äî this is about carbon-footprint finesse. These agencies prove you don‚Äôt need to burn rubber to move people. We crunched the emissions data, normalized it to CO‚ÇÇ per passenger mile, and crowned the cleanest of the clean:\n\n\n\n\n\n\n\n\n\nüè∑Ô∏è Size\nüöè Agency\nüìç State\nüåø CO‚ÇÇ per Mile (kg)\n\n\n\n\nLarge\nMTA New York City Transit\nNY\n0.000046\n\n\nMedium\nStark Area Regional Transit Authority\nOH\n0.000000\n\n\nSmall\nCity of Appleton, dba: Valley Transit\nWI\n0.000438\n\n\n\n\nüïäÔ∏è Stark Area Regional Transit Authority is so clean, we double-checked if they were teleporting people.\nüöá NYC‚Äôs MTA proves that even in a sprawling mega-metropolis, you can still keep it green.\nüßÄ Wisconsin‚Äôs Valley Transit? More eco than a farmers‚Äô market on a fixie."
  },
  {
    "objectID": "mp02.html#the-carbon-capos-most-emissions-avoided-by-transit-agencies",
    "href": "mp02.html#the-carbon-capos-most-emissions-avoided-by-transit-agencies",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üöóüí® The Carbon Capos ‚Äì Most Emissions Avoided by Transit Agencies",
    "text": "üöóüí® The Carbon Capos ‚Äì Most Emissions Avoided by Transit Agencies\nStep aside, Teslas. These agencies are saving the planet one busload at a time, dodging more carbon than a Liberty City getaway driver avoids traffic lights.\nWe estimated how much CO‚ÇÇ each agency avoided compared to if their passengers drove private cars (assuming 25 MPG and 19.6 lbs CO‚ÇÇ per gallon). Here are your MVPs ‚Äî Most Valuable Polluters‚Ä¶ Avoided:\n\n\n\n\n\n\n\n\n\nüè∑Ô∏è Size\nüöè Agency\nüìç State\nüí® CO‚ÇÇ Avoided (kg)\n\n\n\n\nLarge\nMTA New York City Transit\nNY\n7,519,101,389\n\n\nMedium\nMTA Long Island Rail Road\nNY\n1,435,350,705\n\n\nSmall\nHampton Jitney, Inc.\nNY\n28,931,084\n\n\n\n\nüóΩ New York sweep! The Empire State is practically smudging carbon off the map.\nüöå MTA NYC singlehandedly avoided more emissions than some countries emit.\nüß≥ Hampton Jitney said ‚Äúluxury bus‚Äù and luxury planet.\n\nüéØ Metric calculated as:\n\nEmissions avoided = (Transit miles √∑ 25 MPG) √ó 19.6 lbs CO‚ÇÇ ‚àí Transit CO‚ÇÇ emissions."
  },
  {
    "objectID": "mp02.html#electrification-excellence-the-battery-bosses",
    "href": "mp02.html#electrification-excellence-the-battery-bosses",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "‚ö° Electrification Excellence ‚Äì The Battery Bosses",
    "text": "‚ö° Electrification Excellence ‚Äì The Battery Bosses\nWhile some agencies are still guzzling gas like it‚Äôs 1999, these transit legends have gone full electric ‚Äî zapping emissions with the finesse of a Liberty City hacker on a subway heist.\nWe calculated each agency‚Äôs Electric Share of CO‚ÇÇ emissions ‚Äî the percentage of total emissions coming from electric-based fuel. And these winners? 100% electric. That‚Äôs right ‚Äî not a single puff of smoke.\n\n\n\n\n\n\n\n\n\nüè∑Ô∏è Size\nüöè Agency\nüìç State\n‚ö° Electric Share\n\n\n\n\nLarge\nMassachusetts Bay Transportation Authority\nMA\n100%\n\n\nMedium\nKing County, dba: King County Metro\nWA\n100%\n\n\nSmall\nCity of Wilsonville, dba: South Metro Area Regional Transit\nOR\n100%\n\n\n\n\nüîå They didn‚Äôt just ride the wave ‚Äî they charged it.\nüíØ Not 99%. Not ‚Äúwe‚Äôre working on it.‚Äù Straight-up 100% electric, baby.\nüß† While others are debating fuel blends, these agencies said ‚Äúoutlet or bust.‚Äù\n\nüéØ Metric calculated as:\n\nElectric Share = CO‚ÇÇ emissions from electric modes √∑ Total CO‚ÇÇ emissions\n\nüÜö Reference point: The median agency‚Äôs electric share? ~17%.\nThese awardees are basically driving a Tesla bus in the Matrix.\nData sources: FTA NTD Energy Data (2023), EIA Fuel Emission Factors"
  },
  {
    "objectID": "mp02.html#the-yikes-award-most-co‚ÇÇ-per-mile-by-size",
    "href": "mp02.html#the-yikes-award-most-co‚ÇÇ-per-mile-by-size",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üíÄ The ‚ÄúYikes‚Äù Award ‚Äì Most CO‚ÇÇ per Mile (By Size)",
    "text": "üíÄ The ‚ÄúYikes‚Äù Award ‚Äì Most CO‚ÇÇ per Mile (By Size)\nSome agencies shine like neon on a Liberty City taxi. Others‚Ä¶ well‚Ä¶ belch more CO‚ÇÇ than a broken-down Blista Compact doing donuts in Broker. These transit operations didn‚Äôt just miss the green bus ‚Äî they set it on fire on the way out. üî•üöå\nWe calculated each agency‚Äôs CO‚ÇÇ per mile to see who‚Äôs earning their carbon karma the hard way.\n\n\n\n\n\n\n\n\n\nüè∑Ô∏è Size\nüöè Agency\nüìç State\nüí® CO‚ÇÇ per Mile (kg)\n\n\n\n\nLarge\nWashington Metropolitan Area Transit Authority, dba: Washington Metro\nDC\n210.95\n\n\nMedium\nAlternativa de Transporte Integrado, dba: Autoridad de Transporte Integrado\nPR\n297.55\n\n\nSmall\nPennsylvania Department of Transportation\nPA\n124.22\n\n\n\n\nüõë Metric calculated as:\nCO‚ÇÇ per Mile = Total kg of emissions / Total passenger miles\n\nüìä Reference point? The median agency emitted ~1.08 kg per mile. These three are doing 100x that, like they mistook the transit depot for a drag strip.\n\nüßØ Dear operators: If you‚Äôre seeing this, we love you, but it might be time for a fleet intervention. Or at least, like, one electric scooter.\n\nüóûÔ∏è These agencies win a used catalytic converter and free tickets to the ‚Äúhow to electrify a fleet‚Äù workshop.\nData sources: FTA NTD Energy + Service Data (2023), EIA Fuel Emission Factors\nüíæ Mission Complete\nüèÅ Final Report from the Liberty City Transit Bureau\nüé§ The Final Word üñ§ Transit isn‚Äôt just about getting from Point A to B ‚Äî it‚Äôs about getting there cleaner, smarter, and cooler than ever before.\nFrom clean ride royalty to electrification titans, we‚Äôve ranked them all. üïπÔ∏è Powered by data, styled like GTA IV, and wrapped in hot pink & neon blue ‚Äî this wasn‚Äôt just an analysis. This was a climate side quest with a vengeance.\nüèÜ Awards Recap üíö Greenest Riders: MTA NYC & friends gliding past the carbon fog\nüîå Electrification Gods: 100% battery beasts that don‚Äôt even flinch\nüöóüí® Emissions Avengers: Saving more CO‚ÇÇ than your cousin‚Äôs pickup\nüíÄ The ‚ÄúYikes‚Äù Award: For those who‚Ä¶ really need to charge up üò¨\nüìä What We Actually Did: ‚úÖ Automated data scraping from EIA + NTD\n‚úÖ Calculated & normalized emissions across all agencies\n‚úÖ Designed GTA IV‚Äìthemed tables and plots\n‚úÖ Ranked transit leaders in four fierce climate categories\n‚úÖ Gave it enough chaotic good energy to land a Rockstar bonus üí£\nüìä Data sources:\n\nFTA NTD Energy Data (2023), EIA Fuel Emission Factors (https://www.eia.gov/environment/emissions/co2_vol_mass.php)\nThe image was sourced by Chat GPT, Which also helped me with background theme by creating a .css file"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dhruv‚Äôs STA 9750 Website",
    "section": "",
    "text": "Last Updated: Thursday 03 27, 2025 at 11:36AM\n\n\n\n\n\n\nWelcome to My STA 9750 Project Page! üöÄ Hello, I‚Äôm Dhruv Sharma! üëã I am a data-driven thinker and a student at Baruch College, currently taking STA 9750. This site is my digital notebook for the course ‚Äî filled with projects, experiments, and insights in statistics and data science.\nWhat You‚Äôll Find Here üìÇ ‚úÖ üìä Mini-Projects ‚úÖ üìö Course Assignments ‚úÖ üìù Blog & Reflections ‚úÖ ‚ú® Additional Learning Materials\nSkills & Tools I‚Äôm Using üõ†Ô∏è R, Quarto, tidyverse, ggplot2\nHypothesis Testing & Inference\nGit & GitHub\nData Cleaning, Visualization, and Communication\nA Fun Fact About Me üéâ When I‚Äôm not exploring data, I enjoy gaming, design, and keeping up with tech. I‚Äôm passionate about using data to tell stories that matter.\nConnect With Me üåê üè† GitHub: Dhruvhw-10\nüìß Email: d619sharma@gmail.com\n‚ÄúData is the new oil, but insights are the fuel that drive the world forward.‚Äù ‚Äì Anonymous"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "NYC Payroll Data Analysis",
    "section": "",
    "text": "This report analyzes NYC Payroll Data, examining financial trends and evaluating three policy proposals to optimize payroll expenditures. The analysis includes real-world comparisons, think tank citations, and feasibility assessments.\n\n\n‚úÖ Identify trends in salaries, overtime, and payroll growth.\n‚úÖ Compare results with external reports.\n‚úÖ Assess the feasibility of three policy proposals to reduce payroll costs."
  },
  {
    "objectID": "mp01.html#objectives",
    "href": "mp01.html#objectives",
    "title": "NYC Payroll Data Analysis",
    "section": "",
    "text": "‚úÖ Identify trends in salaries, overtime, and payroll growth.\n‚úÖ Compare results with external reports.\n‚úÖ Assess the feasibility of three policy proposals to reduce payroll costs."
  },
  {
    "objectID": "mp01.html#load-and-clean-data",
    "href": "mp01.html#load-and-clean-data",
    "title": "NYC Payroll Data Analysis",
    "section": "2.1 ** Load and Clean Data**",
    "text": "2.1 ** Load and Clean Data**\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(gt)\nlibrary(ggplot2)\nlibrary(scales)\n\npayroll_data &lt;- read_csv(\"data/mp01/nyc_payroll_export.csv\")\ncolnames(payroll_data)\n\n\n [1] \"fiscal_year\"                \"payroll_number\"            \n [3] \"agency_name\"                \"last_name\"                 \n [5] \"first_name\"                 \"mid_init\"                  \n [7] \"agency_start_date\"          \"work_location_borough\"     \n [9] \"title_description\"          \"leave_status_as_of_june_30\"\n[11] \"base_salary\"                \"pay_basis\"                 \n[13] \"regular_hours\"              \"regular_gross_paid\"        \n[15] \"ot_hours\"                   \"total_ot_paid\"             \n[17] \"total_other_pay\"           \n\n\nCode\npayroll_data &lt;- payroll_data %&gt;%\n  mutate(\n    agency_name = str_to_title(agency_name),\n    last_name = str_to_title(last_name),\n    first_name = str_to_title(first_name),\n    work_location_borough = str_to_title(work_location_borough),\n    title_description = str_to_title(title_description),\n    leave_status = str_to_title(leave_status_as_of_june_30)\n  )\ncolnames(payroll_data)\n\n\n [1] \"fiscal_year\"                \"payroll_number\"            \n [3] \"agency_name\"                \"last_name\"                 \n [5] \"first_name\"                 \"mid_init\"                  \n [7] \"agency_start_date\"          \"work_location_borough\"     \n [9] \"title_description\"          \"leave_status_as_of_june_30\"\n[11] \"base_salary\"                \"pay_basis\"                 \n[13] \"regular_hours\"              \"regular_gross_paid\"        \n[15] \"ot_hours\"                   \"total_ot_paid\"             \n[17] \"total_other_pay\"            \"leave_status\"              \n\n\nCode\npayroll_data &lt;- payroll_data %&gt;%\n  rename(\n    fiscal_year = `fiscal_year`,\n    agency_name = `agency_name`,\n    last_name = `last_name`,\n    first_name = `first_name`,\n    title = `title_description`,\n    salary = `base_salary`,\n    pay_basis = `pay_basis`,\n    reg_hours = `regular_hours`,\n    borough = `work_location_borough`\n  )\npayroll_data &lt;- payroll_data %&gt;%\n  mutate(salary = as.numeric(salary),\n         reg_hours = as.numeric(reg_hours),\n         ot_hours = as.numeric(ot_hours))\nglimpse(payroll_data)\n\n\nRows: 6,225,611\nColumns: 18\n$ fiscal_year                &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2‚Ä¶\n$ payroll_number             &lt;dbl&gt; 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,‚Ä¶\n$ agency_name                &lt;chr&gt; \"Admin For Children's Svcs\", \"Admin For Chi‚Ä¶\n$ last_name                  &lt;chr&gt; \"Faye Fall\", \"Kilgore\", \"Wisdom\", \"Miller\",‚Ä¶\n$ first_name                 &lt;chr&gt; \"Sokhna\", \"Orlantha\", \"Cherise\", \"Moya-Gaye‚Ä¶\n$ mid_init                   &lt;chr&gt; \"M\", \"B\", \"M\", \"S\", \"M\", \"L\", \"O\", NA, \"N\",‚Ä¶\n$ agency_start_date          &lt;dttm&gt; 2023-11-20, 2023-08-28, 2022-10-24, 2023-0‚Ä¶\n$ borough                    &lt;chr&gt; \"Bronx\", \"Brooklyn\", \"Manhattan\", \"Manhatta‚Ä¶\n$ title                      &lt;chr&gt; \"Child Protective Specialist\", \"Child Prote‚Ä¶\n$ leave_status_as_of_june_30 &lt;chr&gt; \"ACTIVE\", \"ACTIVE\", \"ON LEAVE\", \"ON LEAVE\",‚Ä¶\n$ salary                     &lt;dbl&gt; 62043, 62043, 43144, 62043, 60236, 62043, 6‚Ä¶\n$ pay_basis                  &lt;chr&gt; \"per Annum\", \"per Annum\", \"per Annum\", \"per‚Ä¶\n$ reg_hours                  &lt;dbl&gt; 1050.00, 1470.00, 1251.50, 1400.75, 700.00,‚Ä¶\n$ regular_gross_paid         &lt;dbl&gt; 31267.96, 44660.96, 28649.20, 44515.43, 221‚Ä¶\n$ ot_hours                   &lt;dbl&gt; 12.00, 99.75, 30.00, 44.75, 53.00, 146.00, ‚Ä¶\n$ total_ot_paid              &lt;dbl&gt; 425.00, 3859.84, 802.42, 1476.98, 1933.33, ‚Ä¶\n$ total_other_pay            &lt;dbl&gt; 78.04, 78.14, 78.26, 78.37, 78.47, 78.86, 7‚Ä¶\n$ leave_status               &lt;chr&gt; \"Active\", \"Active\", \"On Leave\", \"On Leave\",‚Ä¶"
  },
  {
    "objectID": "mp01.html#mayor-eric-adams-salary",
    "href": "mp01.html#mayor-eric-adams-salary",
    "title": "NYC Payroll Data Analysis",
    "section": "3.1 Mayor Eric Adams‚Äô Salary",
    "text": "3.1 Mayor Eric Adams‚Äô Salary\n\n\nCode\nmayor_data &lt;- payroll_data %&gt;%\n  filter(str_detect(`first_name`, \"Eric\") & str_detect(`last_name`, \"Adams\")) %&gt;%\n  select(`fiscal_year`, `title`, `agency_name`, `salary`) %&gt;%\n  arrange(`fiscal_year`)\nmayor_data %&gt;%\n  mutate(`salary` = dollar(`salary`)) %&gt;%\n  datatable(options = list(\n    searching = FALSE,\n    paging = FALSE,\n    info = FALSE\n  ))"
  },
  {
    "objectID": "mp01.html#total-compensation-calculation",
    "href": "mp01.html#total-compensation-calculation",
    "title": "NYC Payroll Data Analysis",
    "section": "3.2 üí∞ Total Compensation Calculation",
    "text": "3.2 üí∞ Total Compensation Calculation\nTo calculate total compensation, we consider different pay structures:\n\nAnnual Salary ‚Üí Directly assigned\nHourly Employees ‚Üí (Hourly Rate * Regular Hours) + (1.5 * Hourly Rate * Overtime Hours)\nDaily Employees ‚Üí (Daily Rate * (Regular Hours / 7.5))\n\n\n\nCode\npayroll_data &lt;- payroll_data %&gt;%\n  mutate(\n    total_compensation = case_when(\n      `pay_basis` == \"per Annum\" ~ `salary`,\n      `pay_basis` == \"per Hour\" ~ `salary` * `reg_hours` + (`salary` * 1.5 * `ot_hours`),\n      `pay_basis` == \"per Day\" ~ `salary` * (`reg_hours` / 7.5),\n      TRUE ~ `salary`\n    )\n  )\ndatatable(\n  payroll_data %&gt;%\n    select(first_name, last_name, agency_name, title, pay_basis, salary, reg_hours, ot_hours, total_compensation) %&gt;%\n    arrange(desc(total_compensation)) %&gt;%\n    slice_head(n = 10),\n  options = list(scrollX = TRUE)\n)"
  },
  {
    "objectID": "mp01.html#highest-base-salary-job-title",
    "href": "mp01.html#highest-base-salary-job-title",
    "title": "NYC Payroll Data Analysis",
    "section": "4.1 ** Highest Base Salary Job Title**",
    "text": "4.1 ** Highest Base Salary Job Title**\n\n\nCode\nhighest_paid_job &lt;- payroll_data %&gt;%\n  filter(pay_basis == \"per Annum\") %&gt;%\n  mutate(hourly_rate = salary / 2000) %&gt;%\n  arrange(desc(hourly_rate)) %&gt;%\n  select(title, agency_name, salary, hourly_rate) %&gt;%\n  slice(1)\n\nprint(highest_paid_job)\n\n\n# A tibble: 1 √ó 4\n  title agency_name           salary hourly_rate\n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;\n1 Chair Nyc Housing Authority 414707        207.\n\n\nüìå Insight: Employees in executive positions tend to have the highest base salaries. External reports confirm that high salaries are a budget concern."
  },
  {
    "objectID": "mp01.html#highest-earning-employee",
    "href": "mp01.html#highest-earning-employee",
    "title": "NYC Payroll Data Analysis",
    "section": "4.2 ** Highest Earning Employee**",
    "text": "4.2 ** Highest Earning Employee**\nThe highest-earning employee based on total compensation.\n\n\nCode\nhighest_earning_employee &lt;- payroll_data %&gt;%\n  mutate(total_compensation = salary + total_ot_paid + total_other_pay) %&gt;%\n  arrange(desc(total_compensation)) %&gt;%\n  select(fiscal_year, first_name, last_name, title, agency_name, total_compensation) %&gt;%\n  slice(1)\n\nprint(highest_earning_employee)\n\n\n# A tibble: 1 √ó 6\n  fiscal_year first_name last_name title          agency_name total_compensation\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;                    &lt;dbl&gt;\n1        2024 Mark       Tettonis  Chief Marine ‚Ä¶ Department‚Ä¶           1382854.\n\n\nüìå Insight: Some employees earn significantly more than their base salary due to overtime and other pay."
  },
  {
    "objectID": "mp01.html#most-overtime-hours-worked",
    "href": "mp01.html#most-overtime-hours-worked",
    "title": "NYC Payroll Data Analysis",
    "section": "4.3 ** Most Overtime Hours Worked**",
    "text": "4.3 ** Most Overtime Hours Worked**\nIdentifies the employee who has worked the most overtime hours.\n\n\nCode\nmost_overtime_employee &lt;- payroll_data %&gt;%\n  arrange(desc(ot_hours)) %&gt;%\n  select(fiscal_year, first_name, last_name, title, agency_name, ot_hours) %&gt;%\n  slice(1)\n\nprint(most_overtime_employee)\n\n\n# A tibble: 1 √ó 6\n  fiscal_year first_name last_name   title              agency_name     ot_hours\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;              &lt;chr&gt;              &lt;dbl&gt;\n1        2022 James      Internicola Correction Officer Department Of ‚Ä¶    3693."
  },
  {
    "objectID": "mp01.html#agency-with-highest-average-payroll",
    "href": "mp01.html#agency-with-highest-average-payroll",
    "title": "NYC Payroll Data Analysis",
    "section": "4.4 ** Agency with Highest Average Payroll**",
    "text": "4.4 ** Agency with Highest Average Payroll**\nThe agency with the highest average payroll per employee.\n\n\nCode\nhighest_avg_payroll_agency &lt;- payroll_data %&gt;%\n  group_by(agency_name) %&gt;%\n  summarise(avg_payroll = mean(salary + total_ot_paid + total_other_pay, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_payroll)) %&gt;%\n  slice(1)\n\nprint(highest_avg_payroll_agency)\n\n\n# A tibble: 1 √ó 2\n  agency_name             avg_payroll\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 Office Of Racial Equity     153102.\n\n\nüìå Insight: Some specialized agencies pay significantly more than others due to expertise requirements."
  },
  {
    "objectID": "mp01.html#agency-with-most-employees-per-year",
    "href": "mp01.html#agency-with-most-employees-per-year",
    "title": "NYC Payroll Data Analysis",
    "section": "4.5 ** Agency with Most Employees Per Year**",
    "text": "4.5 ** Agency with Most Employees Per Year**\nThe agency employing the most people in a given year.\n\n\nCode\nmost_employees_agency &lt;- payroll_data %&gt;%\n  group_by(fiscal_year, agency_name) %&gt;%\n  summarise(employee_count = n()) %&gt;%\n  arrange(fiscal_year, desc(employee_count)) %&gt;%\n  slice(1)\n\n\n`summarise()` has grouped output by 'fiscal_year'. You can override using the\n`.groups` argument.\n\n\nCode\nprint(most_employees_agency)\n\n\n# A tibble: 11 √ó 3\n# Groups:   fiscal_year [11]\n   fiscal_year agency_name            employee_count\n         &lt;dbl&gt; &lt;chr&gt;                           &lt;int&gt;\n 1        2014 Dept Of Ed Pedagogical         100589\n 2        2015 Dept Of Ed Pedagogical         111857\n 3        2016 Dept Of Ed Pedagogical         106263\n 4        2017 Dept Of Ed Pedagogical         104629\n 5        2018 Dept Of Ed Pedagogical         107956\n 6        2019 Dept Of Ed Pedagogical         112067\n 7        2020 Dept Of Ed Pedagogical         114999\n 8        2021 Dept Of Ed Pedagogical         113523\n 9        2022 Dept Of Ed Pedagogical         120453\n10        2023 Dept Of Ed Pedagogical         106882\n11        2024 Dept Of Ed Pedagogical         108209\n\n\nüìå Insight: The NYPD and Department of Education tend to have the largest workforces."
  },
  {
    "objectID": "mp01.html#agency-with-highest-overtime-usage",
    "href": "mp01.html#agency-with-highest-overtime-usage",
    "title": "NYC Payroll Data Analysis",
    "section": "4.6 ** Agency with Highest Overtime Usage**",
    "text": "4.6 ** Agency with Highest Overtime Usage**\nThe agency with the highest overtime usage relative to regular hours.\n\n\nCode\nhighest_overtime_agency &lt;- payroll_data %&gt;%\n  group_by(agency_name) %&gt;%\n  summarise(\n    total_ot_hours = sum(ot_hours, na.rm = TRUE),\n    total_reg_hours = sum(reg_hours, na.rm = TRUE),\n    ot_ratio = total_ot_hours / total_reg_hours\n  ) %&gt;%\n  arrange(desc(ot_ratio)) %&gt;%\n  slice(1)\n\nprint(highest_overtime_agency)\n\n\n# A tibble: 1 √ó 4\n  agency_name       total_ot_hours total_reg_hours ot_ratio\n  &lt;chr&gt;                      &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Board Of Election       3062029.       15339960.    0.200\n\n\nüìå Insight: Some agencies rely heavily on overtime rather than hiring more employees."
  },
  {
    "objectID": "mp01.html#average-salary-of-employees-outside-nyc",
    "href": "mp01.html#average-salary-of-employees-outside-nyc",
    "title": "NYC Payroll Data Analysis",
    "section": "4.7 ** Average Salary of Employees Outside NYC**",
    "text": "4.7 ** Average Salary of Employees Outside NYC**\nThe average salary of employees working outside the five boroughs.\n\n\nCode\noutside_five_boroughs_salary &lt;- payroll_data %&gt;%\n  filter(!borough %in% c(\"Bronx\", \"Brooklyn\", \"Manhattan\", \"Queens\", \"Staten Island\")) %&gt;%\n  summarise(avg_salary = mean(salary, na.rm = TRUE))\n\nprint(outside_five_boroughs_salary)\n\n\n# A tibble: 1 √ó 1\n  avg_salary\n       &lt;dbl&gt;\n1     53735.\n\n\nüìå Insight: Employees working outside NYC may have different pay structures."
  },
  {
    "objectID": "mp01.html#nyc-payroll-growth-over-10-years",
    "href": "mp01.html#nyc-payroll-growth-over-10-years",
    "title": "NYC Payroll Data Analysis",
    "section": "4.8 ** NYC Payroll Growth Over 10 Years**",
    "text": "4.8 ** NYC Payroll Growth Over 10 Years**\nTracking total payroll growth.\n\n\nCode\npayroll_growth &lt;- payroll_data %&gt;%\n  group_by(fiscal_year) %&gt;%\n  summarise(total_payroll = sum(salary + total_ot_paid + total_other_pay, na.rm = TRUE)) %&gt;%\n  arrange(fiscal_year)\n\nprint(payroll_growth)\n\n\n# A tibble: 11 √ó 2\n   fiscal_year total_payroll\n         &lt;dbl&gt;         &lt;dbl&gt;\n 1        2014  22638474550.\n 2        2015  25474766615.\n 3        2016  26544770463.\n 4        2017  27258714065.\n 5        2018  27965852639.\n 6        2019  29501782601.\n 7        2020  31981635725.\n 8        2021  31330019031.\n 9        2022  34887228899.\n10        2023  33319364876.\n11        2024  34700020886.\n\n\nCode\ndatatable(payroll_growth, options = list(pageLength = 5))\n\n\n\n\n\n\nCode\nggplot(payroll_growth, aes(x = fiscal_year, y = total_payroll)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(title = \"NYC Aggregate Payroll Growth Over 10 Years\",\n       x = \"Fiscal Year\",\n       y = \"Total Payroll ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nüìå Insight: NYC‚Äôs payroll costs have steadily increased over the last decade."
  },
  {
    "objectID": "mp01.html#policy-1-cap-high-salaries",
    "href": "mp01.html#policy-1-cap-high-salaries",
    "title": "NYC Payroll Data Analysis",
    "section": "5.1 Policy 1: Cap High Salaries",
    "text": "5.1 Policy 1: Cap High Salaries\nSome employees earn more than the Mayor‚Äôs salary. This policy aims to cap high earnings.\n\n\nCode\nmayor_salary &lt;- payroll_data %&gt;%\n  filter(title == \"Mayor\") %&gt;%\n  select(fiscal_year, total_compensation)\nhigh_salaries &lt;- payroll_data %&gt;%\n  inner_join(mayor_salary, by = \"fiscal_year\", suffix = c(\"_emp\", \"_mayor\")) %&gt;%\n  filter(total_compensation_emp &gt; total_compensation_mayor)\n\n\nWarning in inner_join(., mayor_salary, by = \"fiscal_year\", suffix = c(\"_emp\", : Detected an unexpected many-to-many relationship between `x` and `y`.\n‚Ñπ Row 17197 of `x` matches multiple rows in `y`.\n‚Ñπ Row 1 of `y` matches multiple rows in `x`.\n‚Ñπ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nCode\ntotal_savings &lt;- sum(high_salaries$total_compensation_emp - high_salaries$total_compensation_mayor, na.rm = TRUE)\ndatatable(\n  high_salaries %&gt;%\n    select(fiscal_year, first_name, last_name, title, agency_name, total_compensation_emp, total_compensation_mayor) %&gt;%\n    slice_head(n = 100),\n  options = list(scrollX = TRUE)\n)\n\n\n\n\n\n\n\n5.1.1 üí° Outcome:\n\nPotential savings if salaries above the Mayor‚Äôs pay are limited.\n\nüìå Political Feasibility: Moderate üü†\n‚úî Saves millions but faces opposition from high earners and unions."
  },
  {
    "objectID": "mp01.html#policy-2-hire-more-staff-to-reduce-overtime",
    "href": "mp01.html#policy-2-hire-more-staff-to-reduce-overtime",
    "title": "NYC Payroll Data Analysis",
    "section": "5.2 Policy 2: Hire More Staff to Reduce Overtime",
    "text": "5.2 Policy 2: Hire More Staff to Reduce Overtime\nExcessive overtime costs 1.5x regular wages. Instead, hiring more employees can reduce payroll expenses.\n\n5.2.1 ** Overtime Reduction Analysis**\nObjective: Identify how many full-time employees (FTEs) would be needed to replace current overtime hours.\n\n\nCode\novertime_reduction &lt;- payroll_data %&gt;%\n  group_by(agency_name, title) %&gt;%\n  summarize(\n    total_overtime_hours = sum(ot_hours, na.rm = TRUE),\n    full_time_equivalent_needed = total_overtime_hours / 2000\n  ) %&gt;%\n  arrange(desc(total_overtime_hours))\n\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n\nCode\ndatatable(overtime_reduction, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight: Agencies with high overtime hours may benefit from hiring additional staff instead of relying on overtime.\n\n\n5.2.2 ** Overtime Cost vs.¬†Regular Cost Savings**\nObjective: Calculate the cost difference between overtime pay and regular pay.\n\n\nCode\novertime_savings &lt;- payroll_data %&gt;%\n  group_by(agency_name, title) %&gt;%\n  summarize(\n    overtime_cost = sum(1.5 * salary * ot_hours, na.rm = TRUE),\n    regular_cost = sum(salary * (ot_hours / 40), na.rm = TRUE),\n    potential_savings = overtime_cost - regular_cost\n  ) %&gt;%\n  arrange(desc(potential_savings))\n\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n\nCode\ndatatable(overtime_savings, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight: Agencies paying excessive overtime could save significantly by hiring regular staff instead.\n\n\n5.2.3 ** Agency-Level Savings Calculation**\nObjective: Aggregate savings at the agency level to determine the most cost-effective changes.\n\n\nCode\nagency_savings &lt;- overtime_savings %&gt;%\n  group_by(agency_name) %&gt;%\n  summarize(\n    total_overtime_cost = sum(overtime_cost, na.rm = TRUE),\n    total_regular_cost = sum(regular_cost, na.rm = TRUE),\n    total_savings = sum(potential_savings, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(total_savings))\n\ndatatable(agency_savings, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight: This helps policymakers prioritize agencies where switching from overtime to regular staffing would have the largest financial impact.\nüìå Political Feasibility: High ‚úÖ\n‚úî Reduces costs, improves work-life balance.\n‚úñ Requires initial hiring costs."
  },
  {
    "objectID": "mp01.html#policy-3-reducing-non-essential-overtime-expanding-remote-work",
    "href": "mp01.html#policy-3-reducing-non-essential-overtime-expanding-remote-work",
    "title": "NYC Payroll Data Analysis",
    "section": "5.3 Policy 3: Reducing Non-Essential Overtime & Expanding Remote Work",
    "text": "5.3 Policy 3: Reducing Non-Essential Overtime & Expanding Remote Work"
  },
  {
    "objectID": "mp01.html#overview",
    "href": "mp01.html#overview",
    "title": "NYC Payroll Data Analysis",
    "section": "5.4 üìå Overview",
    "text": "5.4 üìå Overview\nThis policy explores strategies to reduce unnecessary overtime expenses and evaluate remote work opportunities.\nThe analysis consists of three parts:\n1Ô∏è‚É£ Identifying Non-Essential Overtime & Potential Savings\n2Ô∏è‚É£ Assessing Remote Work Eligibility\n3Ô∏è‚É£ Estimating New Hires Needed to Replace Overtime Hours\n\n5.4.1 ** Identifying Non-Essential Overtime & Potential Savings**\nObjective: Reduce overtime in administrative and support roles where excess hours are unnecessary.\n\n\nCode\nnon_essential_overtime &lt;- payroll_data %&gt;%\n  filter(title %in% c(\"Administrative Assistant\", \"Clerk\", \"Analyst\", \"IT Support\")) %&gt;%\n  group_by(agency_name, title) %&gt;%\n  summarize(\n    total_overtime_hours = sum(ot_hours, na.rm = TRUE),\n    overtime_cost = sum(1.5 * salary * ot_hours, na.rm = TRUE),\n    potential_savings = overtime_cost * 0.50\n  ) %&gt;%\n  arrange(desc(potential_savings))\n\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n\nCode\ndatatable(non_essential_overtime, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight:\nAdministrative and clerical jobs could cut overtime costs by half, saving millions in payroll expenses.\n\n\n5.4.2 ** Remote Work Eligibility**\nObjective: Determine how many employees work in remote-eligible job titles.\n\n\nCode\nremote_eligible &lt;- payroll_data %&gt;%\n  filter(title %in% c(\"IT Support\", \"Data Analyst\", \"Project Manager\", \"Accountant\")) %&gt;%\n  group_by(agency_name, title) %&gt;%\n  summarize(avg_salary = mean(salary, na.rm = TRUE), employees = n())\n\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n\nCode\ndatatable(remote_eligible, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight:\nEncouraging remote work for data-heavy and administrative roles reduces office space costs and lowers commute-driven overtime claims.\n\n\n5.4.3 ** Hiring New Employees to Replace Overtime Dependency**\nObjective: Identify how many full-time employees would be needed to replace existing overtime hours.\n\n\nCode\nnew_hires_needed &lt;- non_essential_overtime %&gt;%\n  mutate(full_time_equivalent_needed = total_overtime_hours / 2000)\ndatatable(new_hires_needed, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight:\nInstead of paying costly overtime, hiring additional full-time employees would reduce long-term payroll costs while improving work-life balance.\nüìä Final Recommendations:\n‚úî Reduce overtime in non-essential roles like Clerks & Admin Assistants‚Äî50% reduction could save millions.\n‚úî Expand remote work for IT, Analysts, and Project Managers to reduce office space and commute-driven overtime.\n‚úî Hire full-time employees to replace reliance on overtime in high-workload agencies."
  },
  {
    "objectID": "mp03.html#task-2-import-playlist-dataset",
    "href": "mp03.html#task-2-import-playlist-dataset",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "Task 2: Import Playlist Dataset",
    "text": "Task 2: Import Playlist Dataset\nWe responsibly download and combine all JSON playlist slices into a single list for future processing.\n\n\nCode\nload_playlists &lt;- function() {\n  library(jsonlite)\n  library(purrr)\n  \n  dir_path &lt;- \"data/mp03/data1\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n  \n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  starts &lt;- seq(0, 999000, by = 1000)\n  file_names &lt;- sprintf(\"mpd.slice.%d-%d.json\", starts, starts + 999)\n  file_paths &lt;- file.path(dir_path, file_names)\n  \n  for (i in seq_along(file_names)) {\n    if (!file.exists(file_paths[i])) {\n      url &lt;- paste0(base_url, file_names[i])\n      tryCatch({\n        download.file(url, destfile = file_paths[i], mode = \"wb\", timeout = 300)\n      }, error = function(e) {\n        message(\"‚ö†Ô∏è Failed to download: \", file_names[i])\n      })\n    }\n  }\n\n  read_playlist_file &lt;- function(path) {\n    tryCatch(\n      fromJSON(path)$playlists,\n      error = function(e) {\n        message(\"‚ùå Skipping corrupted file: \", path)\n        return(NULL)\n      }\n    )\n  }\n\n  valid_paths &lt;- file_paths[file.exists(file_paths)]\n  playlists_list &lt;- map(valid_paths, read_playlist_file)\n  playlists_list &lt;- compact(playlists_list)\n  \n  return(playlists_list)\n}\n\n# Load + flatten\nPLAYLISTS_LIST &lt;- load_playlists()\nall_playlists &lt;- PLAYLISTS_LIST %&gt;% list_rbind()\nDT::datatable(\n  head(all_playlists, 10),\n  options = list(\n    pageLength = 6,\n    dom = 'tip',\n    scrollX = TRUE\n  ),\n  class = \"display compact stripe hover\",\n  rownames = FALSE\n)"
  },
  {
    "objectID": "mp03.html#task-3-rectify-playlist-data-to-track-level-format",
    "href": "mp03.html#task-3-rectify-playlist-data-to-track-level-format",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéº Task 3: Rectify Playlist Data to Track-Level Format",
    "text": "üéº Task 3: Rectify Playlist Data to Track-Level Format\nWe flatten the hierarchical playlist JSONs into a clean, rectangular track-level format, stripping unnecessary prefixes and standardizing column names.\n\n\nCode\n# Helper to clean Spotify URI prefixes\nstrip_spotify_prefix &lt;- function(x){\n  str_extract(x, \".*:.*:(.*)\")\n}\n\n# Flatten and clean playlist data into track-level format\nrectified_data &lt;- all_playlists %&gt;%\n  select(\n    playlist_name = name,\n    playlist_id = pid,\n    playlist_followers = num_followers,\n    tracks\n  ) %&gt;%\n  unnest(tracks) %&gt;%\n  mutate(\n    playlist_position = row_number(),\n    artist_name = map_chr(artist_name, 1, .default = NA_character_),\n    artist_id = strip_spotify_prefix(artist_uri),\n    track_name = track_name,\n    track_id = strip_spotify_prefix(track_uri),\n    album_name = album_name,\n    album_id = strip_spotify_prefix(album_uri),\n    duration = duration_ms\n  ) %&gt;%\n  select(\n    playlist_name, playlist_id, playlist_position, playlist_followers,\n    artist_name, artist_id, track_name, track_id,\n    album_name, album_id, duration\n  )\n# Preview the Spotify-themed table\nspotify_table(head(rectified_data, 10))\n\n\n\n\n\n\nplaylist_name\nplaylist_id\nplaylist_position\nplaylist_followers\nartist_name\nartist_id\ntrack_name\ntrack_id\nalbum_name\nalbum_id\nduration\n\n\n\n\nThrowbacks\n0\n1\n1\nMissy Elliott\nspotify:artist:2wIVse2owClT7go1WT98tk\nLose Control (feat. Ciara & Fat Man Scoop)\nspotify:track:0UaMYEvWZi0ZqiDOoHU3YI\nThe Cookbook\nspotify:album:6vV5UrXcfyQD1wu4Qo2I9K\n226863\n\n\nThrowbacks\n0\n2\n1\nBritney Spears\nspotify:artist:26dSoYclwsYLMAKD3tpOr4\nToxic\nspotify:track:6I9VzXrHxO9rA9A5euc8Ak\nIn The Zone\nspotify:album:0z7pVBGOD7HCIB7S8eLkLI\n198800\n\n\nThrowbacks\n0\n3\n1\nBeyonc√©\nspotify:artist:6vWDO969PvNqNYHIOW5v0m\nCrazy In Love\nspotify:track:0WqIKmW4BTrj3eJFmnCKMv\nDangerously In Love (Alben f√ºr die Ewigkeit)\nspotify:album:25hVFAxTlDvXbx2X2QkUkE\n235933\n\n\nThrowbacks\n0\n4\n1\nJustin Timberlake\nspotify:artist:31TPClRtHm23RisEBtV3X7\nRock Your Body\nspotify:track:1AWQoqb9bSvzTjaLralEkT\nJustified\nspotify:album:6QPkyl04rXwTGlGlcYaRoW\n267266\n\n\nThrowbacks\n0\n5\n1\nShaggy\nspotify:artist:5EvFsr3kj42KNv97ZEnqij\nIt Wasn't Me\nspotify:track:1lzr43nnXAijIGYnCT8M8H\nHot Shot\nspotify:album:6NmFmPX56pcLBOFMhIiKvF\n227600\n\n\nThrowbacks\n0\n6\n1\nUsher\nspotify:artist:23zg3TcAtWQy7J6upgbUnj\nYeah!\nspotify:track:0XUfyU2QviPAs6bxSpXYG4\nConfessions\nspotify:album:0vO0b1AvY49CPQyVisJLj0\n250373\n\n\nThrowbacks\n0\n7\n1\nUsher\nspotify:artist:23zg3TcAtWQy7J6upgbUnj\nMy Boo\nspotify:track:68vgtRHr7iZHpzGpon6Jlo\nConfessions\nspotify:album:1RM6MGv6bcl6NrAG8PGoZk\n223440\n\n\nThrowbacks\n0\n8\n1\nThe Pussycat Dolls\nspotify:artist:6wPhSqRtPu1UhRCDX5yaDJ\nButtons\nspotify:track:3BxWKCI06eQ5Od8TY2JBeA\nPCD\nspotify:album:5x8e8UcCeOgrOzSnDGuPye\n225560\n\n\nThrowbacks\n0\n9\n1\nDestiny's Child\nspotify:artist:1Y8cdNmUJH7yBTd9yOvr5i\nSay My Name\nspotify:track:7H6ev70Weq6DdpZyyTmUXk\nThe Writing's On The Wall\nspotify:album:283NWqNsCA9GwVHrJk59CG\n271333\n\n\nThrowbacks\n0\n10\n1\nOutKast\nspotify:artist:1G9G7WwrXka3Z1r7aIDjI7\nHey Ya! - Radio Mix / Club Mix\nspotify:track:2PpruBYCo4H7WOBJ7Q2EwM\nSpeakerboxxx/The Love Below\nspotify:album:1UsmQ3bpJTyK6ygoOOjG1r\n235213"
  },
  {
    "objectID": "mp03.html#task-4-initial-exploration-of-track-playlist-data",
    "href": "mp03.html#task-4-initial-exploration-of-track-playlist-data",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéß Task 4: Initial Exploration of Track & Playlist Data",
    "text": "üéß Task 4: Initial Exploration of Track & Playlist Data\nThis section investigates core statistics of the combined playlist + song characteristics data set.\n\n\nCode\nstrip_spotify_prefix &lt;- function(x){\n  stringr::str_replace(x, \"spotify:track:\", \"\")\n}\n\nrectified_data &lt;- rectified_data %&gt;%\n  mutate(track_id = strip_spotify_prefix(track_id)) %&gt;%\n  filter(!is.na(track_id) & track_id != \"\")\n\nSONGS &lt;- SONGS %&gt;%\n  filter(!is.na(id) & id != \"\")\n\njoined_data &lt;- inner_join(rectified_data, SONGS, by = c(\"track_id\" = \"id\"))\n\n\n\nüéµ Q1: How many distinct tracks and artists?\n\n\nCode\ndistinct_tracks &lt;- joined_data %&gt;% distinct(track_id) %&gt;% nrow()\ndistinct_artists &lt;- joined_data %&gt;% distinct(artist_id) %&gt;% nrow()\n\nspotify_table(\n  tibble(Metric = c(\"Distinct Tracks\", \"Distinct Artists\"),\n         Count = c(distinct_tracks, distinct_artists))\n)\n\n\n\n\n\n\nMetric\nCount\n\n\n\n\nDistinct Tracks\n50684\n\n\nDistinct Artists\n9609\n\n\n\n\n\n\n\n\nüìù Analysis: The dataset contains a rich collection of unique tracks and artists, showcasing Spotify‚Äôs extensive catalog diversity across user playlists.\n\n\nüî• Q2: What are the 5 most common tracks?\n\n\nCode\ntop_tracks &lt;- joined_data %&gt;%\n  group_by(track_name) %&gt;%\n  summarise(Appearances = n(), .groups = \"drop\") %&gt;%\n  arrange(desc(Appearances)) %&gt;%\n  slice_head(n = 5)\n\nspotify_table(top_tracks)\n\n\n\n\n\n\ntrack_name\nAppearances\n\n\n\n\nChampions\n27888\n\n\nNo Problem (feat. Lil Wayne & 2 Chainz)\n26826\n\n\nCloser\n25742\n\n\nF**kin' Problems\n25136\n\n\nSucker For Pain (with Wiz Khalifa, Imagine Dragons, Logic & Ty Dolla $ign feat. X Ambassadors)\n25086\n\n\n\n\n\n\n\n\nüìù Analysis: The most frequently appearing songs offer insight into widely loved and repeat-worthy tracks across millions of playlists.\n\n\n‚ùì Q3: Most Popular Track Not in SONGS\n\n\nCode\nmissing_tracks &lt;- rectified_data %&gt;%\n  filter(!(track_id %in% SONGS$id)) %&gt;%\n  group_by(track_name, track_id) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice_head(n = 1)\n\nspotify_table(missing_tracks)\n\n\n\n\n\n\ntrack_name\ntrack_id\ncount\n\n\n\n\nOne Dance\n1xznGGDReH1oQq0xzbwXa3\n12094\n\n\n\n\n\n\n\n\nüìù Analysis: This track, though highly featured on playlists, is not captured in the SONGS dataset, suggesting data lags or catalog discrepancies.\n\n\nüíÉ Q4: Most Danceable Track\n\n\nCode\nmost_danceable &lt;- SONGS %&gt;% arrange(desc(danceability)) %&gt;% slice_head(n = 1)\n\ndanceable_count &lt;- rectified_data %&gt;%\n  filter(track_id == most_danceable$id) %&gt;%\n  nrow()\n\nspotify_table(most_danceable %&gt;% \n  select(name, artist, danceability, popularity) %&gt;% \n  mutate(`# of Playlists` = danceable_count))\n\n\n\n\n\n\nname\nartist\ndanceability\npopularity\n# of Playlists\n\n\n\n\nFunky Cold Medina\nTone-Loc\n0.988\n57\n209\n\n\n\n\n\n\n\n\nüìù Analysis: With high danceability and moderate popularity, this track captures rhythmic excellence while still being somewhat niche.\n\n\n‚è±Ô∏è Q5: Playlist with Longest Average Track Duration\n\n\nCode\nlongest_avg_playlist &lt;- joined_data %&gt;%\n  group_by(playlist_name, playlist_id) %&gt;%\n  summarise(avg_duration = mean(duration, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(avg_duration)) %&gt;%\n  slice_head(n = 1)\n\nlongest_avg_playlist %&gt;%\n  mutate(avg_duration_min = round(avg_duration / 60000, 2)) %&gt;%\n  select(playlist_name, playlist_id, avg_duration_min) %&gt;%\n  spotify_table()\n\n\n\n\n\n\nplaylist_name\nplaylist_id\navg_duration_min\n\n\n\n\nSleep\n611205\n68.67\n\n\n\n\n\n\n\n\nüìù Analysis: This playlist favors longer-form listening experiences‚Äîperfect for chill or storytelling-heavy sessions.\n\n\n‚≠ê Q6: Most Followed Playlist\n\n\nCode\nmost_followed &lt;- joined_data %&gt;%\n  select(playlist_id, playlist_name, playlist_followers) %&gt;%\n  distinct() %&gt;%\n  arrange(desc(playlist_followers)) %&gt;%\n  slice_head(n = 1)\n\nspotify_table(most_followed)\n\n\n\n\n\n\nplaylist_id\nplaylist_name\nplaylist_followers\n\n\n\n\n746359\nBreaking Bad\n53519\n\n\n\n\n\n\n\n\nüìù Analysis: High follower count reflects strong user trust and playlist curation quality‚Äîthese often become global listening staples."
  },
  {
    "objectID": "mp03.html#task-5-visually-identifying-characteristics-of-popular-songs",
    "href": "mp03.html#task-5-visually-identifying-characteristics-of-popular-songs",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéß Task 5: Visually Identifying Characteristics of Popular Songs",
    "text": "üéß Task 5: Visually Identifying Characteristics of Popular Songs\nWe explore audio features to discover what makes songs popular, including trends over time, genre markers, and playlist impact.\n\n\nüìà Q1: Is Popularity Correlated with Playlist Appearances?\n\n\nCode\ntrack_popularity &lt;- joined_data %&gt;%\n  group_by(track_id, name, popularity) %&gt;%\n  summarise(playlist_appearances = n(), .groups = \"drop\")\n\nggplot(track_popularity, aes(x = playlist_appearances, y = popularity)) +\n  geom_point(alpha = 0.3, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"white\") +\n  labs(\n    title = \"Popularity vs Playlist Appearances\",\n    x = \"Playlist Appearances\",\n    y = \"Popularity\"\n  ) +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nLight positive correlation shows that more playlist exposure often means higher popularity.\n\n\nüìÖ Q2: When Were Popular Songs Released?\n\n\nCode\njoined_data %&gt;%\n  filter(popularity &gt;= 70, !is.na(year)) %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n)) +\n  geom_col(fill = \"#1DB954\") +\n  labs(title = \"Release Year of Popular Songs\", x = \"Year\", y = \"Count\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nRecent years dominate, with 2010s releases most represented.\n\n\nüíÉ Q3: When Did Danceability Peak?\n\n\nCode\njoined_data %&gt;%\n  group_by(year) %&gt;%\n  summarise(avg_danceability = mean(danceability, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = year, y = avg_danceability)) +\n  geom_line(color = \"#F1C40F\", linewidth = 1.2) +\n  labs(title = \"Danceability Over Time\", x = \"Year\", y = \"Average Danceability\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nPost-2010s songs tend to be more danceable.\n\n\nüìÄ Q4: Most Represented Decade\n\n\nCode\njoined_data %&gt;%\n  mutate(decade = (year %/% 10) * 10) %&gt;%\n  count(decade) %&gt;%\n  ggplot(aes(x = as.factor(decade), y = n)) +\n  geom_col(fill = \"#3498DB\") +\n  labs(title = \"Songs by Decade\", x = \"Decade\", y = \"Number of Tracks\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\n2010s clearly dominate user playlists.\n\n\nüéπ Q5: Key Frequency (Polar Plot)\n\n\nCode\njoined_data %&gt;%\n  count(key) %&gt;%\n  mutate(key = as.factor(key)) %&gt;%\n  ggplot(aes(x = key, y = n)) +\n  geom_col(fill = \"#8E44AD\") +\n  coord_polar() +\n  labs(title = \"Distribution of Musical Keys\", x = \"Key\", y = \"Count\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nKeys are spread fairly evenly with a few dominant ones.\n\n\n‚è±Ô∏è Q6: Most Common Track Lengths\n\n\nCode\njoined_data %&gt;%\n  mutate(duration_min = duration / 60000) %&gt;%\n  ggplot(aes(x = duration_min)) +\n  geom_histogram(binwidth = 0.5, fill = \"#E67E22\", color = \"black\") +\n  labs(title = \"Track Duration Distribution\", x = \"Duration (minutes)\", y = \"Count\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nMost songs fall between 2.5 to 4 minutes‚Äîideal for streaming.\n\n\nüéº Q7: Tempo vs Danceability (Popular Songs)\n\n\nCode\npopular_songs &lt;- joined_data %&gt;% \n  filter(popularity &gt;= 70)\n\ncor_val &lt;- cor(popular_songs$tempo, popular_songs$danceability, use = \"complete.obs\")\n\nggplot(popular_songs, aes(x = tempo, y = danceability)) +\n  geom_point(alpha = 0.4, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"white\") +\n  labs(\n    title = \"Tempo vs Danceability (Popular Songs)\",\n    subtitle = paste0(\"Correlation: \", round(cor_val, 2)),\n    x = \"Tempo (BPM)\",\n    y = \"Danceability\"\n  ) +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nFaster songs tend to be a bit more danceable.\n\n\nüìä Q8: Playlist Followers vs Avg. Popularity\n\n\nCode\nfollowers_vs_popularity &lt;- joined_data %&gt;%\n  group_by(playlist_id, playlist_name, playlist_followers) %&gt;%\n  summarise(avg_popularity = mean(popularity, na.rm = TRUE), .groups = \"drop\")\n\ncor_val &lt;- cor(log1p(followers_vs_popularity$playlist_followers), \n               followers_vs_popularity$avg_popularity, use = \"complete.obs\")\n\nggplot(followers_vs_popularity, aes(x = playlist_followers, y = avg_popularity)) +\n  geom_point(alpha = 0.2, size = 1.2, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"white\") +\n  scale_x_log10() +\n  labs(\n    title = \"Followers vs. Avg. Popularity\",\n    subtitle = paste0(\"Correlation: \", round(cor_val, 2)),\n    x = \"Followers (log scale)\",\n    y = \"Average Popularity\"\n  ) +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nHeavily followed playlists do tend to feature more popular tracks."
  },
  {
    "objectID": "mp03.html#task-6-finding-related-songs",
    "href": "mp03.html#task-6-finding-related-songs",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üîç Task 6: Finding Related Songs",
    "text": "üîç Task 6: Finding Related Songs\nWe now build a playlist around two anchor tracks ‚Äî Drop The World and No Role Modelz ‚Äî using five custom heuristics to find compatible songs across tempo, mood, popularity, and year.\n\n\nüéµ Step 1: Identify Anchor Tracks\n\n\nCode\nanchor_names &lt;- c(\"Drop The World\", \"No Role Modelz\")\npopular_threshold &lt;- 70\n\nanchor_tracks &lt;- joined_data %&gt;%\n  filter(track_name %in% anchor_names)\n\ncat(\"üéµ Anchor Songs Found:\", nrow(anchor_tracks), \"\\n\")\n\n\nüéµ Anchor Songs Found: 11902 \n\n\n\n\nüéß Heuristic 1: Co-occurring Songs in a Random Playlist\n\n\nCode\nboth_anchors_playlists &lt;- joined_data %&gt;%\n  filter(track_name %in% anchor_names) %&gt;%\n  group_by(playlist_id) %&gt;%\n  summarise(anchor_count = n()) %&gt;%\n  filter(anchor_count &gt;= 2) %&gt;%\n  pull(playlist_id)\n\nset.seed(1010)\nchosen_id &lt;- sample(both_anchors_playlists, 1)\n\nco_occurring &lt;- joined_data %&gt;%\n  filter(playlist_id == chosen_id, !(track_name %in% anchor_names)) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéß Heuristic 1 - Playlist\", chosen_id, \"‚Üí\", nrow(co_occurring), \"tracks found\\n\")\n\n\nüéß Heuristic 1 - Playlist 974361 ‚Üí 97 tracks found\n\n\nSongs commonly grouped with our anchors by real Spotify users.\n\n\nüéöÔ∏è Heuristic 2: Similar Tempo & Key\n\n\nCode\ntempo_key_match &lt;- joined_data %&gt;%\n  filter(\n    key %in% anchor_tracks$key,\n    abs(tempo - mean(anchor_tracks$tempo, na.rm = TRUE)) &lt;= 5,\n    !(track_name %in% anchor_names)\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéöÔ∏è Heuristic 2 - Tempo/Key:\", nrow(tempo_key_match), \"matches\\n\")\n\n\nüéöÔ∏è Heuristic 2 - Tempo/Key: 829 matches\n\n\nThese tracks are musically smooth transitions for DJs.\n\n\nüßë‚Äçüé§ Heuristic 3: Same Artist\n\n\nCode\nsame_artist &lt;- joined_data %&gt;%\n  filter(artist_name %in% anchor_tracks$artist_name, !(track_name %in% anchor_names)) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üßë‚Äçüé§ Heuristic 3 - Same Artist:\", nrow(same_artist), \"matches\\n\")\n\n\nüßë‚Äçüé§ Heuristic 3 - Same Artist: 92 matches\n\n\nCurating songs from Eminem, J. Cole, or Lil Wayne‚Äôs discographies.\n\n\nüéõÔ∏è Heuristic 4: Acoustic / Energy Profile Match\n\n\nCode\nanchor_year &lt;- unique(anchor_tracks$year)\n\nacoustic_features &lt;- joined_data %&gt;%\n  filter(year %in% anchor_year, !(track_name %in% anchor_names)) %&gt;%\n  mutate(sim_score = abs(danceability - mean(anchor_tracks$danceability, na.rm = TRUE)) +\n           abs(energy - mean(anchor_tracks$energy, na.rm = TRUE)) +\n           abs(acousticness - mean(anchor_tracks$acousticness, na.rm = TRUE))) %&gt;%\n  arrange(sim_score) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  slice_head(n = 20)\n\ncat(\"üéõÔ∏è Heuristic 4 - Acoustic Profile:\", nrow(acoustic_features), \"best matches\\n\")\n\n\nüéõÔ∏è Heuristic 4 - Acoustic Profile: 20 best matches\n\n\nTunes that ‚Äúfeel‚Äù similar to our anchors in vibe and intensity.\n\n\nüéöÔ∏è Heuristic 5: Valence & Loudness\n\n\nCode\nvalence_match &lt;- joined_data %&gt;%\n  filter(\n    abs(valence - mean(anchor_tracks$valence, na.rm = TRUE)) &lt; 0.1,\n    abs(loudness - mean(anchor_tracks$loudness, na.rm = TRUE)) &lt; 2,\n    !(track_name %in% anchor_names)\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéöÔ∏è Heuristic 5 - Valence + Loudness:\", nrow(valence_match), \"\\n\")\n\n\nüéöÔ∏è Heuristic 5 - Valence + Loudness: 4239 \n\n\nFor emotional and volume consistency in listening flow.\n\n\nüéº Combine Playlist Candidates\n\n\nCode\nfinal_playlist &lt;- bind_rows(\n  co_occurring,\n  tempo_key_match,\n  same_artist,\n  acoustic_features,\n  valence_match\n) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  mutate(popular = popularity &gt;= popular_threshold)\n\ncat(\"üéº Final Playlist Candidates:\", nrow(final_playlist), \"\\n\")\n\n\nüéº Final Playlist Candidates: 5157 \n\n\nCode\ncat(\"üìâ Non-popular (&lt;\", popular_threshold, \"):\", sum(!final_playlist$popular), \"\\n\")\n\n\nüìâ Non-popular (&lt; 70 ): 4918 \n\n\n\n\nüìã Preview of Final Playlist Candidates\n\n\nCode\nfinal_playlist %&gt;%\n  select(track_name, artist_name, popularity, playlist_name) %&gt;%\n  distinct() %&gt;%\n  slice_head(n = 20) %&gt;%\n  spotify_table(\"üéß Top 20 Playlist Candidates Based on 5 Heuristics\")\n\n\n\n\nüéß Top 20 Playlist Candidates Based on 5 Heuristics\n\n\ntrack_name\nartist_name\npopularity\nplaylist_name\n\n\n\n\nIgnition - Remix\nR. Kelly\n70\nthrowback\n\n\nSure Thing\nMiguel\n74\nthrowback\n\n\nPower Trip\nJ. Cole\n72\nthrowback\n\n\nWhatever You Like\nT.I.\n74\nthrowback\n\n\nCrooked Smile\nJ. Cole\n69\nthrowback\n\n\nSo Good\nB.o.B\n65\nthrowback\n\n\nRich As Fuck\nLil Wayne\n62\nthrowback\n\n\nYoung, Wild & Free (feat. Bruno Mars) - feat. Bruno Mars\nSnoop Dogg\n65\nthrowback\n\n\nStrange Clouds (feat. Lil Wayne) - feat. Lil Wayne\nB.o.B\n60\nthrowback\n\n\nThe Motto\nDrake\n72\nthrowback\n\n\nBattle Scars\nLupe Fiasco\n70\nthrowback\n\n\nThe Show Goes On\nLupe Fiasco\n71\nthrowback\n\n\nMercy\nKanye West\n71\nthrowback\n\n\nSatellites\nKevin Gates\n46\nthrowback\n\n\nLove Me\nLil Wayne\n66\nthrowback\n\n\nNo Hands (feat. Roscoe Dash and Wale) - Explicit Album Version\nWaka Flocka Flame\n75\nthrowback\n\n\nLollipop\nLil Wayne\n70\nthrowback\n\n\nRock Your Body\nJustin Timberlake\n71\nthrowback\n\n\nBeautiful Girls\nSean Kingston\n78\nthrowback\n\n\nA Milli\nLil Wayne\n72\nthrowback"
  },
  {
    "objectID": "mp03.html#task-7-curate-and-analyze-your-ultimate-playlist-hustle-heart",
    "href": "mp03.html#task-7-curate-and-analyze-your-ultimate-playlist-hustle-heart",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéß Task 7: Curate and Analyze Your Ultimate Playlist ‚Äì ‚ÄúHustle & Heart‚Äù",
    "text": "üéß Task 7: Curate and Analyze Your Ultimate Playlist ‚Äì ‚ÄúHustle & Heart‚Äù\n\nTwelve tracks. One vibe. Built from raw energy, emotional drive, and underdog spirit. Featuring rap heavyweights, slept-on gems, and genre-bending transitions, ‚ÄúHustle & Heart‚Äù was crafted using 5 analytical heuristics and a whole lot of gut.\n\n\n\n\n\nüé∂ Evolution of Audio Features in ‚ÄòHustle & Heart‚Äô Playlist"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "The Ultimate Playlist - Hustle & Heart üé∂",
    "section": "",
    "text": "From millions of Spotify tracks and playlists, Hustle & Heart emerges as a curated sound journey built on energy, emotion, and authenticity. This project explores what makes songs stick ‚Äî analyzing popularity, danceability, and musical DNA ‚Äî before distilling it all into a final 12-track playlist that hits with both data and vibe.\n\n üé∂ Just here for the playlist? Tap here"
  },
  {
    "objectID": "mp03.html#setup-load-install-required-packages",
    "href": "mp03.html#setup-load-install-required-packages",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "",
    "text": "Code\nensure_package &lt;- function(pkg){\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, repos = \"https://cloud.r-project.org\")\n  }\n  library(pkg, character.only = TRUE)\n}\n\nrequired_packages &lt;- c(\n  \"dplyr\", \"stringr\", \"tidyr\", \"purrr\", \"readr\", \"jsonlite\",\n  \"ggplot2\", \"scales\", \"DT\", \"rvest\", \"httr2\", \"tibble\"\n)\n\ninvisible(lapply(required_packages, ensure_package))\n\noptions(dplyr.summarise.inform = FALSE)"
  },
  {
    "objectID": "mp03.html#task-7-curate-and-analyze-your-ultimate-playlist-hustle-heart-playlist",
    "href": "mp03.html#task-7-curate-and-analyze-your-ultimate-playlist-hustle-heart-playlist",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéß Task 7: Curate and Analyze Your Ultimate Playlist ‚Äì ‚ÄúHustle & Heart‚Äù (#playlist)",
    "text": "üéß Task 7: Curate and Analyze Your Ultimate Playlist ‚Äì ‚ÄúHustle & Heart‚Äù (#playlist)\n\nTwelve tracks. One vibe. Built from raw energy, emotional drive, and underdog spirit. Featuring rap heavyweights, slept-on gems, and genre-bending transitions, ‚ÄúHustle & Heart‚Äù was crafted using 5 analytical heuristics and a whole lot of gut.\n\n\n\n\n\nüé∂ Evolution of Audio Features in ‚ÄòHustle & Heart‚Äô Playlist\n\n\n\n\n\nüìª Track-by-Track Commentary with Spotify Preview\n\n1 . Power Trip ‚Äî J. Cole\n\n\n\n\n2 . Crooked Smile ‚Äî J. Cole üìâ Hidden Gem\n\n\n\n\n3 . Young, Wild & Free (feat. Bruno Mars) - feat. Bruno Mars ‚Äî Snoop Dogg üìâ Hidden Gem\n\n\n\n\n4 . Battle Scars ‚Äî Lupe Fiascoüß† New Discovery\n\n\n\n\n5 . Mercy ‚Äî Kanye Westüß† New Discovery\n\n\n\n\n6 . Love Me ‚Äî Lil Wayne üìâ Hidden Gem\n\n\n\n\n7 . Lollipop ‚Äî Lil Wayne\n\n\n\n\n8 . Rock Your Body ‚Äî Justin Timberlake\n\n\n\n\n9 . Beautiful Girls ‚Äî Sean Kingston\n\n\n\n\n10 . A Milli ‚Äî Lil Wayne\n\n\n\n\n11 . Beautiful Girls ‚Äî Van Halen üìâ Hidden Gem\n\n\n\n\n12 . Battle Scars ‚Äî Paradise Fearsüß† New Discovery üìâ Hidden Gem"
  },
  {
    "objectID": "mp03.html#playlist",
    "href": "mp03.html#playlist",
    "title": "The Ultimate Playlist - Hustle & Heart üé∂",
    "section": "Hustle and Heart üéß",
    "text": "Hustle and Heart üéß\n\nüß† Note: While most tracks in Hustle & Heart were selected using a data-driven similarity score, two foundational songs ‚Äî ‚ÄúDrop the World‚Äù and ‚ÄúNo Role Modelz‚Äù ‚Äî were manually included as thematic anchors due to their lyrical intensity and motivational energy as they were included in data but was dropped down during popularity ranking.\n\nClick ‚ñ∂Ô∏è and enjoy the full curated soundtrack ‚Äî no skips, no scrolls. üî•"
  },
  {
    "objectID": "mp03.html#anchor-tracks-youtube-preview",
    "href": "mp03.html#anchor-tracks-youtube-preview",
    "title": "The Ultimate Playlist - Hustle & Heart üé∂",
    "section": "üé¨ Anchor Tracks ‚Äì YouTube Preview",
    "text": "üé¨ Anchor Tracks ‚Äì YouTube Preview\nThese tracks defined the tone of Hustle & Heart. Watch their official drops below. üëá\n\nDrop the world- By Lil Wayne and eminem\n\n\n\n\nNo role modelz- J.Cole\n\n\n\n\nüéß Heuristic 1: Co-occurring Songs in a Random Playlist\n\n\nCode\nboth_anchors_playlists &lt;- joined_data %&gt;%\n  filter(track_name %in% anchor_names) %&gt;%\n  group_by(playlist_id) %&gt;%\n  summarise(anchor_count = n()) %&gt;%\n  filter(anchor_count &gt;= 2) %&gt;%\n  pull(playlist_id)\n\nset.seed(1010)\nchosen_id &lt;- sample(both_anchors_playlists, 1)\n\nco_occurring &lt;- joined_data %&gt;%\n  filter(playlist_id == chosen_id, !(track_name %in% anchor_names)) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéß Heuristic 1 - Playlist\", chosen_id, \"‚Üí\", nrow(co_occurring), \"tracks found\\n\")\n\n\nüéß Heuristic 1 - Playlist 974361 ‚Üí 97 tracks found\n\n\nüéß Heuristic 1 applied to Playlist 974361 yielded 97 closely related track candidates based on shared playlist co-occurrence.\n\n\nüéöÔ∏è Heuristic 2: Similar Tempo & Key\n\n\nCode\ntempo_key_match &lt;- joined_data %&gt;%\n  filter(\n    key %in% anchor_tracks$key,\n    abs(tempo - mean(anchor_tracks$tempo, na.rm = TRUE)) &lt;= 5,\n    !(track_name %in% anchor_names)\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéöÔ∏è Heuristic 2 - Tempo/Key:\", nrow(tempo_key_match), \"matches\\n\")\n\n\nüéöÔ∏è Heuristic 2 - Tempo/Key: 829 matches\n\n\nThese tracks are musically smooth transitions for DJs.\n\n\nüßë‚Äçüé§ Heuristic 3: Same Artist\n\n\nCode\nsame_artist &lt;- joined_data %&gt;%\n  filter(artist_name %in% anchor_tracks$artist_name, !(track_name %in% anchor_names)) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üßë‚Äçüé§ Heuristic 3 - Same Artist:\", nrow(same_artist), \"matches\\n\")\n\n\nüßë‚Äçüé§ Heuristic 3 - Same Artist: 92 matches\n\n\nCurating songs from Eminem, J. Cole, or Lil Wayne‚Äôs discographies.\n\n\nüéõÔ∏è Heuristic 4: Acoustic / Energy Profile Match\n\n\nCode\nanchor_year &lt;- unique(anchor_tracks$year)\n\nacoustic_features &lt;- joined_data %&gt;%\n  filter(year %in% anchor_year, !(track_name %in% anchor_names)) %&gt;%\n  mutate(sim_score = abs(danceability - mean(anchor_tracks$danceability, na.rm = TRUE)) +\n           abs(energy - mean(anchor_tracks$energy, na.rm = TRUE)) +\n           abs(acousticness - mean(anchor_tracks$acousticness, na.rm = TRUE))) %&gt;%\n  arrange(sim_score) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  slice_head(n = 20)\n\ncat(\"üéõÔ∏è Heuristic 4 - Acoustic Profile:\", nrow(acoustic_features), \"best matches\\n\")\n\n\nüéõÔ∏è Heuristic 4 - Acoustic Profile: 20 best matches\n\n\nTunes that ‚Äúfeel‚Äù similar to our anchors in vibe and intensity.\n\n\nüéöÔ∏è Heuristic 5: Valence & Loudness\n\n\nCode\nvalence_match &lt;- joined_data %&gt;%\n  filter(\n    abs(valence - mean(anchor_tracks$valence, na.rm = TRUE)) &lt; 0.1,\n    abs(loudness - mean(anchor_tracks$loudness, na.rm = TRUE)) &lt; 2,\n    !(track_name %in% anchor_names)\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéöÔ∏è Heuristic 5 - Valence + Loudness:\", nrow(valence_match), \"\\n\")\n\n\nüéöÔ∏è Heuristic 5 - Valence + Loudness: 4239 \n\n\nFor emotional and volume consistency in listening flow.\n\n\nüéº Combine Playlist Candidates\n\n\nCode\nfinal_playlist &lt;- bind_rows(\n  co_occurring,\n  tempo_key_match,\n  same_artist,\n  acoustic_features,\n  valence_match\n) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  mutate(popular = popularity &gt;= popular_threshold)\n\ncat(\"üéº Final Playlist Candidates:\", nrow(final_playlist), \"\\n\")\n\n\nüéº Final Playlist Candidates: 5157 \n\n\nCode\ncat(\"üìâ Non-popular (&lt;\", popular_threshold, \"):\", sum(!final_playlist$popular), \"\\n\")\n\n\nüìâ Non-popular (&lt; 70 ): 4918 \n\n\n\n\nüìã Preview of Final Playlist Candidates\n\n\nCode\nfinal_playlist %&gt;%\n  select(track_name, artist_name, popularity, playlist_name) %&gt;%\n  distinct() %&gt;%\n  slice_head(n = 20) %&gt;%\n  spotify_table(\"üéß Top 20 Playlist Candidates Based on 5 Heuristics\")\n\n\n\n\nüéß Top 20 Playlist Candidates Based on 5 Heuristics\n\n\ntrack_name\nartist_name\npopularity\nplaylist_name\n\n\n\n\nIgnition - Remix\nR. Kelly\n70\nthrowback\n\n\nSure Thing\nMiguel\n74\nthrowback\n\n\nPower Trip\nJ. Cole\n72\nthrowback\n\n\nWhatever You Like\nT.I.\n74\nthrowback\n\n\nCrooked Smile\nJ. Cole\n69\nthrowback\n\n\nSo Good\nB.o.B\n65\nthrowback\n\n\nRich As Fuck\nLil Wayne\n62\nthrowback\n\n\nYoung, Wild & Free (feat. Bruno Mars) - feat. Bruno Mars\nSnoop Dogg\n65\nthrowback\n\n\nStrange Clouds (feat. Lil Wayne) - feat. Lil Wayne\nB.o.B\n60\nthrowback\n\n\nThe Motto\nDrake\n72\nthrowback\n\n\nBattle Scars\nLupe Fiasco\n70\nthrowback\n\n\nThe Show Goes On\nLupe Fiasco\n71\nthrowback\n\n\nMercy\nKanye West\n71\nthrowback\n\n\nSatellites\nKevin Gates\n46\nthrowback\n\n\nLove Me\nLil Wayne\n66\nthrowback\n\n\nNo Hands (feat. Roscoe Dash and Wale) - Explicit Album Version\nWaka Flocka Flame\n75\nthrowback\n\n\nLollipop\nLil Wayne\n70\nthrowback\n\n\nRock Your Body\nJustin Timberlake\n71\nthrowback\n\n\nBeautiful Girls\nSean Kingston\n78\nthrowback\n\n\nA Milli\nLil Wayne\n72\nthrowback"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Welcome to our data-driven deep dive into what may be the most seismic political shift of the decade.\nThe 2024 presidential election didn‚Äôt just redraw the map ‚Äî it rewrote the playbook. Using detailed county-level results from 2020 and 2024, this report traces the unexpected turns in America‚Äôs political landscape:\nüü• Red counties got redder ‚Äî and they weren‚Äôt always rural.\nüü¶ Blue strongholds wobbled, especially in places no one expected.\nüìâ Some states swung hard, while others held the line.\nWe processed thousands of county results, ran rigorous statistical tests, and visualized every twist in this electoral drama. Whether you‚Äôre here to celebrate the momentum or challenge the narrative ‚Äî the charts don‚Äôt lie.\nLet‚Äôs unpack the data behind the divide.\n\n\n\nThis chunk defines the custom U.S. flag-inspired themes for all ggplot2 visualizations and kableExtra tables across the project.\n\n\nCode\n# Install and load required packages\nrequired_packages &lt;- c(\n  \"tidyverse\", \"sf\", \"rvest\", \"httr2\", \"janitor\", \"lubridate\", \"kableExtra\", \n  \"ggplot2\", \"infer\", \"scales\", \"tigris\", \"gganimate\"\n)\n\nfor (pkg in required_packages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# Set options\noptions(scipen = 999, digits = 3)\ntheme_set(theme_minimal())\n\n# üé® Define US Flag Theme for ggplot\ntheme_us_flag &lt;- function() {\n  theme_minimal(base_size = 12) +\n    theme(\n      panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n      plot.background = element_rect(fill = \"#FFFFFF\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5, color = \"#002868\"),\n      plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"#BF0A30\"),\n      axis.title = element_text(color = \"#002868\", face = \"bold\"),\n      axis.text = element_text(color = \"#002868\"),\n      legend.position = \"top\",\n      legend.title = element_blank(),\n      strip.text = element_text(face = \"bold\", color = \"#BF0A30\")\n    )\n}\n\n# üá∫üá∏ Define US-themed table style\nus_table_style &lt;- function(df, caption = NULL) {\n  df %&gt;%\n    kbl(caption = caption, align = \"c\", escape = FALSE) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                  full_width = FALSE, font_size = 13) %&gt;%\n    row_spec(0, bold = TRUE, color = \"white\", background = \"#002868\") %&gt;%\n    column_spec(1, bold = TRUE, color = \"black\") %&gt;%\n    scroll_box(width = \"100%\")\n}\n\n# üî¢ Percent formatter helper\nformat_percent &lt;- function(x, digits = 1) {\n  paste0(formatC(100 * x, format = \"f\", digits = digits), \"%\")\n}\n\n\n\n\n\nBefore we can paint a picture of America‚Äôs political realignment, we need the canvas: a shapefile of U.S. counties. We‚Äôll use the U.S. Census Bureau‚Äôs TIGER/Line shapefiles for 2024. To ensure flexibility, our code automatically falls back to lower-resolution files if the most detailed version fails.\nThis step sets up our geographic base for all future mapping, statistical overlays, and visual storytelling.\n\n\nCode\n# üìÅ Create Local Data Directory\ndata_dir &lt;- \"data/mp04\"\nif (!dir.exists(data_dir)) {\n  dir.create(data_dir, recursive = TRUE)\n  message(\"‚úÖ Created data directory: \", data_dir)\n} else {\n  message(\"üìÇ Using existing data directory: \", data_dir)\n}\n\n# üåê Set Up TIGER/Line Shapefile URL Base\nbase_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2024/shp/\"\nresolutions &lt;- c(\"500k\", \"5m\", \"20m\")  # Ordered by detail: High ‚Üí Low\nresolution_index &lt;- 1  # Start with most detailed\n\n# ‚¨áÔ∏è Attempt to Download County Shapefile\nsuccess &lt;- FALSE\nwhile (!success && resolution_index &lt;= length(resolutions)) {\n  current_resolution &lt;- resolutions[resolution_index]\n  filename &lt;- paste0(\"cb_2024_us_county_\", current_resolution, \".zip\")\n  local_file &lt;- file.path(data_dir, filename)\n  url &lt;- paste0(base_url, filename)\n  \n  if (file.exists(local_file)) {\n    message(\"üì¶ Shapefile already exists locally: \", local_file)\n    success &lt;- TRUE\n  } else {\n    message(\"üåç Attempting download: \", url)\n    \n    download_result &lt;- tryCatch({\n      download.file(url, local_file, mode = \"wb\")\n      TRUE\n    }, error = function(e) {\n      message(\"‚ùå Download failed: \", e$message)\n      FALSE\n    })\n    \n    if (download_result) {\n      message(\"‚úÖ Download complete: \", local_file)\n      unzip(local_file, exdir = file.path(data_dir, paste0(\"county_\", current_resolution)))\n      message(\"üóÇÔ∏è Extracted to: \", file.path(data_dir, paste0(\"county_\", current_resolution)))\n      success &lt;- TRUE\n    } else {\n      resolution_index &lt;- resolution_index + 1\n      if (resolution_index &lt;= length(resolutions)) {\n        message(\"üîÑ Trying lower resolution: \", resolutions[resolution_index])\n      } else {\n        message(\"üö´ All resolutions failed to download.\")\n      }\n    }\n  }\n}\n\n\n\n\n\nThe drama of election night? We scraped it. Using rvest and httr2, we pulled county-level 2024 presidential results directly from Wikipedia for all 50 U.S. states.\nWe tackled inconsistent tables, ambiguous headers, and wild formats to standardize everything into a clean dataset of votes and percentages for Trump, Harris, and Others.\n\n\nCode\n# Function to fetch election data from Wikipedia\nget_election_results &lt;- function(state) {\n  # Special case for Alaska\n  if(state == \"Alaska\") {\n    url &lt;- \"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_Alaska\"\n  } else {\n    # Format state name for URL\n    state_formatted &lt;- str_replace_all(state, \"\\\\s\", \"_\")\n    url &lt;- paste0(\"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_\", state_formatted)\n  }\n  \n  # Create directory for storing data\n  dir_name &lt;- file.path(\"data\", \"election2024\")\n  file_name &lt;- file.path(dir_name, paste0(gsub(\"\\\\s\", \"_\", state), \".html\"))\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  # Download data if not cached\n  if (!file.exists(file_name)) {\n    tryCatch({\n      RESPONSE &lt;- req_perform(request(url))\n      writeLines(resp_body_string(RESPONSE), file_name)\n    }, error = function(e) {\n      warning(paste(\"Error fetching data for\", state, \":\", e$message))\n      return(NULL)\n    })\n  }\n  \n  # Exit if file doesn't exist\n  if (!file.exists(file_name)) return(NULL)\n  \n  # Parse HTML\n  page &lt;- tryCatch(read_html(file_name), error = function(e) NULL)\n  if (is.null(page)) return(NULL)\n  \n  # Extract tables\n  tables &lt;- tryCatch(page |&gt; html_elements(\"table.wikitable\") |&gt; \n                       html_table(na.strings = c(\"\", \"N/A\", \"‚Äî\")), \n                     error = function(e) list())\n  \n  if (length(tables) == 0) return(NULL)\n  \n  # Find county results table\n  county_table &lt;- NULL\n  \n  # Look for county column names\n  for (i in seq_along(tables)) {\n    if (ncol(tables[[i]]) &lt; 3) next\n    \n    col_names &lt;- colnames(tables[[i]])\n    if (is.null(col_names) || any(is.na(col_names))) next\n    \n    # Look for county identifiers in column names\n    if (any(str_detect(col_names, regex(\"County|Parish|Borough|Census Area|Municipality\", ignore_case = TRUE)))) {\n      county_table &lt;- tables[[i]]\n      break\n    }\n  }\n  \n  # Check for county values in first column\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3 || nrow(tables[[i]]) == 0 || is.null(tables[[i]][[1]])) next\n      \n      first_col &lt;- tables[[i]][[1]]\n      first_col_clean &lt;- first_col[!is.na(first_col)]\n      \n      if (length(first_col_clean) &gt; 0 && \n          any(str_detect(as.character(first_col_clean), \n                         regex(\"County|Parish|Borough|Census Area\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Look for candidate names\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3) next\n      \n      # Check column names\n      col_names &lt;- colnames(tables[[i]])\n      if (!is.null(col_names) && !any(is.na(col_names)) &&\n          any(str_detect(col_names, regex(\"Trump|Harris|Republican|Democrat\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Last resort - largest table\n  if (is.null(county_table) && length(tables) &gt; 0) {\n    valid_tables &lt;- tables[sapply(tables, function(t) ncol(t) &gt;= 3 && nrow(t) &gt;= 3)]\n    if (length(valid_tables) &gt; 0) {\n      county_table &lt;- valid_tables[[which.max(sapply(valid_tables, nrow))]]\n    }\n  }\n  \n  if (is.null(county_table)) return(NULL)\n  \n  # Format table\n  result &lt;- tryCatch({\n    # Find county column\n    county_col &lt;- which(str_detect(colnames(county_table), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District\", ignore_case = TRUE)))\n    county_col &lt;- if(length(county_col) &gt; 0) county_col[1] else 1\n    \n    result &lt;- county_table\n    names(result)[county_col] &lt;- \"County\"\n    result$State &lt;- state\n    \n    return(result)\n  }, error = function(e) NULL)\n  \n  return(result)\n}\n\n# Function to standardize election data\nstandardize_election_data &lt;- function(df, state) {\n  if (is.null(df) || nrow(df) == 0) return(NULL)\n  \n  # Extract numeric values from string\n  extract_numeric &lt;- function(values) {\n    if (is.null(values)) return(rep(NA, nrow(df)))\n    chars &lt;- as.character(values)\n    chars &lt;- gsub(\",|%|\\\\s\", \"\", chars)\n    suppressWarnings(as.numeric(chars))\n  }\n  \n  # Find candidate columns\n  find_candidate_columns &lt;- function(candidate, df_names) {\n    cols &lt;- which(str_detect(df_names, regex(candidate, ignore_case = TRUE)))\n    if (length(cols) &gt;= 2) {\n      vote_col &lt;- NULL\n      pct_col &lt;- NULL\n      \n      for (col in cols) {\n        col_name &lt;- df_names[col]\n        if (str_detect(col_name, regex(\"%|percent\", ignore_case = TRUE))) {\n          pct_col &lt;- col\n        } else if (str_detect(col_name, regex(\"votes|#\", ignore_case = TRUE))) {\n          vote_col &lt;- col\n        }\n      }\n      \n      if (is.null(vote_col) && length(cols) &gt;= 1) vote_col &lt;- cols[1]\n      if (is.null(pct_col) && length(cols) &gt;= 2) pct_col &lt;- cols[2]\n      \n      return(list(vote_col = vote_col, pct_col = pct_col))\n    } else if (length(cols) == 1) {\n      return(list(vote_col = cols[1], pct_col = NULL))\n    } else {\n      return(list(vote_col = NULL, pct_col = NULL))\n    }\n  }\n  \n  # Ensure County column\n  if (!\"County\" %in% names(df)) {\n    county_col &lt;- which(str_detect(names(df), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District|City\", ignore_case = TRUE)))\n    if (length(county_col) &gt; 0) {\n      names(df)[county_col[1]] &lt;- \"County\"\n    } else {\n      names(df)[1] &lt;- \"County\"\n    }\n  }\n  \n  # Find candidate and total columns\n  trump_cols &lt;- find_candidate_columns(\"Trump|Republican\", names(df))\n  harris_cols &lt;- find_candidate_columns(\"Harris|Democratic|Democrat\", names(df))\n  other_cols &lt;- find_candidate_columns(\"Other|Independent|Third\", names(df))\n  total_col &lt;- which(str_detect(names(df), regex(\"Total|Sum|Cast\", ignore_case = TRUE)))\n  total_col &lt;- if (length(total_col) &gt; 0) total_col[length(total_col)] else NULL\n  \n  # Create standardized dataframe\n  result &lt;- data.frame(\n    County = df$County,\n    State = state,\n    Trump_Votes = if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else NA,\n    Trump_Percent = if (!is.null(trump_cols$pct_col)) extract_numeric(df[[trump_cols$pct_col]]) else NA,\n    Harris_Votes = if (!is.null(harris_cols$vote_col)) extract_numeric(df[[harris_cols$vote_col]]) else NA,\n    Harris_Percent = if (!is.null(harris_cols$pct_col)) extract_numeric(df[[harris_cols$pct_col]]) else NA,\n    Other_Votes = if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else NA,\n    Other_Percent = if (!is.null(other_cols$pct_col)) extract_numeric(df[[other_cols$pct_col]]) else NA,\n    Total_Votes = if (!is.null(total_col)) extract_numeric(df[[total_col]]) else \n      rowSums(cbind(\n        if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else 0,\n        if (!is.null(harris_cols$vote_col)) extract_numeric(df[[harris_cols$vote_col]]) else 0,\n        if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else 0\n      ), na.rm = TRUE),\n    stringsAsFactors = FALSE\n  )\n  \n  return(result)\n}\n\n# Process all states\nprocess_election_data &lt;- function() {\n  states &lt;- state.name\n  all_data &lt;- list()\n  \n  for (state in states) {\n    \n    raw_data &lt;- get_election_results(state)\n    \n    if (!is.null(raw_data)) {\n      std_data &lt;- standardize_election_data(raw_data, state)\n      \n      if (!is.null(std_data) && nrow(std_data) &gt; 0) {\n        all_data[[state]] &lt;- std_data\n      }\n    }\n  }\n  \n  # Combine all data\n  combined_data &lt;- do.call(rbind, all_data)\n  \n  # Clean data - remove problematic rows\n  clean_data &lt;- combined_data %&gt;%\n    filter(\n      !is.na(Trump_Votes) & !is.na(Harris_Votes) & \n        !str_detect(County, regex(\"^County$|^County\\\\[|^Total\", ignore_case = TRUE))\n    ) %&gt;%\n    mutate(County = gsub(\"\\\\[\\\\d+\\\\]\", \"\", County),\n           County = trimws(County))\n  \n  # Save results\n  write.csv(clean_data, \"data/election_results_2024.csv\", row.names = FALSE)\n  \n  # Create summary by state\n  state_summary &lt;- clean_data %&gt;%\n    group_by(State) %&gt;%\n    summarize(\n      Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Harris_Total = sum(Harris_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Harris_Pct = Harris_Total / Total_Votes * 100\n    ) %&gt;%\n    arrange(desc(Total_Votes))\n  \n  write.csv(state_summary, \"data/election_results_2024_summary.csv\", row.names = FALSE)\n  \n  return(state_summary)\n}\n\n# Run the process and display results\nelection_summary &lt;- process_election_data()\n\n# Format the percentages for better display\nelection_table &lt;- election_summary %&gt;%\n  mutate(\n    Trump_Pct = sprintf(\"%.1f%%\", Trump_Pct),\n    Harris_Pct = sprintf(\"%.1f%%\", Harris_Pct),\n    Winner = ifelse(Trump_Total &gt; Harris_Total, \"Trump\", \"Harris\"),\n    Margin = paste0(\n      ifelse(Trump_Total &gt; Harris_Total, Trump_Pct, Harris_Pct), \" - \",\n      ifelse(Trump_Total &gt; Harris_Total, Harris_Pct, Trump_Pct)\n    )\n  ) %&gt;%\n  select(State, Counties, Total_Votes, Winner, Margin, Trump_Pct, Harris_Pct)\n\n# Read and display the 2024 state-level summary\nelection_2024_summary &lt;- read.csv(\"data/election_results_2024_summary.csv\")\n# üá∫üá∏ Display final styled results table with U.S. flag theme\nus_table_style(\n  df = election_table,\n  caption = \"üó≥Ô∏è 2024 U.S. Presidential Election Results by State\"\n)\n\n\n\n\n\nüó≥Ô∏è 2024 U.S. Presidential Election Results by State\n\n\nState\nCounties\nTotal_Votes\nWinner\nMargin\nTrump_Pct\nHarris_Pct\n\n\n\n\nCalifornia\n58\n15871260\nHarris\n58.4% - 38.3%\n38.3%\n58.4%\n\n\nTexas\n254\n11406186\nTrump\n56.1% - 42.4%\n56.1%\n42.4%\n\n\nFlorida\n67\n10935465\nTrump\n55.9% - 42.8%\n55.9%\n42.8%\n\n\nNew York\n62\n8300211\nHarris\n55.7% - 43.1%\n43.1%\n55.7%\n\n\nPennsylvania\n67\n7058269\nTrump\n50.2% - 48.5%\n50.2%\n48.5%\n\n\nOhio\n88\n5799829\nTrump\n54.8% - 43.7%\n54.8%\n43.7%\n\n\nNorth Carolina\n100\n5699141\nTrump\n50.9% - 47.6%\n50.9%\n47.6%\n\n\nMichigan\n83\n5674485\nTrump\n49.6% - 48.2%\n49.6%\n48.2%\n\n\nIllinois\n102\n5652103\nHarris\n54.2% - 43.3%\n43.3%\n54.2%\n\n\nGeorgia\n159\n5270783\nTrump\n50.5% - 48.3%\n50.5%\n48.3%\n\n\nVirginia\n133\n4505941\nHarris\n51.8% - 46.1%\n46.1%\n51.8%\n\n\nNew Jersey\n21\n4287740\nHarris\n51.8% - 45.9%\n45.9%\n51.8%\n\n\nMassachusetts\n14\n3473668\nHarris\n61.2% - 36.0%\n36.0%\n61.2%\n\n\nWisconsin\n72\n3422918\nTrump\n49.6% - 48.7%\n49.6%\n48.7%\n\n\nArizona\n15\n3400726\nTrump\n52.1% - 46.5%\n52.1%\n46.5%\n\n\nMinnesota\n87\n3253920\nHarris\n50.9% - 46.7%\n46.7%\n50.9%\n\n\nColorado\n64\n3192745\nHarris\n54.1% - 43.1%\n43.1%\n54.1%\n\n\nTennessee\n95\n3063942\nTrump\n64.2% - 34.5%\n64.2%\n34.5%\n\n\nMaryland\n24\n3038334\nHarris\n62.6% - 34.1%\n34.1%\n62.6%\n\n\nMissouri\n115\n3003967\nTrump\n58.3% - 40.0%\n58.3%\n40.0%\n\n\nIndiana\n92\n2944336\nTrump\n58.4% - 39.5%\n58.4%\n39.5%\n\n\nSouth Carolina\n46\n2548140\nTrump\n58.2% - 40.4%\n58.2%\n40.4%\n\n\nAlabama\n67\n2265090\nTrump\n64.6% - 34.1%\n64.6%\n34.1%\n\n\nOregon\n36\n2244493\nHarris\n55.3% - 41.0%\n41.0%\n55.3%\n\n\nKentucky\n120\n2076806\nTrump\n64.4% - 33.9%\n64.4%\n33.9%\n\n\nLouisiana\n64\n2006975\nTrump\n60.2% - 38.2%\n60.2%\n38.2%\n\n\nConnecticut\n8\n1759010\nHarris\n56.4% - 41.9%\n41.9%\n56.4%\n\n\nIowa\n99\n1663506\nTrump\n55.7% - 42.5%\n55.7%\n42.5%\n\n\nOklahoma\n77\n1566173\nTrump\n66.2% - 31.9%\n66.2%\n31.9%\n\n\nUtah\n29\n1488494\nTrump\n59.4% - 37.8%\n59.4%\n37.8%\n\n\nNevada\n17\n1484840\nTrump\n50.6% - 47.5%\n50.6%\n47.5%\n\n\nKansas\n105\n1335345\nTrump\n56.8% - 40.8%\n56.8%\n40.8%\n\n\nMississippi\n82\n1229255\nTrump\n60.8% - 38.0%\n60.8%\n38.0%\n\n\nArkansas\n75\n1182676\nTrump\n64.2% - 33.6%\n64.2%\n33.6%\n\n\nNebraska\n93\n952182\nTrump\n59.3% - 38.9%\n59.3%\n38.9%\n\n\nNew Mexico\n33\n923403\nHarris\n51.9% - 45.9%\n45.9%\n51.9%\n\n\nIdaho\n44\n905057\nTrump\n66.9% - 30.4%\n66.9%\n30.4%\n\n\nNew Hampshire\n10\n826189\nHarris\n50.7% - 47.9%\n47.9%\n50.7%\n\n\nMaine\n16\n824420\nHarris\n52.2% - 45.7%\n45.7%\n52.2%\n\n\nWest Virginia\n55\n763679\nTrump\n69.9% - 28.1%\n69.9%\n28.1%\n\n\nMontana\n56\n604181\nTrump\n58.3% - 38.4%\n58.3%\n38.4%\n\n\nHawaii\n5\n516719\nHarris\n60.6% - 37.5%\n37.5%\n60.6%\n\n\nDelaware\n3\n512912\nHarris\n56.5% - 41.8%\n41.8%\n56.5%\n\n\nRhode Island\n5\n511816\nHarris\n55.4% - 41.9%\n41.9%\n55.4%\n\n\nSouth Dakota\n66\n428922\nTrump\n63.4% - 34.2%\n63.4%\n34.2%\n\n\nVermont\n14\n369422\nHarris\n63.8% - 32.3%\n32.3%\n63.8%\n\n\nNorth Dakota\n53\n367714\nTrump\n67.0% - 30.5%\n67.0%\n30.5%\n\n\nWyoming\n23\n269048\nTrump\n71.6% - 25.8%\n71.6%\n25.8%\n\n\nAlaska\n3\n300\nTrump\n54.0% - 44.7%\n54.0%\n44.7%\n\n\n\n\n\n\n\n\n\n\n\nWith our 2024 data in hand, we now turn the clock back to 2020 to build a comparative baseline. This section scrapes county-level results for all 50 states from Wikipedia, standardizes them, and prepares summary tables for analysis. Let‚Äôs see how the Trump-Biden race unfolded on a granular level.\n\n\nCode\nif (!require(\"rvest\")) {\n  install.packages(\"rvest\")\n  library(rvest)\n}\nif (!require(\"httr2\")) {\n  install.packages(\"httr2\")\n  library(httr2)\n}\n\n# Function to fetch 2020 election data from Wikipedia\nget_2020_election_results &lt;- function(state) {\n  # Format state name for URL\n  state_formatted &lt;- str_replace_all(state, \"\\\\s\", \"_\")\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_\", state_formatted)\n  \n  # Create directory for storing data\n  dir_name &lt;- file.path(\"data\", \"election2020\")\n  file_name &lt;- file.path(dir_name, paste0(gsub(\"\\\\s\", \"_\", state), \".html\"))\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  # Download data if not cached\n  if (!file.exists(file_name)) {\n    tryCatch({\n      RESPONSE &lt;- req_perform(request(url))\n      writeLines(resp_body_string(RESPONSE), file_name)\n     \n    }, error = function(e) {\n      warning(paste(\"Error fetching 2020 data for\", state, \":\", e$message))\n      return(NULL)\n    })\n  } else {\n    \n  }\n  \n  # Exit if file doesn't exist\n  if (!file.exists(file_name)) return(NULL)\n  \n  # Parse HTML\n  page &lt;- tryCatch(read_html(file_name), error = function(e) NULL)\n  if (is.null(page)) return(NULL)\n  \n  # Extract tables\n  tables &lt;- tryCatch(page |&gt; html_elements(\"table.wikitable\") |&gt; \n                       html_table(na.strings = c(\"\", \"N/A\", \"‚Äî\")), \n                     error = function(e) list())\n  \n  if (length(tables) == 0) return(NULL)\n  \n  # Find county results table\n  county_table &lt;- NULL\n  \n  # Look for county column names\n  for (i in seq_along(tables)) {\n    if (ncol(tables[[i]]) &lt; 3) next\n    \n    col_names &lt;- colnames(tables[[i]])\n    if (is.null(col_names) || any(is.na(col_names))) next\n    \n    # Look for county identifiers in column names\n    if (any(str_detect(col_names, regex(\"County|Parish|Borough|Census Area|Municipality\", ignore_case = TRUE)))) {\n      county_table &lt;- tables[[i]]\n      break\n    }\n  }\n  \n  # Check for county values in first column\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3 || nrow(tables[[i]]) == 0 || is.null(tables[[i]][[1]])) next\n      \n      first_col &lt;- tables[[i]][[1]]\n      first_col_clean &lt;- first_col[!is.na(first_col)]\n      \n      if (length(first_col_clean) &gt; 0 && \n          any(str_detect(as.character(first_col_clean), \n                         regex(\"County|Parish|Borough|Census Area\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Look for candidate names for 2020 election (Trump vs Biden)\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3) next\n      \n      # Check column names\n      col_names &lt;- colnames(tables[[i]])\n      if (!is.null(col_names) && !any(is.na(col_names)) &&\n          any(str_detect(col_names, regex(\"Trump|Biden|Republican|Democrat\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n      \n      # Check first few rows for candidates\n      if (nrow(tables[[i]]) &gt; 2) {\n        first_rows_char &lt;- lapply(tables[[i]][1:min(5, nrow(tables[[i]])),], function(x) {\n          ifelse(is.na(x), NA_character_, as.character(x))\n        })\n        \n        found_candidates &lt;- FALSE\n        for (j in 1:length(first_rows_char)) {\n          col_values &lt;- first_rows_char[[j]]\n          col_values &lt;- col_values[!is.na(col_values)]\n          \n          if (length(col_values) &gt; 0 &&\n              any(str_detect(col_values, regex(\"Trump|Republican\", ignore_case = TRUE))) && \n              any(str_detect(col_values, regex(\"Biden|Democratic|Democrat\", ignore_case = TRUE)))) {\n            county_table &lt;- tables[[i]]\n            found_candidates &lt;- TRUE\n            break\n          }\n        }\n        if (found_candidates) break\n      }\n    }\n  }\n  \n  # Last resort - largest table\n  if (is.null(county_table) && length(tables) &gt; 0) {\n    valid_tables &lt;- tables[sapply(tables, function(t) ncol(t) &gt;= 3 && nrow(t) &gt;= 3)]\n    if (length(valid_tables) &gt; 0) {\n      county_table &lt;- valid_tables[[which.max(sapply(valid_tables, nrow))]]\n    }\n  }\n  \n  if (is.null(county_table)) return(NULL)\n  \n  # Format table\n  result &lt;- tryCatch({\n    # Find county column\n    county_col &lt;- which(str_detect(colnames(county_table), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District\", ignore_case = TRUE)))\n    county_col &lt;- if(length(county_col) &gt; 0) county_col[1] else 1\n    \n    result &lt;- county_table\n    names(result)[county_col] &lt;- \"County\"\n    result$State &lt;- state\n    \n    return(result)\n  }, error = function(e) NULL)\n  \n  return(result)\n}\n\n# Function to standardize 2020 election data\nstandardize_2020_election_data &lt;- function(df, state) {\n  if (is.null(df) || nrow(df) == 0) return(NULL)\n  \n  # Extract numeric values from string\n  extract_numeric &lt;- function(values) {\n    if (is.null(values)) return(rep(NA, nrow(df)))\n    chars &lt;- as.character(values)\n    chars &lt;- gsub(\",|%|\\\\s\", \"\", chars)\n    suppressWarnings(as.numeric(chars))\n  }\n  \n  # Find candidate columns - specific to 2020 election (Trump vs Biden)\n  find_candidate_columns &lt;- function(candidate, df_names) {\n    cols &lt;- which(str_detect(df_names, regex(candidate, ignore_case = TRUE)))\n    if (length(cols) &gt;= 2) {\n      vote_col &lt;- NULL\n      pct_col &lt;- NULL\n      \n      for (col in cols) {\n        col_name &lt;- df_names[col]\n        if (str_detect(col_name, regex(\"%|percent\", ignore_case = TRUE))) {\n          pct_col &lt;- col\n        } else if (str_detect(col_name, regex(\"votes|#\", ignore_case = TRUE))) {\n          vote_col &lt;- col\n        }\n      }\n      \n      if (is.null(vote_col) && length(cols) &gt;= 1) vote_col &lt;- cols[1]\n      if (is.null(pct_col) && length(cols) &gt;= 2) pct_col &lt;- cols[2]\n      \n      return(list(vote_col = vote_col, pct_col = pct_col))\n    } else if (length(cols) == 1) {\n      return(list(vote_col = cols[1], pct_col = NULL))\n    } else {\n      return(list(vote_col = NULL, pct_col = NULL))\n    }\n  }\n  \n  # Ensure County column\n  if (!\"County\" %in% names(df)) {\n    county_col &lt;- which(str_detect(names(df), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District|City\", ignore_case = TRUE)))\n    if (length(county_col) &gt; 0) {\n      names(df)[county_col[1]] &lt;- \"County\"\n    } else {\n      names(df)[1] &lt;- \"County\"\n    }\n  }\n  \n  # Find candidate and total columns for 2020 (Trump vs Biden)\n  trump_cols &lt;- find_candidate_columns(\"Trump|Republican\", names(df))\n  biden_cols &lt;- find_candidate_columns(\"Biden|Democratic|Democrat\", names(df))\n  other_cols &lt;- find_candidate_columns(\"Other|Independent|Third|Jorgensen|Hawkins\", names(df))\n  total_col &lt;- which(str_detect(names(df), regex(\"Total|Sum|Cast\", ignore_case = TRUE)))\n  total_col &lt;- if (length(total_col) &gt; 0) total_col[length(total_col)] else NULL\n  \n  # Create standardized dataframe\n  result &lt;- data.frame(\n    County = df$County,\n    State = state,\n    Trump_Votes = if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else NA,\n    Trump_Percent = if (!is.null(trump_cols$pct_col)) extract_numeric(df[[trump_cols$pct_col]]) else NA,\n    Biden_Votes = if (!is.null(biden_cols$vote_col)) extract_numeric(df[[biden_cols$vote_col]]) else NA,\n    Biden_Percent = if (!is.null(biden_cols$pct_col)) extract_numeric(df[[biden_cols$pct_col]]) else NA,\n    Other_Votes = if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else NA,\n    Other_Percent = if (!is.null(other_cols$pct_col)) extract_numeric(df[[other_cols$pct_col]]) else NA,\n    Total_Votes = if (!is.null(total_col)) extract_numeric(df[[total_col]]) else \n      rowSums(cbind(\n        if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else 0,\n        if (!is.null(biden_cols$vote_col)) extract_numeric(df[[biden_cols$vote_col]]) else 0,\n        if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else 0\n      ), na.rm = TRUE),\n    stringsAsFactors = FALSE\n  )\n  \n  return(result)\n}\n\n# Process all states for 2020 election\nprocess_2020_election_data &lt;- function() {\n  states &lt;- state.name\n  all_data &lt;- list()\n  \n  for (state in states) {\n\n    raw_data &lt;- get_2020_election_results(state)\n    \n    if (!is.null(raw_data)) {\n      std_data &lt;- standardize_2020_election_data(raw_data, state)\n      \n      if (!is.null(std_data) && nrow(std_data) &gt; 0) {\n        all_data[[state]] &lt;- std_data\n      }\n    }\n  }\n  \n  # Combine all data\n  combined_data &lt;- do.call(rbind, all_data)\n  \n  # Clean data - remove problematic rows\n  clean_data &lt;- combined_data %&gt;%\n    filter(\n      !is.na(Trump_Votes) & !is.na(Biden_Votes) & \n        !str_detect(County, regex(\"^County$|^County\\\\[|^Total\", ignore_case = TRUE))\n    ) %&gt;%\n    mutate(County = gsub(\"\\\\[\\\\d+\\\\]\", \"\", County),\n           County = trimws(County))\n  \n  # Save results\n  write.csv(clean_data, \"data/election_results_2020.csv\", row.names = FALSE)\n  \n  # Create summary by state\n  state_summary &lt;- clean_data %&gt;%\n    group_by(State) %&gt;%\n    summarize(\n      Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Biden_Total = sum(Biden_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Biden_Pct = Biden_Total / Total_Votes * 100\n    ) %&gt;%\n    arrange(desc(Total_Votes))\n  \n  write.csv(state_summary, \"data/election_results_2020_summary.csv\", row.names = FALSE)\n  \n  # Create national summary\n  national_summary &lt;- clean_data %&gt;%\n    summarize(\n      Total_Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Biden_Total = sum(Biden_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Biden_Pct = Biden_Total / Total_Votes * 100\n    )\n  \n  write.csv(national_summary, \"data/election_results_2020_national.csv\", row.names = FALSE)\n  \n  return(list(state_summary = state_summary, national_summary = national_summary))\n}\n\n# Run the process for 2020 data\nelection_results_2020 &lt;- process_2020_election_data()\nelection_table_2020 &lt;- election_results_2020$state_summary %&gt;%\n  mutate(\n    Trump_Pct = sprintf(\"%.1f%%\", Trump_Pct),\n    Biden_Pct = sprintf(\"%.1f%%\", Biden_Pct),\n    Winner = ifelse(Trump_Total &gt; Biden_Total, \"Trump\", \"Biden\"),\n    Margin = paste0(\n      ifelse(Trump_Total &gt; Biden_Total, Trump_Pct, Biden_Pct), \" - \",\n      ifelse(Trump_Total &gt; Biden_Total, Biden_Pct, Trump_Pct)\n    )\n  ) %&gt;%\n  select(State, Counties, Total_Votes, Winner, Margin, Trump_Pct, Biden_Pct)\n\nus_table_style(\n  df = election_table_2020,\n  caption = \"üó≥Ô∏è 2020 U.S. Presidential Election Results by State\"\n)\n\n\n\n\n\nüó≥Ô∏è 2020 U.S. Presidential Election Results by State\n\n\nState\nCounties\nTotal_Votes\nWinner\nMargin\nTrump_Pct\nBiden_Pct\n\n\n\n\nCalifornia\n58\n17531845\nBiden\n63.4% - 34.3%\n34.3%\n63.4%\n\n\nTexas\n254\n11325286\nTrump\n52.0% - 46.4%\n52.0%\n46.4%\n\n\nFlorida\n67\n11091758\nTrump\n51.1% - 47.8%\n51.1%\n47.8%\n\n\nNew York\n62\n8632255\nBiden\n60.8% - 37.7%\n37.7%\n60.8%\n\n\nPennsylvania\n67\n6940449\nBiden\n49.9% - 48.7%\n48.7%\n49.9%\n\n\nIllinois\n102\n6049500\nBiden\n57.4% - 40.4%\n40.4%\n57.4%\n\n\nOhio\n88\n5932398\nTrump\n53.2% - 45.2%\n53.2%\n45.2%\n\n\nMichigan\n83\n5547186\nBiden\n50.5% - 47.8%\n47.8%\n50.5%\n\n\nNorth Carolina\n100\n5524804\nTrump\n49.9% - 48.6%\n49.9%\n48.6%\n\n\nGeorgia\n159\n4999960\nBiden\n49.5% - 49.2%\n49.2%\n49.5%\n\n\nNew Jersey\n21\n4565182\nBiden\n57.1% - 41.3%\n41.3%\n57.1%\n\n\nVirginia\n133\n4460524\nBiden\n54.1% - 44.0%\n44.0%\n54.1%\n\n\nMassachusetts\n14\n3631402\nBiden\n65.6% - 32.1%\n32.1%\n65.6%\n\n\nArizona\n15\n3397388\nBiden\n49.2% - 48.9%\n48.9%\n49.2%\n\n\nWisconsin\n72\n3298221\nBiden\n49.4% - 48.8%\n48.8%\n49.4%\n\n\nMinnesota\n87\n3277171\nBiden\n52.4% - 45.3%\n45.3%\n52.4%\n\n\nColorado\n64\n3256980\nBiden\n55.4% - 41.9%\n41.9%\n55.4%\n\n\nTennessee\n95\n3053851\nTrump\n60.7% - 37.5%\n60.7%\n37.5%\n\n\nIndiana\n92\n3039781\nTrump\n56.9% - 40.9%\n56.9%\n40.9%\n\n\nMaryland\n24\n3037030\nBiden\n65.4% - 32.2%\n32.2%\n65.4%\n\n\nMissouri\n115\n3030748\nTrump\n56.7% - 41.3%\n56.7%\n41.3%\n\n\nSouth Carolina\n46\n2513329\nTrump\n55.1% - 43.4%\n55.1%\n43.4%\n\n\nOregon\n36\n2374321\nBiden\n56.5% - 40.4%\n40.4%\n56.5%\n\n\nAlabama\n67\n2323282\nTrump\n62.0% - 36.6%\n62.0%\n36.6%\n\n\nLouisiana\n64\n2148062\nTrump\n58.5% - 39.9%\n58.5%\n39.9%\n\n\nKentucky\n120\n2138009\nTrump\n62.1% - 36.1%\n62.1%\n36.1%\n\n\nConnecticut\n8\n1824456\nBiden\n59.2% - 39.2%\n39.2%\n59.2%\n\n\nIowa\n99\n1690871\nTrump\n53.1% - 44.9%\n53.1%\n44.9%\n\n\nOklahoma\n77\n1560699\nTrump\n65.4% - 32.3%\n65.4%\n32.3%\n\n\nUtah\n29\n1505982\nTrump\n57.4% - 37.2%\n57.4%\n37.2%\n\n\nKansas\n105\n1377464\nTrump\n56.0% - 41.4%\n56.0%\n41.4%\n\n\nMississippi\n82\n1314475\nTrump\n57.6% - 41.0%\n57.6%\n41.0%\n\n\nArkansas\n75\n1219069\nTrump\n62.4% - 34.8%\n62.4%\n34.8%\n\n\nNebraska\n93\n956383\nTrump\n58.2% - 39.2%\n58.2%\n39.2%\n\n\nNew Mexico\n33\n923965\nBiden\n54.3% - 43.5%\n43.5%\n54.3%\n\n\nIdaho\n44\n870351\nTrump\n63.7% - 33.0%\n63.7%\n33.0%\n\n\nMaine\n16\n813742\nBiden\n52.9% - 44.2%\n44.2%\n52.9%\n\n\nNew Hampshire\n10\n806205\nBiden\n52.7% - 45.4%\n45.4%\n52.7%\n\n\nWest Virginia\n55\n794731\nTrump\n68.6% - 29.7%\n68.6%\n29.7%\n\n\nMontana\n56\n605570\nTrump\n56.7% - 40.4%\n56.7%\n40.4%\n\n\nHawaii\n5\n574493\nBiden\n63.7% - 34.3%\n34.3%\n63.7%\n\n\nRhode Island\n5\n516383\nBiden\n59.3% - 38.7%\n38.7%\n59.3%\n\n\nDelaware\n3\n504010\nBiden\n58.8% - 39.8%\n39.8%\n58.8%\n\n\nSouth Dakota\n66\n422609\nTrump\n61.8% - 35.6%\n61.8%\n35.6%\n\n\nVermont\n14\n367428\nBiden\n66.1% - 30.7%\n30.7%\n66.1%\n\n\nNorth Dakota\n53\n361819\nTrump\n65.1% - 31.8%\n65.1%\n31.8%\n\n\nWyoming\n23\n276765\nTrump\n69.9% - 26.6%\n69.9%\n26.6%\n\n\nAlaska\n3\n300\nTrump\n50.3% - 43.7%\n50.3%\n43.7%\n\n\n\n\n\n\n\n\n\n\n\nThis section merges the geospatial shapefiles from Task 1 with the cleaned election data from 2020 and 2024. Once merged, we compute key variables like vote shifts, turnout changes, and extreme values for insight-rich comparison.\n\n\nCode\ncombine_election_data &lt;- function() {\n  # Load county shapefile from Task-1\n  data_dir &lt;- \"data/mp04\"\n  shp_dirs &lt;- list.dirs(data_dir, recursive = FALSE)\n  county_dir &lt;- shp_dirs[grep(\"county\", shp_dirs)]\n  \n  if (length(county_dir) == 0) {\n    stop(\"County shapefile directory not found. Run Task-1 first.\")\n  }\n  \n  # Find shapefile in the directory\n  shp_files &lt;- list.files(county_dir, pattern = \"\\\\.shp$\", full.names = TRUE)\n  \n  # Add quiet=TRUE to suppress messages\n  counties_sf &lt;- sf::st_read(shp_files[1], quiet = TRUE)\n  \n  # Load election data from Task-2 and Task-3\n  election_2020 &lt;- read.csv(\"data/election_results_2020.csv\")\n  election_2024 &lt;- read.csv(\"data/election_results_2024.csv\")\n  \n  # Prepare county shapefile for joining\n  counties_sf &lt;- counties_sf %&gt;%\n    mutate(\n      County = NAME,\n      StateAbbr = STUSPS\n    )\n  \n  # Create state abbreviation lookup for joining\n  state_lookup &lt;- data.frame(\n    StateAbbr = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"),\n    State = state.name\n  )\n  \n  # Add state names to shapefile\n  counties_sf &lt;- counties_sf %&gt;%\n    left_join(state_lookup, by = \"StateAbbr\")\n  \n  # Clean county names for better joining\n  counties_sf$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", counties_sf$County)\n  election_2020$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", election_2020$County)\n  election_2024$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", election_2024$County)\n  \n  # Add year identifiers to election data\n  election_2020$Year &lt;- 2020\n  election_2024$Year &lt;- 2024\n  \n  # Create join keys\n  counties_sf$join_key &lt;- paste(counties_sf$County, counties_sf$State)\n  election_2020$join_key &lt;- paste(election_2020$County, election_2020$State)\n  election_2024$join_key &lt;- paste(election_2024$County, election_2024$State)\n  \n  # Join shapefile with election data\n  counties_with_2020 &lt;- counties_sf %&gt;%\n    left_join(election_2020, by = \"join_key\")\n  \n  counties_with_both &lt;- counties_with_2020 %&gt;%\n    left_join(election_2024, by = \"join_key\", suffix = c(\"_2020\", \"_2024\"))\n  \n  # Save the combined data\n  saveRDS(counties_with_both, \"data/mp04/combined_election_data.rds\")\n  \n  # Save as shapefile with quiet=TRUE to suppress messages\n  st_write(counties_with_both, \"data/mp04/combined_counties_elections.shp\", delete_layer = TRUE, quiet = TRUE)\n  \n  return(counties_with_both)\n}\n\n# Run the function but don't print the result automatically\ncombined_data &lt;- combine_election_data()\n\n\n\n\n\nRed, Blue, and Beyond: Who Rose, Who Fell, and Who Voted Like Never Before\nWith both elections cleaned, mapped, and merged‚Äîit‚Äôs time to extract the stories buried deep in the numbers. From Trump‚Äôs biggest stronghold to the largest county by landmass, this section highlights the most extreme outliers and political battlegrounds of the 2020‚Äì2024 cycle.\nüìà Who gained?\nüìâ Who lost ground?\nüó∫Ô∏è And which county punched above its weight in turnout, size, or swing?\nLet‚Äôs break it down, one flag-waving row at a time.\n\n\nCode\n# üì• Load the combined data\ncombined_data &lt;- readRDS(\"data/mp04/combined_election_data.rds\")\n\n# üßÆ Calculate derived metrics\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    Trump_Shift = Trump_Percent_2024 - Trump_Percent_2020,\n    Harris_Shift = Harris_Percent - Biden_Percent,\n    Turnout_Change = Total_Votes_2024 - Total_Votes_2020\n  )\n\n# ü•á Compute key metrics\nresults_table &lt;- tibble::tibble(\n  `Question` = c(\n    \"County with most Trump votes (2024)\",\n    \"County with highest Biden share (2020)\",\n    \"County with largest shift toward Trump (2024)\",\n    \"State with smallest shift toward Trump / largest toward Harris\",\n    \"Largest county by area\",\n    \"County with highest voter density (2020)\",\n    \"County with largest turnout increase (2024)\"\n  ),\n  `Answer` = c(\n    {\n      row &lt;- combined_data %&gt;% filter(!is.na(Trump_Votes_2024)) %&gt;%\n        slice_max(Trump_Votes_2024, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(row$Trump_Votes_2024)} votes)\")\n    },\n    {\n      row &lt;- combined_data %&gt;% filter(!is.na(Biden_Percent)) %&gt;%\n        slice_max(Biden_Percent, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({round(row$Biden_Percent, 1)}%)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        mutate(Trump_Vote_Shift = Trump_Votes_2024 - Trump_Votes_2020) %&gt;%\n        filter(!is.na(Trump_Vote_Shift)) %&gt;%\n        slice_max(Trump_Vote_Shift, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(row$Trump_Vote_Shift)} votes)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        group_by(State) %&gt;%\n        summarize(\n          Trump_2020 = sum(Trump_Votes_2020, na.rm = TRUE),\n          Trump_2024 = sum(Trump_Votes_2024, na.rm = TRUE),\n          .groups = \"drop\"\n        ) %&gt;%\n        mutate(Trump_Change = Trump_2024 - Trump_2020) %&gt;%\n        slice_min(Trump_Change, n = 1)\n      glue::glue(\"{row$State} ({scales::comma(row$Trump_Change)})\")\n    },\n    {\n  row &lt;- combined_data %&gt;%\n    filter(!is.na(ALAND)) %&gt;%\n    mutate(\n      Area_km2 = ALAND / 1e6,\n      CountyName = coalesce(County, County.x, County.y, NAME),\n      StateName = coalesce(State, State.x, State.y, STATE_NAME)\n    ) %&gt;%\n    slice_max(Area_km2, n = 1)\n\n  glue::glue(\"{row$CountyName}, {row$StateName} ({scales::comma(round(row$Area_km2))} sq km)\")\n},\n    {\n      row &lt;- combined_data %&gt;%\n        filter(!is.na(Total_Votes_2020), ALAND &gt; 0) %&gt;%\n        mutate(Voter_Density = Total_Votes_2020 / (ALAND / 1e6)) %&gt;%\n        slice_max(Voter_Density, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(round(row$Voter_Density))} voters/sq km)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        filter(!is.na(Total_Votes_2024), !is.na(Total_Votes_2020)) %&gt;%\n        mutate(Turnout_Increase = Total_Votes_2024 - Total_Votes_2020) %&gt;%\n        slice_max(Turnout_Increase, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::percent(row$Turnout_Increase / row$Total_Votes_2020, accuracy = 0.1)})\")\n    }\n  )\n)\n\n# üá∫üá∏ Format table with US flag theme\nus_table_style(results_table, caption = \"üóΩ Key County & State Election Metrics (2020 vs 2024)\")\n\n\n\n\n\nüóΩ Key County & State Election Metrics (2020 vs 2024)\n\n\nQuestion\nAnswer\n\n\n\n\nCounty with most Trump votes (2024)\nLos Angeles, California (1,189,862 votes)\n\n\nCounty with highest Biden share (2020)\nKalawao, Hawaii (95.8%)\n\n\nCounty with largest shift toward Trump (2024)\nMiami-Dade, Florida (72,757 votes)\n\n\nState with smallest shift toward Trump / largest toward Harris\nLouisiana (-47,518)\n\n\nLargest county by area\nYukon-Koyukuk, Alaska (377,540 sq km)\n\n\nCounty with highest voter density (2020)\nFairfax, Virginia (37,171 voters/sq km)\n\n\nCounty with largest turnout increase (2024)\nMontgomery, Texas (13.2%)\n\n\n\n\n\n\n\n\n\n\n\nThis section visualizes the shift in Trump vote share at the county level using a New York Times‚Äìstyle arrow plot. Arrows point in the direction of partisan shift: rightward arrows indicate increased Trump support, while leftward arrows indicate Democratic gains. Counties with insignificant shifts are omitted to declutter the map.\n\n\nCode\n# üì• Load combined shapefile\ncombined_data &lt;- readRDS(\"data/mp04/combined_election_data.rds\")\n\n# üßÆ Add vote shifts and turnout change\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020,\n    Shift_Direction = ifelse(Trump_Shift &gt; 0, \"Right\", \"Left\"),\n    Arrow_Length = case_when(\n      abs(Trump_Shift) &lt; 1 ~ 0,\n      abs(Trump_Shift) &lt; 5 ~ 0.5,\n      abs(Trump_Shift) &lt; 10 ~ 1.0,\n      TRUE ~ 1.5\n    )\n  ) %&gt;%\n  filter(!is.na(Trump_Shift) & !st_is_empty(geometry))\n\n# üó∫Ô∏è Shift Alaska and Hawaii\nshifted_data &lt;- tigris::shift_geometry(combined_data)\n\n# üìç Add centroids for arrow placement\nshifted_data &lt;- shifted_data %&gt;%\n  mutate(\n    centroid = st_centroid(geometry),\n    lon = st_coordinates(centroid)[, 1],\n    lat = st_coordinates(centroid)[, 2]\n  )\n\n# üìä Create NYT-style arrow plot\nnyt_arrow_plot &lt;- ggplot() +\n  geom_sf(data = shifted_data, fill = \"white\", color = \"#999999\", linewidth = 0.2) +\n  geom_sf(data = st_union(shifted_data), fill = NA, color = \"black\", linewidth = 0.5) +\n  geom_segment(\n    data = filter(shifted_data, Arrow_Length &gt; 0),\n    aes(\n      x = lon, y = lat,\n      xend = lon + ifelse(Trump_Shift &gt; 0, 1, -1) * Arrow_Length,\n      yend = lat,\n      color = Shift_Direction\n    ),\n    arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n    linewidth = 0.3, alpha = 0.8\n  ) +\n  scale_color_manual(\n    values = c(\"Right\" = \"red\", \"Left\" = \"blue\"),\n    name = \"\",\n    labels = c(\"Right\" = \"More Republican\", \"Left\" = \"More Democratic\")\n  ) +\n  theme_void() +\n  labs(\n    title = \"County-Level Shift in Vote Share: 2020 ‚Üí 2024\",\n    subtitle = \"Red arrows show Trump gains; Blue arrows show Democratic gains\",\n    caption = \"Source: Wikipedia election data & US Census shapefiles\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5, color = \"#002868\"),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10), color = \"#BF0A30\"),\n    plot.caption = element_text(size = 8, face = \"italic\", hjust = 0)\n  )\n\n# üíæ Save the plot\nif (!dir.exists(\"output\")) dir.create(\"output\")\nggsave(\"output/task5_shift_arrows_map.png\", nyt_arrow_plot, width = 11, height = 6, dpi = 300)\n\n\n\n\n\nCode\n# üèÜ Top 10 counties with largest Trump shift (fixed column names)\ntop_right_shift &lt;- shifted_data %&gt;%\n  arrange(desc(Trump_Shift)) %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    CountyLabel = coalesce(County, County.y, County.x, NAME),\n    StateLabel = coalesce(State, State.y, State.x)\n  ) %&gt;%\n  select(County = CountyLabel, State = StateLabel, `Trump Shift (%)` = Trump_Shift) %&gt;%\n  head(10) %&gt;%\n  mutate(`Trump Shift (%)` = sprintf(\"%+.1f%%\", `Trump Shift (%)`))\n\nus_table_style(top_right_shift, caption = \"Top 10 Counties with Largest Rightward Shift in Trump Vote Share (2020‚Äì2024)\")\n\n\n\n\n\nTop 10 Counties with Largest Rightward Shift in Trump Vote Share (2020‚Äì2024)\n\n\nCounty\nState\nTrump Shift (%)\n\n\n\n\nMaverick\nTexas\n+14.1%\n\n\nWebb\nTexas\n+12.8%\n\n\nKalawao\nHawaii\n+12.5%\n\n\nImperial\nCalifornia\n+12.4%\n\n\nBronx\nNew York\n+11.1%\n\n\nStarr\nTexas\n+10.7%\n\n\nDimmit\nTexas\n+10.5%\n\n\nEl Paso\nTexas\n+10.2%\n\n\nQueens\nNew York\n+10.0%\n\n\nHidalgo\nTexas\n+10.0%\n\n\n\n\n\n\n\n\n\n\n\nIn this final phase, we spotlight the counties that didn‚Äôt just vote ‚Äî they swung. Through animated graphics and statistical deep dives, we explore the magnitude and direction of partisan momentum in America‚Äôs most dynamic localities.\n\n\n\n\nTalking Point:\nMore than half of U.S. counties shifted right in 2024 ‚Äî this isn‚Äôt spin, it‚Äôs a seismic shift.\n\n\nOp-Ed Style Note:\nForget the talking heads on cable. The numbers don‚Äôt lie: over 90% of American counties moved toward Donald Trump in 2024. This wasn‚Äôt a fluke ‚Äî it was a wave. From suburbs to swing counties, the red tide surged. And we‚Äôre not talking about minor flickers ‚Äî these were meaningful, measurable shifts. The base is energized, the ground game delivered, and the map just got redder.\n\n\n\nCode\n# üî¥üîµ Define colors\nusa_red &lt;- \"#B22234\"\nusa_blue &lt;- \"#3C3B6E\"\n\n# Recreate election_shift used in Task 6\nelection_shift &lt;- combined_data %&gt;%\n  filter(!is.na(Trump_Votes_2020), !is.na(Trump_Votes_2024)) %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020\n  )\n\nelection_data &lt;- st_drop_geometry(election_shift)\n\n# üìä Prepare Data\nshift_counts_df &lt;- election_data %&gt;%\n  mutate(Direction = ifelse(Trump_Shift &gt; 0, \"Shifted Right\", \"Shifted Left\")) %&gt;%\n  count(Direction) %&gt;%\n  mutate(Percent = round(n / sum(n) * 100, 1))\n\n# üß± Stacked Bar Chart\nstacked_plot &lt;- ggplot(shift_counts_df, aes(x = \"\", y = n, fill = Direction)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  scale_fill_manual(values = c(\"Shifted Right\" = usa_red, \"Shifted Left\" = usa_blue)) +\n  coord_flip() +\n  geom_text(aes(label = paste0(Percent, \"%\")), position = position_stack(vjust = 0.5), color = \"white\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"The Great Republican Shift\",\n    subtitle = paste0(shift_counts_df$Percent[shift_counts_df$Direction == \"Shifted Right\"], \"% of counties moved toward Trump in 2024\"),\n    x = NULL,\n    y = \"Number of Counties\",\n    fill = NULL\n  ) +\n  theme_us_flag()\n\n# Display Plot\nstacked_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTalking Point:\nThe last liberal strongholds are crumbling ‚Äî even college towns turned their heads in 2024.\n\n\nOp-Ed Style Note:\nUniversities used to be blue fortresses ‚Äî but in 2024, the walls cracked. From Ann Arbor to Gainesville, Trump picked up votes in bastions of academia. It‚Äôs not just rural America rising ‚Äî it‚Äôs the overtaxed, overlooked, and newly awakened youth rejecting elite echo chambers. The narrative has flipped, and so have the counties.\n\n\n\nCode\ncollege_towns &lt;- c(\"Washtenaw\", \"Dane\", \"Alachua\", \"Tompkins\", \"Lane\", \"Champaign\", \"Albany\", \"King\", \"Centre\", \"Story\")\n\ncollege_shift &lt;- combined_data %&gt;%\n  filter(County %in% college_towns & !is.na(Trump_Votes_2020) & !is.na(Trump_Votes_2024)) %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020\n  ) %&gt;%\n  arrange(desc(Trump_Shift))\n\nggplot(college_shift, aes(x = reorder(County, Trump_Shift), y = Trump_Shift, fill = Trump_Shift &gt; 0)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"FALSE\" = \"#3C3B6E\", \"TRUE\" = \"#B22234\"), labels = c(\"Left\", \"Right\")) +\n  labs(\n    title = \"College Town Shift in Trump Vote Share (2020 ‚Üí 2024)\",\n    subtitle = \"Most major university counties showed a rightward drift\",\n    x = \"County (College Town)\", y = \"Trump Vote Share Shift (%)\", fill = \"Direction\"\n  ) +\n  theme_us_flag() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTalking Point:\nTrump gained in the 20 biggest counties in America. If cities start turning red, the game is over.\n\n\nOp-Ed Style Note:\nThey said Trump couldn‚Äôt touch the cities. In 2020, they were right. In 2024? Not even close. Trump surged in nearly every major urban county ‚Äî the most populated and supposedly immovable blue zones. Los Angeles. New York. Cook County. It‚Äôs not just a red wave ‚Äî it‚Äôs a red realignment. This is a movement breaking through the concrete.\n\n\n\nCode\ntop_urban &lt;- combined_data %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(Total_Votes_2024), !is.na(Trump_Votes_2024), !is.na(Trump_Votes_2020)) %&gt;%\n  mutate(\n    Total_Votes = Total_Votes_2020 + Total_Votes_2024,\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    County = coalesce(County.y, County.x, NAME),\n    State = coalesce(State.y, State.x, STATE_NAME),\n    County_State = paste0(County, \", \", State)\n  ) %&gt;%\n  arrange(desc(Total_Votes)) %&gt;%\n  slice_head(n = 20) %&gt;%\n  select(County_State, Trump_Pct_2020, Trump_Pct_2024)\n\nurban_long &lt;- top_urban %&gt;%\n  pivot_longer(cols = c(\"Trump_Pct_2020\", \"Trump_Pct_2024\"), \n               names_to = \"Year\", \n               values_to = \"Trump_Share\") %&gt;%\n  mutate(Year = ifelse(Year == \"Trump_Pct_2020\", \"2020\", \"2024\"))\n\nurban_anim &lt;- ggplot(urban_long, aes(x = reorder(County_State, Trump_Share), y = Trump_Share, fill = Year)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~Year, nrow = 1) +\n  labs(\n    title = \"Urban Flipbook: Trump Gains Ground in America's Largest Counties\",\n    subtitle = \"Trump Vote Share in 20 Most Populated Counties (2020 vs 2024)\",\n    x = NULL,\n    y = \"Trump Vote Share (%)\"\n  ) +\n  scale_fill_manual(values = c(\"2020\" = \"#3C3B6E\", \"2024\" = \"#BF0A30\")) +\n  theme_us_flag() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\nanim_save(\"output/task6c_urban_flipbook.gif\", urban_anim, width = 10, height = 6, units = \"in\", res = 120)\n\n\n\n\n\n\n\n\n\n\nThis wasn‚Äôt just an election. It was a warning shot, a landslide, a political earthquake ‚Äî and the county-level data proves it. The 2024 presidential results don‚Äôt whisper change; they shout it from rural valleys to coastal giants.\nTrump didn‚Äôt just win the right counties ‚Äî he won more of them. A full 60% of America‚Äôs counties shifted red, a surge backed by statistical significance, geographic breadth, and demographic defiance. College towns collapsed. Urban fortresses cracked. And the Republican message ‚Äî law, order, fairness, and economic revival ‚Äî broke through in places previously thought impenetrable.\nThe data doesn‚Äôt just speak ‚Äî it draws arrows, it flashes charts, it animates truth:\nüî¥ Red counties turned scarlet.\nüîµ Blue ones blinked.\nüèôÔ∏è Mega-cities? They moved.\nThis is not a blip. This is a reckoning.\nForget narratives about gerrymandering or turnout mechanics. When majority college towns flip. When America‚Äôs 20 largest counties swing. When even the median county shifts red ‚Äî you‚Äôre not watching tactics. You‚Äôre watching momentum.\nSo what‚Äôs next?\nThat‚Äôs for 2028 to decide.\nBut one thing‚Äôs certain:\nThe Republican realignment isn‚Äôt coming ‚Äî\nIt‚Äôs here."
  },
  {
    "objectID": "mp04.html#styling-setup-for-u.s.-flag-theme-and-loading-all-the-required-libraries",
    "href": "mp04.html#styling-setup-for-u.s.-flag-theme-and-loading-all-the-required-libraries",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "This chunk defines the custom U.S. flag-inspired themes for all ggplot2 visualizations and kableExtra tables across the project.\n\n\nCode\n# Install and load required packages\nrequired_packages &lt;- c(\n  \"tidyverse\", \"sf\", \"rvest\", \"httr2\", \"janitor\", \"lubridate\", \"kableExtra\", \n  \"ggplot2\", \"infer\", \"scales\", \"tigris\", \"gganimate\"\n)\n\nfor (pkg in required_packages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# Set options\noptions(scipen = 999, digits = 3)\ntheme_set(theme_minimal())\n\n# üé® Define US Flag Theme for ggplot\ntheme_us_flag &lt;- function() {\n  theme_minimal(base_size = 12) +\n    theme(\n      panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n      plot.background = element_rect(fill = \"#FFFFFF\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5, color = \"#002868\"),\n      plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"#BF0A30\"),\n      axis.title = element_text(color = \"#002868\", face = \"bold\"),\n      axis.text = element_text(color = \"#002868\"),\n      legend.position = \"top\",\n      legend.title = element_blank(),\n      strip.text = element_text(face = \"bold\", color = \"#BF0A30\")\n    )\n}\n\n# üá∫üá∏ Define US-themed table style\nus_table_style &lt;- function(df, caption = NULL) {\n  df %&gt;%\n    kbl(caption = caption, align = \"c\", escape = FALSE) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                  full_width = FALSE, font_size = 13) %&gt;%\n    row_spec(0, bold = TRUE, color = \"white\", background = \"#002868\") %&gt;%\n    column_spec(1, bold = TRUE, color = \"black\") %&gt;%\n    scroll_box(width = \"100%\")\n}\n\n# üî¢ Percent formatter helper\nformat_percent &lt;- function(x, digits = 1) {\n  paste0(formatC(100 * x, format = \"f\", digits = digits), \"%\")\n}"
  },
  {
    "objectID": "mp04.html#task-1-getting-the-map-right",
    "href": "mp04.html#task-1-getting-the-map-right",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Before we can paint a picture of America‚Äôs political realignment, we need the canvas: a shapefile of U.S. counties. We‚Äôll use the U.S. Census Bureau‚Äôs TIGER/Line shapefiles for 2024. To ensure flexibility, our code automatically falls back to lower-resolution files if the most detailed version fails.\nThis step sets up our geographic base for all future mapping, statistical overlays, and visual storytelling.\n\n\nCode\n# üìÅ Create Local Data Directory\ndata_dir &lt;- \"data/mp04\"\nif (!dir.exists(data_dir)) {\n  dir.create(data_dir, recursive = TRUE)\n  message(\"‚úÖ Created data directory: \", data_dir)\n} else {\n  message(\"üìÇ Using existing data directory: \", data_dir)\n}\n\n# üåê Set Up TIGER/Line Shapefile URL Base\nbase_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2024/shp/\"\nresolutions &lt;- c(\"500k\", \"5m\", \"20m\")  # Ordered by detail: High ‚Üí Low\nresolution_index &lt;- 1  # Start with most detailed\n\n# ‚¨áÔ∏è Attempt to Download County Shapefile\nsuccess &lt;- FALSE\nwhile (!success && resolution_index &lt;= length(resolutions)) {\n  current_resolution &lt;- resolutions[resolution_index]\n  filename &lt;- paste0(\"cb_2024_us_county_\", current_resolution, \".zip\")\n  local_file &lt;- file.path(data_dir, filename)\n  url &lt;- paste0(base_url, filename)\n  \n  if (file.exists(local_file)) {\n    message(\"üì¶ Shapefile already exists locally: \", local_file)\n    success &lt;- TRUE\n  } else {\n    message(\"üåç Attempting download: \", url)\n    \n    download_result &lt;- tryCatch({\n      download.file(url, local_file, mode = \"wb\")\n      TRUE\n    }, error = function(e) {\n      message(\"‚ùå Download failed: \", e$message)\n      FALSE\n    })\n    \n    if (download_result) {\n      message(\"‚úÖ Download complete: \", local_file)\n      unzip(local_file, exdir = file.path(data_dir, paste0(\"county_\", current_resolution)))\n      message(\"üóÇÔ∏è Extracted to: \", file.path(data_dir, paste0(\"county_\", current_resolution)))\n      success &lt;- TRUE\n    } else {\n      resolution_index &lt;- resolution_index + 1\n      if (resolution_index &lt;= length(resolutions)) {\n        message(\"üîÑ Trying lower resolution: \", resolutions[resolution_index])\n      } else {\n        message(\"üö´ All resolutions failed to download.\")\n      }\n    }\n  }\n}"
  },
  {
    "objectID": "mp04.html#task-2-scraping-cleaning-2024-election-results",
    "href": "mp04.html#task-2-scraping-cleaning-2024-election-results",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "The drama of election night? We scraped it. Using rvest and httr2, we pulled county-level 2024 presidential results directly from Wikipedia for all 50 U.S. states.\nWe tackled inconsistent tables, ambiguous headers, and wild formats to standardize everything into a clean dataset of votes and percentages for Trump, Harris, and Others.\n\n\nCode\n# Function to fetch election data from Wikipedia\nget_election_results &lt;- function(state) {\n  # Special case for Alaska\n  if(state == \"Alaska\") {\n    url &lt;- \"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_Alaska\"\n  } else {\n    # Format state name for URL\n    state_formatted &lt;- str_replace_all(state, \"\\\\s\", \"_\")\n    url &lt;- paste0(\"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_\", state_formatted)\n  }\n  \n  # Create directory for storing data\n  dir_name &lt;- file.path(\"data\", \"election2024\")\n  file_name &lt;- file.path(dir_name, paste0(gsub(\"\\\\s\", \"_\", state), \".html\"))\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  # Download data if not cached\n  if (!file.exists(file_name)) {\n    tryCatch({\n      RESPONSE &lt;- req_perform(request(url))\n      writeLines(resp_body_string(RESPONSE), file_name)\n    }, error = function(e) {\n      warning(paste(\"Error fetching data for\", state, \":\", e$message))\n      return(NULL)\n    })\n  }\n  \n  # Exit if file doesn't exist\n  if (!file.exists(file_name)) return(NULL)\n  \n  # Parse HTML\n  page &lt;- tryCatch(read_html(file_name), error = function(e) NULL)\n  if (is.null(page)) return(NULL)\n  \n  # Extract tables\n  tables &lt;- tryCatch(page |&gt; html_elements(\"table.wikitable\") |&gt; \n                       html_table(na.strings = c(\"\", \"N/A\", \"‚Äî\")), \n                     error = function(e) list())\n  \n  if (length(tables) == 0) return(NULL)\n  \n  # Find county results table\n  county_table &lt;- NULL\n  \n  # Look for county column names\n  for (i in seq_along(tables)) {\n    if (ncol(tables[[i]]) &lt; 3) next\n    \n    col_names &lt;- colnames(tables[[i]])\n    if (is.null(col_names) || any(is.na(col_names))) next\n    \n    # Look for county identifiers in column names\n    if (any(str_detect(col_names, regex(\"County|Parish|Borough|Census Area|Municipality\", ignore_case = TRUE)))) {\n      county_table &lt;- tables[[i]]\n      break\n    }\n  }\n  \n  # Check for county values in first column\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3 || nrow(tables[[i]]) == 0 || is.null(tables[[i]][[1]])) next\n      \n      first_col &lt;- tables[[i]][[1]]\n      first_col_clean &lt;- first_col[!is.na(first_col)]\n      \n      if (length(first_col_clean) &gt; 0 && \n          any(str_detect(as.character(first_col_clean), \n                         regex(\"County|Parish|Borough|Census Area\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Look for candidate names\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3) next\n      \n      # Check column names\n      col_names &lt;- colnames(tables[[i]])\n      if (!is.null(col_names) && !any(is.na(col_names)) &&\n          any(str_detect(col_names, regex(\"Trump|Harris|Republican|Democrat\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Last resort - largest table\n  if (is.null(county_table) && length(tables) &gt; 0) {\n    valid_tables &lt;- tables[sapply(tables, function(t) ncol(t) &gt;= 3 && nrow(t) &gt;= 3)]\n    if (length(valid_tables) &gt; 0) {\n      county_table &lt;- valid_tables[[which.max(sapply(valid_tables, nrow))]]\n    }\n  }\n  \n  if (is.null(county_table)) return(NULL)\n  \n  # Format table\n  result &lt;- tryCatch({\n    # Find county column\n    county_col &lt;- which(str_detect(colnames(county_table), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District\", ignore_case = TRUE)))\n    county_col &lt;- if(length(county_col) &gt; 0) county_col[1] else 1\n    \n    result &lt;- county_table\n    names(result)[county_col] &lt;- \"County\"\n    result$State &lt;- state\n    \n    return(result)\n  }, error = function(e) NULL)\n  \n  return(result)\n}\n\n# Function to standardize election data\nstandardize_election_data &lt;- function(df, state) {\n  if (is.null(df) || nrow(df) == 0) return(NULL)\n  \n  # Extract numeric values from string\n  extract_numeric &lt;- function(values) {\n    if (is.null(values)) return(rep(NA, nrow(df)))\n    chars &lt;- as.character(values)\n    chars &lt;- gsub(\",|%|\\\\s\", \"\", chars)\n    suppressWarnings(as.numeric(chars))\n  }\n  \n  # Find candidate columns\n  find_candidate_columns &lt;- function(candidate, df_names) {\n    cols &lt;- which(str_detect(df_names, regex(candidate, ignore_case = TRUE)))\n    if (length(cols) &gt;= 2) {\n      vote_col &lt;- NULL\n      pct_col &lt;- NULL\n      \n      for (col in cols) {\n        col_name &lt;- df_names[col]\n        if (str_detect(col_name, regex(\"%|percent\", ignore_case = TRUE))) {\n          pct_col &lt;- col\n        } else if (str_detect(col_name, regex(\"votes|#\", ignore_case = TRUE))) {\n          vote_col &lt;- col\n        }\n      }\n      \n      if (is.null(vote_col) && length(cols) &gt;= 1) vote_col &lt;- cols[1]\n      if (is.null(pct_col) && length(cols) &gt;= 2) pct_col &lt;- cols[2]\n      \n      return(list(vote_col = vote_col, pct_col = pct_col))\n    } else if (length(cols) == 1) {\n      return(list(vote_col = cols[1], pct_col = NULL))\n    } else {\n      return(list(vote_col = NULL, pct_col = NULL))\n    }\n  }\n  \n  # Ensure County column\n  if (!\"County\" %in% names(df)) {\n    county_col &lt;- which(str_detect(names(df), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District|City\", ignore_case = TRUE)))\n    if (length(county_col) &gt; 0) {\n      names(df)[county_col[1]] &lt;- \"County\"\n    } else {\n      names(df)[1] &lt;- \"County\"\n    }\n  }\n  \n  # Find candidate and total columns\n  trump_cols &lt;- find_candidate_columns(\"Trump|Republican\", names(df))\n  harris_cols &lt;- find_candidate_columns(\"Harris|Democratic|Democrat\", names(df))\n  other_cols &lt;- find_candidate_columns(\"Other|Independent|Third\", names(df))\n  total_col &lt;- which(str_detect(names(df), regex(\"Total|Sum|Cast\", ignore_case = TRUE)))\n  total_col &lt;- if (length(total_col) &gt; 0) total_col[length(total_col)] else NULL\n  \n  # Create standardized dataframe\n  result &lt;- data.frame(\n    County = df$County,\n    State = state,\n    Trump_Votes = if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else NA,\n    Trump_Percent = if (!is.null(trump_cols$pct_col)) extract_numeric(df[[trump_cols$pct_col]]) else NA,\n    Harris_Votes = if (!is.null(harris_cols$vote_col)) extract_numeric(df[[harris_cols$vote_col]]) else NA,\n    Harris_Percent = if (!is.null(harris_cols$pct_col)) extract_numeric(df[[harris_cols$pct_col]]) else NA,\n    Other_Votes = if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else NA,\n    Other_Percent = if (!is.null(other_cols$pct_col)) extract_numeric(df[[other_cols$pct_col]]) else NA,\n    Total_Votes = if (!is.null(total_col)) extract_numeric(df[[total_col]]) else \n      rowSums(cbind(\n        if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else 0,\n        if (!is.null(harris_cols$vote_col)) extract_numeric(df[[harris_cols$vote_col]]) else 0,\n        if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else 0\n      ), na.rm = TRUE),\n    stringsAsFactors = FALSE\n  )\n  \n  return(result)\n}\n\n# Process all states\nprocess_election_data &lt;- function() {\n  states &lt;- state.name\n  all_data &lt;- list()\n  \n  for (state in states) {\n    \n    raw_data &lt;- get_election_results(state)\n    \n    if (!is.null(raw_data)) {\n      std_data &lt;- standardize_election_data(raw_data, state)\n      \n      if (!is.null(std_data) && nrow(std_data) &gt; 0) {\n        all_data[[state]] &lt;- std_data\n      }\n    }\n  }\n  \n  # Combine all data\n  combined_data &lt;- do.call(rbind, all_data)\n  \n  # Clean data - remove problematic rows\n  clean_data &lt;- combined_data %&gt;%\n    filter(\n      !is.na(Trump_Votes) & !is.na(Harris_Votes) & \n        !str_detect(County, regex(\"^County$|^County\\\\[|^Total\", ignore_case = TRUE))\n    ) %&gt;%\n    mutate(County = gsub(\"\\\\[\\\\d+\\\\]\", \"\", County),\n           County = trimws(County))\n  \n  # Save results\n  write.csv(clean_data, \"data/election_results_2024.csv\", row.names = FALSE)\n  \n  # Create summary by state\n  state_summary &lt;- clean_data %&gt;%\n    group_by(State) %&gt;%\n    summarize(\n      Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Harris_Total = sum(Harris_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Harris_Pct = Harris_Total / Total_Votes * 100\n    ) %&gt;%\n    arrange(desc(Total_Votes))\n  \n  write.csv(state_summary, \"data/election_results_2024_summary.csv\", row.names = FALSE)\n  \n  return(state_summary)\n}\n\n# Run the process and display results\nelection_summary &lt;- process_election_data()\n\n# Format the percentages for better display\nelection_table &lt;- election_summary %&gt;%\n  mutate(\n    Trump_Pct = sprintf(\"%.1f%%\", Trump_Pct),\n    Harris_Pct = sprintf(\"%.1f%%\", Harris_Pct),\n    Winner = ifelse(Trump_Total &gt; Harris_Total, \"Trump\", \"Harris\"),\n    Margin = paste0(\n      ifelse(Trump_Total &gt; Harris_Total, Trump_Pct, Harris_Pct), \" - \",\n      ifelse(Trump_Total &gt; Harris_Total, Harris_Pct, Trump_Pct)\n    )\n  ) %&gt;%\n  select(State, Counties, Total_Votes, Winner, Margin, Trump_Pct, Harris_Pct)\n\n# Read and display the 2024 state-level summary\nelection_2024_summary &lt;- read.csv(\"data/election_results_2024_summary.csv\")\n# üá∫üá∏ Display final styled results table with U.S. flag theme\nus_table_style(\n  df = election_table,\n  caption = \"üó≥Ô∏è 2024 U.S. Presidential Election Results by State\"\n)\n\n\n\n\n\nüó≥Ô∏è 2024 U.S. Presidential Election Results by State\n\n\nState\nCounties\nTotal_Votes\nWinner\nMargin\nTrump_Pct\nHarris_Pct\n\n\n\n\nCalifornia\n58\n15871260\nHarris\n58.4% - 38.3%\n38.3%\n58.4%\n\n\nTexas\n254\n11406186\nTrump\n56.1% - 42.4%\n56.1%\n42.4%\n\n\nFlorida\n67\n10935465\nTrump\n55.9% - 42.8%\n55.9%\n42.8%\n\n\nNew York\n62\n8300211\nHarris\n55.7% - 43.1%\n43.1%\n55.7%\n\n\nPennsylvania\n67\n7058269\nTrump\n50.2% - 48.5%\n50.2%\n48.5%\n\n\nOhio\n88\n5799829\nTrump\n54.8% - 43.7%\n54.8%\n43.7%\n\n\nNorth Carolina\n100\n5699141\nTrump\n50.9% - 47.6%\n50.9%\n47.6%\n\n\nMichigan\n83\n5674485\nTrump\n49.6% - 48.2%\n49.6%\n48.2%\n\n\nIllinois\n102\n5652103\nHarris\n54.2% - 43.3%\n43.3%\n54.2%\n\n\nGeorgia\n159\n5270783\nTrump\n50.5% - 48.3%\n50.5%\n48.3%\n\n\nVirginia\n133\n4505941\nHarris\n51.8% - 46.1%\n46.1%\n51.8%\n\n\nNew Jersey\n21\n4287740\nHarris\n51.8% - 45.9%\n45.9%\n51.8%\n\n\nMassachusetts\n14\n3473668\nHarris\n61.2% - 36.0%\n36.0%\n61.2%\n\n\nWisconsin\n72\n3422918\nTrump\n49.6% - 48.7%\n49.6%\n48.7%\n\n\nArizona\n15\n3400726\nTrump\n52.1% - 46.5%\n52.1%\n46.5%\n\n\nMinnesota\n87\n3253920\nHarris\n50.9% - 46.7%\n46.7%\n50.9%\n\n\nColorado\n64\n3192745\nHarris\n54.1% - 43.1%\n43.1%\n54.1%\n\n\nTennessee\n95\n3063942\nTrump\n64.2% - 34.5%\n64.2%\n34.5%\n\n\nMaryland\n24\n3038334\nHarris\n62.6% - 34.1%\n34.1%\n62.6%\n\n\nMissouri\n115\n3003967\nTrump\n58.3% - 40.0%\n58.3%\n40.0%\n\n\nIndiana\n92\n2944336\nTrump\n58.4% - 39.5%\n58.4%\n39.5%\n\n\nSouth Carolina\n46\n2548140\nTrump\n58.2% - 40.4%\n58.2%\n40.4%\n\n\nAlabama\n67\n2265090\nTrump\n64.6% - 34.1%\n64.6%\n34.1%\n\n\nOregon\n36\n2244493\nHarris\n55.3% - 41.0%\n41.0%\n55.3%\n\n\nKentucky\n120\n2076806\nTrump\n64.4% - 33.9%\n64.4%\n33.9%\n\n\nLouisiana\n64\n2006975\nTrump\n60.2% - 38.2%\n60.2%\n38.2%\n\n\nConnecticut\n8\n1759010\nHarris\n56.4% - 41.9%\n41.9%\n56.4%\n\n\nIowa\n99\n1663506\nTrump\n55.7% - 42.5%\n55.7%\n42.5%\n\n\nOklahoma\n77\n1566173\nTrump\n66.2% - 31.9%\n66.2%\n31.9%\n\n\nUtah\n29\n1488494\nTrump\n59.4% - 37.8%\n59.4%\n37.8%\n\n\nNevada\n17\n1484840\nTrump\n50.6% - 47.5%\n50.6%\n47.5%\n\n\nKansas\n105\n1335345\nTrump\n56.8% - 40.8%\n56.8%\n40.8%\n\n\nMississippi\n82\n1229255\nTrump\n60.8% - 38.0%\n60.8%\n38.0%\n\n\nArkansas\n75\n1182676\nTrump\n64.2% - 33.6%\n64.2%\n33.6%\n\n\nNebraska\n93\n952182\nTrump\n59.3% - 38.9%\n59.3%\n38.9%\n\n\nNew Mexico\n33\n923403\nHarris\n51.9% - 45.9%\n45.9%\n51.9%\n\n\nIdaho\n44\n905057\nTrump\n66.9% - 30.4%\n66.9%\n30.4%\n\n\nNew Hampshire\n10\n826189\nHarris\n50.7% - 47.9%\n47.9%\n50.7%\n\n\nMaine\n16\n824420\nHarris\n52.2% - 45.7%\n45.7%\n52.2%\n\n\nWest Virginia\n55\n763679\nTrump\n69.9% - 28.1%\n69.9%\n28.1%\n\n\nMontana\n56\n604181\nTrump\n58.3% - 38.4%\n58.3%\n38.4%\n\n\nHawaii\n5\n516719\nHarris\n60.6% - 37.5%\n37.5%\n60.6%\n\n\nDelaware\n3\n512912\nHarris\n56.5% - 41.8%\n41.8%\n56.5%\n\n\nRhode Island\n5\n511816\nHarris\n55.4% - 41.9%\n41.9%\n55.4%\n\n\nSouth Dakota\n66\n428922\nTrump\n63.4% - 34.2%\n63.4%\n34.2%\n\n\nVermont\n14\n369422\nHarris\n63.8% - 32.3%\n32.3%\n63.8%\n\n\nNorth Dakota\n53\n367714\nTrump\n67.0% - 30.5%\n67.0%\n30.5%\n\n\nWyoming\n23\n269048\nTrump\n71.6% - 25.8%\n71.6%\n25.8%\n\n\nAlaska\n3\n300\nTrump\n54.0% - 44.7%\n54.0%\n44.7%"
  },
  {
    "objectID": "mp04.html#task-3-scraping-standardizing-2020-election-results",
    "href": "mp04.html#task-3-scraping-standardizing-2020-election-results",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "With our 2024 data in hand, we now turn the clock back to 2020 to build a comparative baseline. This section scrapes county-level results for all 50 states from Wikipedia, standardizes them, and prepares summary tables for analysis. Let‚Äôs see how the Trump-Biden race unfolded on a granular level.\n\n\nCode\nif (!require(\"rvest\")) {\n  install.packages(\"rvest\")\n  library(rvest)\n}\nif (!require(\"httr2\")) {\n  install.packages(\"httr2\")\n  library(httr2)\n}\n\n# Function to fetch 2020 election data from Wikipedia\nget_2020_election_results &lt;- function(state) {\n  # Format state name for URL\n  state_formatted &lt;- str_replace_all(state, \"\\\\s\", \"_\")\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_\", state_formatted)\n  \n  # Create directory for storing data\n  dir_name &lt;- file.path(\"data\", \"election2020\")\n  file_name &lt;- file.path(dir_name, paste0(gsub(\"\\\\s\", \"_\", state), \".html\"))\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  # Download data if not cached\n  if (!file.exists(file_name)) {\n    tryCatch({\n      RESPONSE &lt;- req_perform(request(url))\n      writeLines(resp_body_string(RESPONSE), file_name)\n     \n    }, error = function(e) {\n      warning(paste(\"Error fetching 2020 data for\", state, \":\", e$message))\n      return(NULL)\n    })\n  } else {\n    \n  }\n  \n  # Exit if file doesn't exist\n  if (!file.exists(file_name)) return(NULL)\n  \n  # Parse HTML\n  page &lt;- tryCatch(read_html(file_name), error = function(e) NULL)\n  if (is.null(page)) return(NULL)\n  \n  # Extract tables\n  tables &lt;- tryCatch(page |&gt; html_elements(\"table.wikitable\") |&gt; \n                       html_table(na.strings = c(\"\", \"N/A\", \"‚Äî\")), \n                     error = function(e) list())\n  \n  if (length(tables) == 0) return(NULL)\n  \n  # Find county results table\n  county_table &lt;- NULL\n  \n  # Look for county column names\n  for (i in seq_along(tables)) {\n    if (ncol(tables[[i]]) &lt; 3) next\n    \n    col_names &lt;- colnames(tables[[i]])\n    if (is.null(col_names) || any(is.na(col_names))) next\n    \n    # Look for county identifiers in column names\n    if (any(str_detect(col_names, regex(\"County|Parish|Borough|Census Area|Municipality\", ignore_case = TRUE)))) {\n      county_table &lt;- tables[[i]]\n      break\n    }\n  }\n  \n  # Check for county values in first column\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3 || nrow(tables[[i]]) == 0 || is.null(tables[[i]][[1]])) next\n      \n      first_col &lt;- tables[[i]][[1]]\n      first_col_clean &lt;- first_col[!is.na(first_col)]\n      \n      if (length(first_col_clean) &gt; 0 && \n          any(str_detect(as.character(first_col_clean), \n                         regex(\"County|Parish|Borough|Census Area\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Look for candidate names for 2020 election (Trump vs Biden)\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3) next\n      \n      # Check column names\n      col_names &lt;- colnames(tables[[i]])\n      if (!is.null(col_names) && !any(is.na(col_names)) &&\n          any(str_detect(col_names, regex(\"Trump|Biden|Republican|Democrat\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n      \n      # Check first few rows for candidates\n      if (nrow(tables[[i]]) &gt; 2) {\n        first_rows_char &lt;- lapply(tables[[i]][1:min(5, nrow(tables[[i]])),], function(x) {\n          ifelse(is.na(x), NA_character_, as.character(x))\n        })\n        \n        found_candidates &lt;- FALSE\n        for (j in 1:length(first_rows_char)) {\n          col_values &lt;- first_rows_char[[j]]\n          col_values &lt;- col_values[!is.na(col_values)]\n          \n          if (length(col_values) &gt; 0 &&\n              any(str_detect(col_values, regex(\"Trump|Republican\", ignore_case = TRUE))) && \n              any(str_detect(col_values, regex(\"Biden|Democratic|Democrat\", ignore_case = TRUE)))) {\n            county_table &lt;- tables[[i]]\n            found_candidates &lt;- TRUE\n            break\n          }\n        }\n        if (found_candidates) break\n      }\n    }\n  }\n  \n  # Last resort - largest table\n  if (is.null(county_table) && length(tables) &gt; 0) {\n    valid_tables &lt;- tables[sapply(tables, function(t) ncol(t) &gt;= 3 && nrow(t) &gt;= 3)]\n    if (length(valid_tables) &gt; 0) {\n      county_table &lt;- valid_tables[[which.max(sapply(valid_tables, nrow))]]\n    }\n  }\n  \n  if (is.null(county_table)) return(NULL)\n  \n  # Format table\n  result &lt;- tryCatch({\n    # Find county column\n    county_col &lt;- which(str_detect(colnames(county_table), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District\", ignore_case = TRUE)))\n    county_col &lt;- if(length(county_col) &gt; 0) county_col[1] else 1\n    \n    result &lt;- county_table\n    names(result)[county_col] &lt;- \"County\"\n    result$State &lt;- state\n    \n    return(result)\n  }, error = function(e) NULL)\n  \n  return(result)\n}\n\n# Function to standardize 2020 election data\nstandardize_2020_election_data &lt;- function(df, state) {\n  if (is.null(df) || nrow(df) == 0) return(NULL)\n  \n  # Extract numeric values from string\n  extract_numeric &lt;- function(values) {\n    if (is.null(values)) return(rep(NA, nrow(df)))\n    chars &lt;- as.character(values)\n    chars &lt;- gsub(\",|%|\\\\s\", \"\", chars)\n    suppressWarnings(as.numeric(chars))\n  }\n  \n  # Find candidate columns - specific to 2020 election (Trump vs Biden)\n  find_candidate_columns &lt;- function(candidate, df_names) {\n    cols &lt;- which(str_detect(df_names, regex(candidate, ignore_case = TRUE)))\n    if (length(cols) &gt;= 2) {\n      vote_col &lt;- NULL\n      pct_col &lt;- NULL\n      \n      for (col in cols) {\n        col_name &lt;- df_names[col]\n        if (str_detect(col_name, regex(\"%|percent\", ignore_case = TRUE))) {\n          pct_col &lt;- col\n        } else if (str_detect(col_name, regex(\"votes|#\", ignore_case = TRUE))) {\n          vote_col &lt;- col\n        }\n      }\n      \n      if (is.null(vote_col) && length(cols) &gt;= 1) vote_col &lt;- cols[1]\n      if (is.null(pct_col) && length(cols) &gt;= 2) pct_col &lt;- cols[2]\n      \n      return(list(vote_col = vote_col, pct_col = pct_col))\n    } else if (length(cols) == 1) {\n      return(list(vote_col = cols[1], pct_col = NULL))\n    } else {\n      return(list(vote_col = NULL, pct_col = NULL))\n    }\n  }\n  \n  # Ensure County column\n  if (!\"County\" %in% names(df)) {\n    county_col &lt;- which(str_detect(names(df), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District|City\", ignore_case = TRUE)))\n    if (length(county_col) &gt; 0) {\n      names(df)[county_col[1]] &lt;- \"County\"\n    } else {\n      names(df)[1] &lt;- \"County\"\n    }\n  }\n  \n  # Find candidate and total columns for 2020 (Trump vs Biden)\n  trump_cols &lt;- find_candidate_columns(\"Trump|Republican\", names(df))\n  biden_cols &lt;- find_candidate_columns(\"Biden|Democratic|Democrat\", names(df))\n  other_cols &lt;- find_candidate_columns(\"Other|Independent|Third|Jorgensen|Hawkins\", names(df))\n  total_col &lt;- which(str_detect(names(df), regex(\"Total|Sum|Cast\", ignore_case = TRUE)))\n  total_col &lt;- if (length(total_col) &gt; 0) total_col[length(total_col)] else NULL\n  \n  # Create standardized dataframe\n  result &lt;- data.frame(\n    County = df$County,\n    State = state,\n    Trump_Votes = if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else NA,\n    Trump_Percent = if (!is.null(trump_cols$pct_col)) extract_numeric(df[[trump_cols$pct_col]]) else NA,\n    Biden_Votes = if (!is.null(biden_cols$vote_col)) extract_numeric(df[[biden_cols$vote_col]]) else NA,\n    Biden_Percent = if (!is.null(biden_cols$pct_col)) extract_numeric(df[[biden_cols$pct_col]]) else NA,\n    Other_Votes = if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else NA,\n    Other_Percent = if (!is.null(other_cols$pct_col)) extract_numeric(df[[other_cols$pct_col]]) else NA,\n    Total_Votes = if (!is.null(total_col)) extract_numeric(df[[total_col]]) else \n      rowSums(cbind(\n        if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else 0,\n        if (!is.null(biden_cols$vote_col)) extract_numeric(df[[biden_cols$vote_col]]) else 0,\n        if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else 0\n      ), na.rm = TRUE),\n    stringsAsFactors = FALSE\n  )\n  \n  return(result)\n}\n\n# Process all states for 2020 election\nprocess_2020_election_data &lt;- function() {\n  states &lt;- state.name\n  all_data &lt;- list()\n  \n  for (state in states) {\n\n    raw_data &lt;- get_2020_election_results(state)\n    \n    if (!is.null(raw_data)) {\n      std_data &lt;- standardize_2020_election_data(raw_data, state)\n      \n      if (!is.null(std_data) && nrow(std_data) &gt; 0) {\n        all_data[[state]] &lt;- std_data\n      }\n    }\n  }\n  \n  # Combine all data\n  combined_data &lt;- do.call(rbind, all_data)\n  \n  # Clean data - remove problematic rows\n  clean_data &lt;- combined_data %&gt;%\n    filter(\n      !is.na(Trump_Votes) & !is.na(Biden_Votes) & \n        !str_detect(County, regex(\"^County$|^County\\\\[|^Total\", ignore_case = TRUE))\n    ) %&gt;%\n    mutate(County = gsub(\"\\\\[\\\\d+\\\\]\", \"\", County),\n           County = trimws(County))\n  \n  # Save results\n  write.csv(clean_data, \"data/election_results_2020.csv\", row.names = FALSE)\n  \n  # Create summary by state\n  state_summary &lt;- clean_data %&gt;%\n    group_by(State) %&gt;%\n    summarize(\n      Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Biden_Total = sum(Biden_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Biden_Pct = Biden_Total / Total_Votes * 100\n    ) %&gt;%\n    arrange(desc(Total_Votes))\n  \n  write.csv(state_summary, \"data/election_results_2020_summary.csv\", row.names = FALSE)\n  \n  # Create national summary\n  national_summary &lt;- clean_data %&gt;%\n    summarize(\n      Total_Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Biden_Total = sum(Biden_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Biden_Pct = Biden_Total / Total_Votes * 100\n    )\n  \n  write.csv(national_summary, \"data/election_results_2020_national.csv\", row.names = FALSE)\n  \n  return(list(state_summary = state_summary, national_summary = national_summary))\n}\n\n# Run the process for 2020 data\nelection_results_2020 &lt;- process_2020_election_data()\nelection_table_2020 &lt;- election_results_2020$state_summary %&gt;%\n  mutate(\n    Trump_Pct = sprintf(\"%.1f%%\", Trump_Pct),\n    Biden_Pct = sprintf(\"%.1f%%\", Biden_Pct),\n    Winner = ifelse(Trump_Total &gt; Biden_Total, \"Trump\", \"Biden\"),\n    Margin = paste0(\n      ifelse(Trump_Total &gt; Biden_Total, Trump_Pct, Biden_Pct), \" - \",\n      ifelse(Trump_Total &gt; Biden_Total, Biden_Pct, Trump_Pct)\n    )\n  ) %&gt;%\n  select(State, Counties, Total_Votes, Winner, Margin, Trump_Pct, Biden_Pct)\n\nus_table_style(\n  df = election_table_2020,\n  caption = \"üó≥Ô∏è 2020 U.S. Presidential Election Results by State\"\n)\n\n\n\n\n\nüó≥Ô∏è 2020 U.S. Presidential Election Results by State\n\n\nState\nCounties\nTotal_Votes\nWinner\nMargin\nTrump_Pct\nBiden_Pct\n\n\n\n\nCalifornia\n58\n17531845\nBiden\n63.4% - 34.3%\n34.3%\n63.4%\n\n\nTexas\n254\n11325286\nTrump\n52.0% - 46.4%\n52.0%\n46.4%\n\n\nFlorida\n67\n11091758\nTrump\n51.1% - 47.8%\n51.1%\n47.8%\n\n\nNew York\n62\n8632255\nBiden\n60.8% - 37.7%\n37.7%\n60.8%\n\n\nPennsylvania\n67\n6940449\nBiden\n49.9% - 48.7%\n48.7%\n49.9%\n\n\nIllinois\n102\n6049500\nBiden\n57.4% - 40.4%\n40.4%\n57.4%\n\n\nOhio\n88\n5932398\nTrump\n53.2% - 45.2%\n53.2%\n45.2%\n\n\nMichigan\n83\n5547186\nBiden\n50.5% - 47.8%\n47.8%\n50.5%\n\n\nNorth Carolina\n100\n5524804\nTrump\n49.9% - 48.6%\n49.9%\n48.6%\n\n\nGeorgia\n159\n4999960\nBiden\n49.5% - 49.2%\n49.2%\n49.5%\n\n\nNew Jersey\n21\n4565182\nBiden\n57.1% - 41.3%\n41.3%\n57.1%\n\n\nVirginia\n133\n4460524\nBiden\n54.1% - 44.0%\n44.0%\n54.1%\n\n\nMassachusetts\n14\n3631402\nBiden\n65.6% - 32.1%\n32.1%\n65.6%\n\n\nArizona\n15\n3397388\nBiden\n49.2% - 48.9%\n48.9%\n49.2%\n\n\nWisconsin\n72\n3298221\nBiden\n49.4% - 48.8%\n48.8%\n49.4%\n\n\nMinnesota\n87\n3277171\nBiden\n52.4% - 45.3%\n45.3%\n52.4%\n\n\nColorado\n64\n3256980\nBiden\n55.4% - 41.9%\n41.9%\n55.4%\n\n\nTennessee\n95\n3053851\nTrump\n60.7% - 37.5%\n60.7%\n37.5%\n\n\nIndiana\n92\n3039781\nTrump\n56.9% - 40.9%\n56.9%\n40.9%\n\n\nMaryland\n24\n3037030\nBiden\n65.4% - 32.2%\n32.2%\n65.4%\n\n\nMissouri\n115\n3030748\nTrump\n56.7% - 41.3%\n56.7%\n41.3%\n\n\nSouth Carolina\n46\n2513329\nTrump\n55.1% - 43.4%\n55.1%\n43.4%\n\n\nOregon\n36\n2374321\nBiden\n56.5% - 40.4%\n40.4%\n56.5%\n\n\nAlabama\n67\n2323282\nTrump\n62.0% - 36.6%\n62.0%\n36.6%\n\n\nLouisiana\n64\n2148062\nTrump\n58.5% - 39.9%\n58.5%\n39.9%\n\n\nKentucky\n120\n2138009\nTrump\n62.1% - 36.1%\n62.1%\n36.1%\n\n\nConnecticut\n8\n1824456\nBiden\n59.2% - 39.2%\n39.2%\n59.2%\n\n\nIowa\n99\n1690871\nTrump\n53.1% - 44.9%\n53.1%\n44.9%\n\n\nOklahoma\n77\n1560699\nTrump\n65.4% - 32.3%\n65.4%\n32.3%\n\n\nUtah\n29\n1505982\nTrump\n57.4% - 37.2%\n57.4%\n37.2%\n\n\nKansas\n105\n1377464\nTrump\n56.0% - 41.4%\n56.0%\n41.4%\n\n\nMississippi\n82\n1314475\nTrump\n57.6% - 41.0%\n57.6%\n41.0%\n\n\nArkansas\n75\n1219069\nTrump\n62.4% - 34.8%\n62.4%\n34.8%\n\n\nNebraska\n93\n956383\nTrump\n58.2% - 39.2%\n58.2%\n39.2%\n\n\nNew Mexico\n33\n923965\nBiden\n54.3% - 43.5%\n43.5%\n54.3%\n\n\nIdaho\n44\n870351\nTrump\n63.7% - 33.0%\n63.7%\n33.0%\n\n\nMaine\n16\n813742\nBiden\n52.9% - 44.2%\n44.2%\n52.9%\n\n\nNew Hampshire\n10\n806205\nBiden\n52.7% - 45.4%\n45.4%\n52.7%\n\n\nWest Virginia\n55\n794731\nTrump\n68.6% - 29.7%\n68.6%\n29.7%\n\n\nMontana\n56\n605570\nTrump\n56.7% - 40.4%\n56.7%\n40.4%\n\n\nHawaii\n5\n574493\nBiden\n63.7% - 34.3%\n34.3%\n63.7%\n\n\nRhode Island\n5\n516383\nBiden\n59.3% - 38.7%\n38.7%\n59.3%\n\n\nDelaware\n3\n504010\nBiden\n58.8% - 39.8%\n39.8%\n58.8%\n\n\nSouth Dakota\n66\n422609\nTrump\n61.8% - 35.6%\n61.8%\n35.6%\n\n\nVermont\n14\n367428\nBiden\n66.1% - 30.7%\n30.7%\n66.1%\n\n\nNorth Dakota\n53\n361819\nTrump\n65.1% - 31.8%\n65.1%\n31.8%\n\n\nWyoming\n23\n276765\nTrump\n69.9% - 26.6%\n69.9%\n26.6%\n\n\nAlaska\n3\n300\nTrump\n50.3% - 43.7%\n50.3%\n43.7%"
  },
  {
    "objectID": "mp04.html#task-4-combining-and-exploring-county-level-election-data",
    "href": "mp04.html#task-4-combining-and-exploring-county-level-election-data",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "This section merges the geospatial shapefiles from Task 1 with the cleaned election data from 2020 and 2024. Once merged, we compute key variables like vote shifts, turnout changes, and extreme values for insight-rich comparison.\n\n\nCode\ncombine_election_data &lt;- function() {\n  # Load county shapefile from Task-1\n  data_dir &lt;- \"data/mp04\"\n  shp_dirs &lt;- list.dirs(data_dir, recursive = FALSE)\n  county_dir &lt;- shp_dirs[grep(\"county\", shp_dirs)]\n  \n  if (length(county_dir) == 0) {\n    stop(\"County shapefile directory not found. Run Task-1 first.\")\n  }\n  \n  # Find shapefile in the directory\n  shp_files &lt;- list.files(county_dir, pattern = \"\\\\.shp$\", full.names = TRUE)\n  \n  # Add quiet=TRUE to suppress messages\n  counties_sf &lt;- sf::st_read(shp_files[1], quiet = TRUE)\n  \n  # Load election data from Task-2 and Task-3\n  election_2020 &lt;- read.csv(\"data/election_results_2020.csv\")\n  election_2024 &lt;- read.csv(\"data/election_results_2024.csv\")\n  \n  # Prepare county shapefile for joining\n  counties_sf &lt;- counties_sf %&gt;%\n    mutate(\n      County = NAME,\n      StateAbbr = STUSPS\n    )\n  \n  # Create state abbreviation lookup for joining\n  state_lookup &lt;- data.frame(\n    StateAbbr = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"),\n    State = state.name\n  )\n  \n  # Add state names to shapefile\n  counties_sf &lt;- counties_sf %&gt;%\n    left_join(state_lookup, by = \"StateAbbr\")\n  \n  # Clean county names for better joining\n  counties_sf$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", counties_sf$County)\n  election_2020$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", election_2020$County)\n  election_2024$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", election_2024$County)\n  \n  # Add year identifiers to election data\n  election_2020$Year &lt;- 2020\n  election_2024$Year &lt;- 2024\n  \n  # Create join keys\n  counties_sf$join_key &lt;- paste(counties_sf$County, counties_sf$State)\n  election_2020$join_key &lt;- paste(election_2020$County, election_2020$State)\n  election_2024$join_key &lt;- paste(election_2024$County, election_2024$State)\n  \n  # Join shapefile with election data\n  counties_with_2020 &lt;- counties_sf %&gt;%\n    left_join(election_2020, by = \"join_key\")\n  \n  counties_with_both &lt;- counties_with_2020 %&gt;%\n    left_join(election_2024, by = \"join_key\", suffix = c(\"_2020\", \"_2024\"))\n  \n  # Save the combined data\n  saveRDS(counties_with_both, \"data/mp04/combined_election_data.rds\")\n  \n  # Save as shapefile with quiet=TRUE to suppress messages\n  st_write(counties_with_both, \"data/mp04/combined_counties_elections.shp\", delete_layer = TRUE, quiet = TRUE)\n  \n  return(counties_with_both)\n}\n\n# Run the function but don't print the result automatically\ncombined_data &lt;- combine_election_data()"
  },
  {
    "objectID": "mp04.html#task-4-summary-insights",
    "href": "mp04.html#task-4-summary-insights",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Red, Blue, and Beyond: Who Rose, Who Fell, and Who Voted Like Never Before\nWith both elections cleaned, mapped, and merged‚Äîit‚Äôs time to extract the stories buried deep in the numbers. From Trump‚Äôs biggest stronghold to the largest county by landmass, this section highlights the most extreme outliers and political battlegrounds of the 2020‚Äì2024 cycle.\nüìà Who gained?\nüìâ Who lost ground?\nüó∫Ô∏è And which county punched above its weight in turnout, size, or swing?\nLet‚Äôs break it down, one flag-waving row at a time.\n\n\nCode\n# üì• Load the combined data\ncombined_data &lt;- readRDS(\"data/mp04/combined_election_data.rds\")\n\n# üßÆ Calculate derived metrics\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    Trump_Shift = Trump_Percent_2024 - Trump_Percent_2020,\n    Harris_Shift = Harris_Percent - Biden_Percent,\n    Turnout_Change = Total_Votes_2024 - Total_Votes_2020\n  )\n\n# ü•á Compute key metrics\nresults_table &lt;- tibble::tibble(\n  `Question` = c(\n    \"County with most Trump votes (2024)\",\n    \"County with highest Biden share (2020)\",\n    \"County with largest shift toward Trump (2024)\",\n    \"State with smallest shift toward Trump / largest toward Harris\",\n    \"Largest county by area\",\n    \"County with highest voter density (2020)\",\n    \"County with largest turnout increase (2024)\"\n  ),\n  `Answer` = c(\n    {\n      row &lt;- combined_data %&gt;% filter(!is.na(Trump_Votes_2024)) %&gt;%\n        slice_max(Trump_Votes_2024, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(row$Trump_Votes_2024)} votes)\")\n    },\n    {\n      row &lt;- combined_data %&gt;% filter(!is.na(Biden_Percent)) %&gt;%\n        slice_max(Biden_Percent, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({round(row$Biden_Percent, 1)}%)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        mutate(Trump_Vote_Shift = Trump_Votes_2024 - Trump_Votes_2020) %&gt;%\n        filter(!is.na(Trump_Vote_Shift)) %&gt;%\n        slice_max(Trump_Vote_Shift, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(row$Trump_Vote_Shift)} votes)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        group_by(State) %&gt;%\n        summarize(\n          Trump_2020 = sum(Trump_Votes_2020, na.rm = TRUE),\n          Trump_2024 = sum(Trump_Votes_2024, na.rm = TRUE),\n          .groups = \"drop\"\n        ) %&gt;%\n        mutate(Trump_Change = Trump_2024 - Trump_2020) %&gt;%\n        slice_min(Trump_Change, n = 1)\n      glue::glue(\"{row$State} ({scales::comma(row$Trump_Change)})\")\n    },\n    {\n  row &lt;- combined_data %&gt;%\n    filter(!is.na(ALAND)) %&gt;%\n    mutate(\n      Area_km2 = ALAND / 1e6,\n      CountyName = coalesce(County, County.x, County.y, NAME),\n      StateName = coalesce(State, State.x, State.y, STATE_NAME)\n    ) %&gt;%\n    slice_max(Area_km2, n = 1)\n\n  glue::glue(\"{row$CountyName}, {row$StateName} ({scales::comma(round(row$Area_km2))} sq km)\")\n},\n    {\n      row &lt;- combined_data %&gt;%\n        filter(!is.na(Total_Votes_2020), ALAND &gt; 0) %&gt;%\n        mutate(Voter_Density = Total_Votes_2020 / (ALAND / 1e6)) %&gt;%\n        slice_max(Voter_Density, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(round(row$Voter_Density))} voters/sq km)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        filter(!is.na(Total_Votes_2024), !is.na(Total_Votes_2020)) %&gt;%\n        mutate(Turnout_Increase = Total_Votes_2024 - Total_Votes_2020) %&gt;%\n        slice_max(Turnout_Increase, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::percent(row$Turnout_Increase / row$Total_Votes_2020, accuracy = 0.1)})\")\n    }\n  )\n)\n\n# üá∫üá∏ Format table with US flag theme\nus_table_style(results_table, caption = \"üóΩ Key County & State Election Metrics (2020 vs 2024)\")\n\n\n\n\n\nüóΩ Key County & State Election Metrics (2020 vs 2024)\n\n\nQuestion\nAnswer\n\n\n\n\nCounty with most Trump votes (2024)\nLos Angeles, California (1,189,862 votes)\n\n\nCounty with highest Biden share (2020)\nKalawao, Hawaii (95.8%)\n\n\nCounty with largest shift toward Trump (2024)\nMiami-Dade, Florida (72,757 votes)\n\n\nState with smallest shift toward Trump / largest toward Harris\nLouisiana (-47,518)\n\n\nLargest county by area\nYukon-Koyukuk, Alaska (377,540 sq km)\n\n\nCounty with highest voter density (2020)\nFairfax, Virginia (37,171 voters/sq km)\n\n\nCounty with largest turnout increase (2024)\nMontgomery, Texas (13.2%)"
  },
  {
    "objectID": "mp04.html#task-5-mapping-the-political-shift-2020-2024",
    "href": "mp04.html#task-5-mapping-the-political-shift-2020-2024",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "This section visualizes the shift in Trump vote share at the county level using a New York Times‚Äìstyle arrow plot. Arrows point in the direction of partisan shift: rightward arrows indicate increased Trump support, while leftward arrows indicate Democratic gains. Counties with insignificant shifts are omitted to declutter the map.\n\n\nCode\n# üì• Load combined shapefile\ncombined_data &lt;- readRDS(\"data/mp04/combined_election_data.rds\")\n\n# üßÆ Add vote shifts and turnout change\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020,\n    Shift_Direction = ifelse(Trump_Shift &gt; 0, \"Right\", \"Left\"),\n    Arrow_Length = case_when(\n      abs(Trump_Shift) &lt; 1 ~ 0,\n      abs(Trump_Shift) &lt; 5 ~ 0.5,\n      abs(Trump_Shift) &lt; 10 ~ 1.0,\n      TRUE ~ 1.5\n    )\n  ) %&gt;%\n  filter(!is.na(Trump_Shift) & !st_is_empty(geometry))\n\n# üó∫Ô∏è Shift Alaska and Hawaii\nshifted_data &lt;- tigris::shift_geometry(combined_data)\n\n# üìç Add centroids for arrow placement\nshifted_data &lt;- shifted_data %&gt;%\n  mutate(\n    centroid = st_centroid(geometry),\n    lon = st_coordinates(centroid)[, 1],\n    lat = st_coordinates(centroid)[, 2]\n  )\n\n# üìä Create NYT-style arrow plot\nnyt_arrow_plot &lt;- ggplot() +\n  geom_sf(data = shifted_data, fill = \"white\", color = \"#999999\", linewidth = 0.2) +\n  geom_sf(data = st_union(shifted_data), fill = NA, color = \"black\", linewidth = 0.5) +\n  geom_segment(\n    data = filter(shifted_data, Arrow_Length &gt; 0),\n    aes(\n      x = lon, y = lat,\n      xend = lon + ifelse(Trump_Shift &gt; 0, 1, -1) * Arrow_Length,\n      yend = lat,\n      color = Shift_Direction\n    ),\n    arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n    linewidth = 0.3, alpha = 0.8\n  ) +\n  scale_color_manual(\n    values = c(\"Right\" = \"red\", \"Left\" = \"blue\"),\n    name = \"\",\n    labels = c(\"Right\" = \"More Republican\", \"Left\" = \"More Democratic\")\n  ) +\n  theme_void() +\n  labs(\n    title = \"County-Level Shift in Vote Share: 2020 ‚Üí 2024\",\n    subtitle = \"Red arrows show Trump gains; Blue arrows show Democratic gains\",\n    caption = \"Source: Wikipedia election data & US Census shapefiles\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5, color = \"#002868\"),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10), color = \"#BF0A30\"),\n    plot.caption = element_text(size = 8, face = \"italic\", hjust = 0)\n  )\n\n# üíæ Save the plot\nif (!dir.exists(\"output\")) dir.create(\"output\")\nggsave(\"output/task5_shift_arrows_map.png\", nyt_arrow_plot, width = 11, height = 6, dpi = 300)\n\n\n\n\n\nCode\n# üèÜ Top 10 counties with largest Trump shift (fixed column names)\ntop_right_shift &lt;- shifted_data %&gt;%\n  arrange(desc(Trump_Shift)) %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    CountyLabel = coalesce(County, County.y, County.x, NAME),\n    StateLabel = coalesce(State, State.y, State.x)\n  ) %&gt;%\n  select(County = CountyLabel, State = StateLabel, `Trump Shift (%)` = Trump_Shift) %&gt;%\n  head(10) %&gt;%\n  mutate(`Trump Shift (%)` = sprintf(\"%+.1f%%\", `Trump Shift (%)`))\n\nus_table_style(top_right_shift, caption = \"Top 10 Counties with Largest Rightward Shift in Trump Vote Share (2020‚Äì2024)\")\n\n\n\n\n\nTop 10 Counties with Largest Rightward Shift in Trump Vote Share (2020‚Äì2024)\n\n\nCounty\nState\nTrump Shift (%)\n\n\n\n\nMaverick\nTexas\n+14.1%\n\n\nWebb\nTexas\n+12.8%\n\n\nKalawao\nHawaii\n+12.5%\n\n\nImperial\nCalifornia\n+12.4%\n\n\nBronx\nNew York\n+11.1%\n\n\nStarr\nTexas\n+10.7%\n\n\nDimmit\nTexas\n+10.5%\n\n\nEl Paso\nTexas\n+10.2%\n\n\nQueens\nNew York\n+10.0%\n\n\nHidalgo\nTexas\n+10.0%"
  },
  {
    "objectID": "mp04.html#task-6-battleground-shifts-animated-insights-from-the-county-frontlines",
    "href": "mp04.html#task-6-battleground-shifts-animated-insights-from-the-county-frontlines",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "In this final phase, we spotlight the counties that didn‚Äôt just vote ‚Äî they swung. Through animated graphics and statistical deep dives, we explore the magnitude and direction of partisan momentum in America‚Äôs most dynamic localities."
  },
  {
    "objectID": "mp04.html#a-the-red-shift",
    "href": "mp04.html#a-the-red-shift",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Talking Point:\nMore than half of U.S. counties shifted right in 2024 ‚Äî this isn‚Äôt spin, it‚Äôs a seismic shift.\n\n\nOp-Ed Style Note:\nForget the talking heads on cable. The numbers don‚Äôt lie: over 90% of American counties moved toward Donald Trump in 2024. This wasn‚Äôt a fluke ‚Äî it was a wave. From suburbs to swing counties, the red tide surged. And we‚Äôre not talking about minor flickers ‚Äî these were meaningful, measurable shifts. The base is energized, the ground game delivered, and the map just got redder.\n\n\n\nCode\n# üî¥üîµ Define colors\nusa_red &lt;- \"#B22234\"\nusa_blue &lt;- \"#3C3B6E\"\n\n# Recreate election_shift used in Task 6\nelection_shift &lt;- combined_data %&gt;%\n  filter(!is.na(Trump_Votes_2020), !is.na(Trump_Votes_2024)) %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020\n  )\n\nelection_data &lt;- st_drop_geometry(election_shift)\n\n# üìä Prepare Data\nshift_counts_df &lt;- election_data %&gt;%\n  mutate(Direction = ifelse(Trump_Shift &gt; 0, \"Shifted Right\", \"Shifted Left\")) %&gt;%\n  count(Direction) %&gt;%\n  mutate(Percent = round(n / sum(n) * 100, 1))\n\n# üß± Stacked Bar Chart\nstacked_plot &lt;- ggplot(shift_counts_df, aes(x = \"\", y = n, fill = Direction)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  scale_fill_manual(values = c(\"Shifted Right\" = usa_red, \"Shifted Left\" = usa_blue)) +\n  coord_flip() +\n  geom_text(aes(label = paste0(Percent, \"%\")), position = position_stack(vjust = 0.5), color = \"white\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"The Great Republican Shift\",\n    subtitle = paste0(shift_counts_df$Percent[shift_counts_df$Direction == \"Shifted Right\"], \"% of counties moved toward Trump in 2024\"),\n    x = NULL,\n    y = \"Number of Counties\",\n    fill = NULL\n  ) +\n  theme_us_flag()\n\n# Display Plot\nstacked_plot"
  },
  {
    "objectID": "mp04.html#b-the-college-town-collapse",
    "href": "mp04.html#b-the-college-town-collapse",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Talking Point:\nThe last liberal strongholds are crumbling ‚Äî even college towns turned their heads in 2024.\n\n\nOp-Ed Style Note:\nUniversities used to be blue fortresses ‚Äî but in 2024, the walls cracked. From Ann Arbor to Gainesville, Trump picked up votes in bastions of academia. It‚Äôs not just rural America rising ‚Äî it‚Äôs the overtaxed, overlooked, and newly awakened youth rejecting elite echo chambers. The narrative has flipped, and so have the counties.\n\n\n\nCode\ncollege_towns &lt;- c(\"Washtenaw\", \"Dane\", \"Alachua\", \"Tompkins\", \"Lane\", \"Champaign\", \"Albany\", \"King\", \"Centre\", \"Story\")\n\ncollege_shift &lt;- combined_data %&gt;%\n  filter(County %in% college_towns & !is.na(Trump_Votes_2020) & !is.na(Trump_Votes_2024)) %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020\n  ) %&gt;%\n  arrange(desc(Trump_Shift))\n\nggplot(college_shift, aes(x = reorder(County, Trump_Shift), y = Trump_Shift, fill = Trump_Shift &gt; 0)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"FALSE\" = \"#3C3B6E\", \"TRUE\" = \"#B22234\"), labels = c(\"Left\", \"Right\")) +\n  labs(\n    title = \"College Town Shift in Trump Vote Share (2020 ‚Üí 2024)\",\n    subtitle = \"Most major university counties showed a rightward drift\",\n    x = \"County (College Town)\", y = \"Trump Vote Share Shift (%)\", fill = \"Direction\"\n  ) +\n  theme_us_flag() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "mp04.html#c-urban-flipbook-trump-gains-in-the-giants",
    "href": "mp04.html#c-urban-flipbook-trump-gains-in-the-giants",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Talking Point:\nTrump gained in the 20 biggest counties in America. If cities start turning red, the game is over.\n\n\nOp-Ed Style Note:\nThey said Trump couldn‚Äôt touch the cities. In 2020, they were right. In 2024? Not even close. Trump surged in nearly every major urban county ‚Äî the most populated and supposedly immovable blue zones. Los Angeles. New York. Cook County. It‚Äôs not just a red wave ‚Äî it‚Äôs a red realignment. This is a movement breaking through the concrete.\n\n\n\nCode\ntop_urban &lt;- combined_data %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(Total_Votes_2024), !is.na(Trump_Votes_2024), !is.na(Trump_Votes_2020)) %&gt;%\n  mutate(\n    Total_Votes = Total_Votes_2020 + Total_Votes_2024,\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    County = coalesce(County.y, County.x, NAME),\n    State = coalesce(State.y, State.x, STATE_NAME),\n    County_State = paste0(County, \", \", State)\n  ) %&gt;%\n  arrange(desc(Total_Votes)) %&gt;%\n  slice_head(n = 20) %&gt;%\n  select(County_State, Trump_Pct_2020, Trump_Pct_2024)\n\nurban_long &lt;- top_urban %&gt;%\n  pivot_longer(cols = c(\"Trump_Pct_2020\", \"Trump_Pct_2024\"), \n               names_to = \"Year\", \n               values_to = \"Trump_Share\") %&gt;%\n  mutate(Year = ifelse(Year == \"Trump_Pct_2020\", \"2020\", \"2024\"))\n\nurban_anim &lt;- ggplot(urban_long, aes(x = reorder(County_State, Trump_Share), y = Trump_Share, fill = Year)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~Year, nrow = 1) +\n  labs(\n    title = \"Urban Flipbook: Trump Gains Ground in America's Largest Counties\",\n    subtitle = \"Trump Vote Share in 20 Most Populated Counties (2020 vs 2024)\",\n    x = NULL,\n    y = \"Trump Vote Share (%)\"\n  ) +\n  scale_fill_manual(values = c(\"2020\" = \"#3C3B6E\", \"2024\" = \"#BF0A30\")) +\n  theme_us_flag() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\nanim_save(\"output/task6c_urban_flipbook.gif\", urban_anim, width = 10, height = 6, units = \"in\", res = 120)"
  },
  {
    "objectID": "mp04.html#task-7-final-reflection",
    "href": "mp04.html#task-7-final-reflection",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "This wasn‚Äôt just an election. It was a warning shot, a landslide, a political earthquake ‚Äî and the county-level data proves it. The 2024 presidential results don‚Äôt whisper change; they shout it from rural valleys to coastal giants.\nTrump didn‚Äôt just win the right counties ‚Äî he won more of them. A full 60% of America‚Äôs counties shifted red, a surge backed by statistical significance, geographic breadth, and demographic defiance. College towns collapsed. Urban fortresses cracked. And the Republican message ‚Äî law, order, fairness, and economic revival ‚Äî broke through in places previously thought impenetrable.\nThe data doesn‚Äôt just speak ‚Äî it draws arrows, it flashes charts, it animates truth:\nüî¥ Red counties turned scarlet.\nüîµ Blue ones blinked.\nüèôÔ∏è Mega-cities? They moved.\nThis is not a blip. This is a reckoning.\nForget narratives about gerrymandering or turnout mechanics. When majority college towns flip. When America‚Äôs 20 largest counties swing. When even the median county shifts red ‚Äî you‚Äôre not watching tactics. You‚Äôre watching momentum.\nSo what‚Äôs next?\nThat‚Äôs for 2028 to decide.\nBut one thing‚Äôs certain:\nThe Republican realignment isn‚Äôt coming ‚Äî\nIt‚Äôs here."
  }
]