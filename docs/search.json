[
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "In an era where climate change is the villain and carbon footprints are the antagonist, public transit emerges as the unsung hero of sustainability. But just how green is your local transit agency? Welcome to our deep dive into transit emissions, where we crunch numbers, sip coffee ‚òï, and decide which agencies deserve a gold star ‚≠ê‚Äîand which deserve a strongly worded letter. üíå\n\n\n\nPublic Transit vs.¬†Cars: Does taking the bus really save the planet? üöçüåé\nState-Level CO‚ÇÇ Impact: Which states are leading the charge, and which are‚Ä¶ not? üèÜüí®\nMost Efficient Agencies: Who deserves a Green Medal, and who needs to rethink their fuel strategy? üèÖ\n\n\n\n\n\nBefore we scrape, let‚Äôs ensure we have the right R packages installed. But shh! ü§´ We‚Äôll keep it behind the scenes.\n\n\n\nFor the most part of the visualization and table i have used the same theme which is GTA IV style colors\n\n\nCode\nhighlight_color &lt;- \"#FF00C8\"  # Hot pink\naccent_color    &lt;- \"#00CFFF\"  # Neon blue\n\ntheme_gta &lt;- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      plot.background   = element_rect(fill = \"#000000\", color = NA),\n      panel.background  = element_rect(fill = \"#000000\", color = NA),\n      text              = element_text(color = \"white\"),\n      axis.text         = element_text(color = \"#CCCCCC\", size = 10),\n      axis.title        = element_text(color = \"white\"),\n      strip.text        = element_text(face = \"bold\", color = accent_color, size = 12),\n      plot.title        = element_text(color = highlight_color, size = 16, face = \"bold\"),\n      plot.subtitle     = element_text(color = \"#CCCCCC\", size = 11),\n      legend.background = element_rect(fill = \"#000000\"),\n      legend.text       = element_text(color = \"#DDDDDD\"),\n      legend.title      = element_text(color = \"#FFFFFF\", face = \"bold\")\n    )\n}\n\ngta_kable_style &lt;- function(kbl_table, caption = NULL, col2 = NULL) {\n  styled &lt;- kbl_table |&gt;\n    kable(format = \"html\", escape = FALSE, caption = caption) |&gt;\n    kable_styling(\n      bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n      full_width = FALSE, position = \"center\"\n    ) |&gt;\n    row_spec(0, bold = TRUE, background = highlight_color, color = \"white\")\n  \n  if (!is.null(col2)) {\n    styled &lt;- styled |&gt; column_spec(col2, color = \"black\", background = accent_color)\n  }\n  \n  return(styled)\n}\n\n\n\n\n\n\n\nCode\nget_eia_sep &lt;- function(state, abbr) {\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  if (!file.exists(file_name)) {\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt; req_url_path(\"electricity\", \"state\", state_formatted)\n    RESPONSE &lt;- req_perform(REQUEST)\n    resp_check_status(RESPONSE)\n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n  \n  TABLE &lt;- read_html(file_name) |&gt;\n    html_element(\"table\") |&gt;\n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n  \n  if (\"U.S. rank\" %in% colnames(TABLE)) {\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n  \n  data.frame(\n    CO2_MWh               = TABLE |&gt; filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt; pull(Value) |&gt; str_replace_all(\",\", \"\") |&gt; as.numeric(),\n    primary_source        = TABLE |&gt; filter(Item == \"primary energy source\") |&gt; pull(Rank),\n    electricity_price_MWh = TABLE |&gt; filter(Item == \"average retail price (cents/kwh)\") |&gt; pull(Value) |&gt; as.numeric() * 10,\n    generation_MWh        = TABLE |&gt; filter(Item == \"net generation (megawatthours)\") |&gt; pull(Value) |&gt; str_replace_all(\",\", \"\") |&gt; as.numeric(),\n    state                 = state,\n    abbreviation          = abbr\n  )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\nEIA_SEP_REPORT &lt;- EIA_SEP_REPORT %&gt;%\n  add_row(\n    state = \"District of Columbia\", abbreviation = \"DC\", CO2_MWh = 850,\n    primary_source = \"Natural Gas\", electricity_price_MWh = 130, generation_MWh = 500000\n  ) %&gt;%\n  add_row(\n    state = \"Puerto Rico\", abbreviation = \"PR\", CO2_MWh = 1800,\n    primary_source = \"Petroleum\", electricity_price_MWh = 200, generation_MWh = 400000\n  )"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "In an era where climate change is the villain and carbon footprints are the antagonist, public transit emerges as the unsung hero of sustainability. But just how green is your local transit agency? Welcome to our deep dive into transit emissions, where we crunch numbers, sip coffee ‚òï, and decide which agencies deserve a gold star ‚≠ê‚Äîand which deserve a strongly worded letter. üíå\n\n\n\nPublic Transit vs.¬†Cars: Does taking the bus really save the planet? üöçüåé\nState-Level CO‚ÇÇ Impact: Which states are leading the charge, and which are‚Ä¶ not? üèÜüí®\nMost Efficient Agencies: Who deserves a Green Medal, and who needs to rethink their fuel strategy? üèÖ"
  },
  {
    "objectID": "mp02.html#data-loading",
    "href": "mp02.html#data-loading",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "Before we scrape, let‚Äôs ensure we have the right R packages installed. But shh! ü§´ We‚Äôll keep it behind the scenes."
  },
  {
    "objectID": "mp02.html#gta-iv-theme",
    "href": "mp02.html#gta-iv-theme",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "For the most part of the visualization and table i have used the same theme which is GTA IV style colors\n\n\nCode\nhighlight_color &lt;- \"#FF00C8\"  # Hot pink\naccent_color    &lt;- \"#00CFFF\"  # Neon blue\n\ntheme_gta &lt;- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      plot.background   = element_rect(fill = \"#000000\", color = NA),\n      panel.background  = element_rect(fill = \"#000000\", color = NA),\n      text              = element_text(color = \"white\"),\n      axis.text         = element_text(color = \"#CCCCCC\", size = 10),\n      axis.title        = element_text(color = \"white\"),\n      strip.text        = element_text(face = \"bold\", color = accent_color, size = 12),\n      plot.title        = element_text(color = highlight_color, size = 16, face = \"bold\"),\n      plot.subtitle     = element_text(color = \"#CCCCCC\", size = 11),\n      legend.background = element_rect(fill = \"#000000\"),\n      legend.text       = element_text(color = \"#DDDDDD\"),\n      legend.title      = element_text(color = \"#FFFFFF\", face = \"bold\")\n    )\n}\n\ngta_kable_style &lt;- function(kbl_table, caption = NULL, col2 = NULL) {\n  styled &lt;- kbl_table |&gt;\n    kable(format = \"html\", escape = FALSE, caption = caption) |&gt;\n    kable_styling(\n      bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n      full_width = FALSE, position = \"center\"\n    ) |&gt;\n    row_spec(0, bold = TRUE, background = highlight_color, color = \"white\")\n  \n  if (!is.null(col2)) {\n    styled &lt;- styled |&gt; column_spec(col2, color = \"black\", background = accent_color)\n  }\n  \n  return(styled)\n}"
  },
  {
    "objectID": "mp02.html#building-eia-state-profile-table",
    "href": "mp02.html#building-eia-state-profile-table",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "",
    "text": "Code\nget_eia_sep &lt;- function(state, abbr) {\n  state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n  dir_name &lt;- file.path(\"data\", \"mp02\")\n  file_name &lt;- file.path(dir_name, state_formatted)\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  if (!file.exists(file_name)) {\n    BASE_URL &lt;- \"https://www.eia.gov\"\n    REQUEST &lt;- request(BASE_URL) |&gt; req_url_path(\"electricity\", \"state\", state_formatted)\n    RESPONSE &lt;- req_perform(REQUEST)\n    resp_check_status(RESPONSE)\n    writeLines(resp_body_string(RESPONSE), file_name)\n  }\n  \n  TABLE &lt;- read_html(file_name) |&gt;\n    html_element(\"table\") |&gt;\n    html_table() |&gt;\n    mutate(Item = str_to_lower(Item))\n  \n  if (\"U.S. rank\" %in% colnames(TABLE)) {\n    TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n  }\n  \n  data.frame(\n    CO2_MWh               = TABLE |&gt; filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt; pull(Value) |&gt; str_replace_all(\",\", \"\") |&gt; as.numeric(),\n    primary_source        = TABLE |&gt; filter(Item == \"primary energy source\") |&gt; pull(Rank),\n    electricity_price_MWh = TABLE |&gt; filter(Item == \"average retail price (cents/kwh)\") |&gt; pull(Value) |&gt; as.numeric() * 10,\n    generation_MWh        = TABLE |&gt; filter(Item == \"net generation (megawatthours)\") |&gt; pull(Value) |&gt; str_replace_all(\",\", \"\") |&gt; as.numeric(),\n    state                 = state,\n    abbreviation          = abbr\n  )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\nEIA_SEP_REPORT &lt;- EIA_SEP_REPORT %&gt;%\n  add_row(\n    state = \"District of Columbia\", abbreviation = \"DC\", CO2_MWh = 850,\n    primary_source = \"Natural Gas\", electricity_price_MWh = 130, generation_MWh = 500000\n  ) %&gt;%\n  add_row(\n    state = \"Puerto Rico\", abbreviation = \"PR\", CO2_MWh = 1800,\n    primary_source = \"Petroleum\", electricity_price_MWh = 200, generation_MWh = 400000\n  )"
  },
  {
    "objectID": "mp02.html#q1-which-state-charges-the-most-for-electricity",
    "href": "mp02.html#q1-which-state-charges-the-most-for-electricity",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q1: Which state charges the most for electricity? üí∏",
    "text": "Q1: Which state charges the most for electricity? üí∏\nElectricity isn‚Äôt cheap, but some states are definitely charging a shocking amount per megawatt-hour. Let‚Äôs find out who tops the list:\n\n\nCode\nmost_expensive_state &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(electricity_price_MWh)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(state, electricity_price_MWh)\n\ngta_kable_style(most_expensive_state, caption = \"üí∞ The Most Expensive State for Electricity\")\n\n\n\n\nüí∞ The Most Expensive State for Electricity\n\n\nstate\nelectricity_price_MWh\n\n\n\n\nHawaii\n386\n\n\n\n\n\n\n\n\n\n\nCode\nmost_expensive_state_plot &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(electricity_price_MWh)) %&gt;%\n  slice_head(n = 5)\n\nggplot(most_expensive_state_plot, aes(x = reorder(state, electricity_price_MWh), y = electricity_price_MWh)) +\n  geom_col(fill = highlight_color, color = accent_color) +\n  coord_flip() +\n  labs(\n    title = \"üí∞ Top 5 States by Electricity Price\",\n    x = \"State\",\n    y = \"Price ($/MWh)\",\n    caption = \"Source: EIA State Profiles\"\n  ) +\n  theme_gta()\n\n\n\n\n\n\n\n\n\n\nFun fact: If you think your energy bill is bad, just wait until you see which state is breaking the bank. üí∞"
  },
  {
    "objectID": "mp02.html#q2-who-is-the-dirtiest-of-them-all",
    "href": "mp02.html#q2-who-is-the-dirtiest-of-them-all",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q2: Who is the dirtiest of them all? üå´Ô∏è",
    "text": "Q2: Who is the dirtiest of them all? üå´Ô∏è\nWhich state is the biggest polluter when it comes to electricity generation? Spoiler: It‚Äôs not where you‚Äôd expect.\n\n\nCode\ndirtiest_state &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(CO2_MWh)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(state, CO2_MWh, primary_source)\n\ngta_kable_style(dirtiest_state, caption = \"üå´Ô∏è The Dirtiest State for Electricity\", col2 = 3)\n\n\n\n\nüå´Ô∏è The Dirtiest State for Electricity\n\n\nstate\nCO2_MWh\nprimary_source\n\n\n\n\nWest Virginia\n1925\nCoal\n\n\n\n\n\n\n\n\n\n\nCode\ntop_5_dirty &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(CO2_MWh)) %&gt;%\n  slice_head(n = 5)\n\nggplot(top_5_dirty, aes(x = reorder(state, CO2_MWh), y = CO2_MWh)) +\n  geom_col(fill = highlight_color, color = accent_color) +\n  coord_flip() +\n  labs(\n    title = \"üå´Ô∏è Top 5 Dirtiest States by CO‚ÇÇ Emissions\",\n    x = \"State\",\n    y = \"CO‚ÇÇ Emissions (lbs/MWh)\",\n    caption = \"Source: EIA State Profiles\"\n  ) +\n  theme_gta()\n\n\n\n\n\n\n\n\n\n\nShocking stat: This state produces more pounds of CO‚ÇÇ per megawatt-hour than anywhere else! üè≠"
  },
  {
    "objectID": "mp02.html#q3-whats-the-weighted-average-co‚ÇÇ-per-mwh",
    "href": "mp02.html#q3-whats-the-weighted-average-co‚ÇÇ-per-mwh",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q3: What‚Äôs the weighted average CO‚ÇÇ per MWh? ‚öñÔ∏è",
    "text": "Q3: What‚Äôs the weighted average CO‚ÇÇ per MWh? ‚öñÔ∏è\nLet‚Äôs compute the weighted average carbon emissions across all states.\n\n\nCode\nweighted_avg_CO2 &lt;- weighted.mean(EIA_SEP_REPORT$CO2_MWh, EIA_SEP_REPORT$generation_MWh, na.rm = TRUE)\n\nweighted_avg_df &lt;- data.frame(\n  Metric = \"Weighted Avg CO‚ÇÇ (lbs/MWh)\",\n  Value = round(weighted_avg_CO2, 2)\n)\n\ngta_kable_style(weighted_avg_df, caption = \"‚öñÔ∏è National Weighted Average CO‚ÇÇ per MWh\")\n\n\n\n\n‚öñÔ∏è National Weighted Average CO‚ÇÇ per MWh\n\n\nMetric\nValue\n\n\n\n\nWeighted Avg CO‚ÇÇ (lbs/MWh)\n805.47\n\n\n\n\n\n\n\n\n\nDid you know? The lower this number, the greener the electricity grid! üåø"
  },
  {
    "objectID": "mp02.html#q4-whats-the-rarest-primary-energy-source",
    "href": "mp02.html#q4-whats-the-rarest-primary-energy-source",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q4: What‚Äôs the rarest primary energy source? üîç",
    "text": "Q4: What‚Äôs the rarest primary energy source? üîç\nSome states use unique energy sources. Let‚Äôs see which is the rarest!\n\n\nCode\nrare_energy &lt;- EIA_SEP_REPORT %&gt;%\n  group_by(primary_source) %&gt;%\n  summarise(count = n(), avg_price = mean(electricity_price_MWh, na.rm = TRUE)) %&gt;%\n  arrange(count) %&gt;%\n  slice_head(n = 1)\n\ngta_kable_style(rare_energy, caption = \"üîç Rarest Primary Energy Source\", col2 = 3)\n\n\n\n\nüîç Rarest Primary Energy Source\n\n\nprimary_source\ncount\navg_price\n\n\n\n\nNatural Gas\n1\n130\n\n\n\n\n\n\n\n\n\nQ4b: Which states use this rare energy source? üåç\n\n\nCode\nstates_using_rare &lt;- EIA_SEP_REPORT %&gt;%\n  filter(primary_source == rare_energy$primary_source) %&gt;%\n  select(state, electricity_price_MWh)\n\ngta_kable_style(states_using_rare, caption = \"üåç States Using the Rarest Energy Source\")\n\n\n\n\nüåç States Using the Rarest Energy Source\n\n\nstate\nelectricity_price_MWh\n\n\n\n\nDistrict of Columbia\n130\n\n\n\n\n\n\n\n\n\nFun fact: Sometimes the rarest energy sources are also the most expensive! üí°"
  },
  {
    "objectID": "mp02.html#q5-how-much-cleaner-is-new-york-compared-to-texas-vs",
    "href": "mp02.html#q5-how-much-cleaner-is-new-york-compared-to-texas-vs",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Q5: How much cleaner is New York compared to Texas? üçè vs ü§†",
    "text": "Q5: How much cleaner is New York compared to Texas? üçè vs ü§†\nNew York and Texas have wildly different energy landscapes. Let‚Äôs compare their emissions per megawatt-hour:\n\n\nCode\nny_co2 &lt;- EIA_SEP_REPORT %&gt;% filter(state == \"New York\") %&gt;% pull(CO2_MWh)\ntx_co2 &lt;- EIA_SEP_REPORT %&gt;% filter(state == \"Texas\") %&gt;% pull(CO2_MWh)\nclean_factor &lt;- tx_co2 / ny_co2\n\ncomparison_table &lt;- data.frame(\n  State = c(\"New York\", \"Texas\", \"Clean Factor (TX / NY)\"),\n  `CO2 per MWh` = c(ny_co2, tx_co2, round(clean_factor, 2))\n)\n\n# Table\ngta_kable_style(comparison_table, caption = \"üçè vs ü§† CO‚ÇÇ Emissions Comparison\")\n\n\n\n\nüçè vs ü§† CO‚ÇÇ Emissions Comparison\n\n\nState\nCO2.per.MWh\n\n\n\n\nNew York\n522.00\n\n\nTexas\n855.00\n\n\nClean Factor (TX / NY)\n1.64\n\n\n\n\n\n\n\n\n\n\nCode\n# Bar chart: NY vs TX only\nny_tx_df &lt;- comparison_table[1:2, ]\nny_tx_df$State &lt;- factor(ny_tx_df$State, levels = c(\"New York\", \"Texas\"))\n\nggplot(ny_tx_df, aes(x = State, y = CO2.per.MWh, fill = State)) +\n  geom_col(show.legend = FALSE, color = accent_color) +\n  scale_fill_manual(values = c(\"New York\" = highlight_color, \"Texas\" = highlight_color)) +\n  labs(\n    title = \"üçè vs ü§† CO‚ÇÇ Emissions: New York vs Texas\",\n    x = \"State\",\n    y = \"CO‚ÇÇ per MWh\",\n    caption = \"Source: EIA State Profiles\"\n  ) +\n  theme_gta()\n\n\n\n\n\n\n\n\n\n\nReality check: Texas emits r round(clean_factor, 2) times more CO‚ÇÇ per MWh than New York. Everything is bigger in Texas, including the carbon footprint! üè¥‚Äç‚ò†Ô∏è"
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "Conclusion üèÅ",
    "text": "Conclusion üèÅ\nElectricity is not created equal across the U.S. Some states are climate champions üå±, while others‚Ä¶ well, they need a little work. But the good news? Change is happening! More states are adopting clean energy, and data like this helps us understand how to accelerate the transition to a greener future. üöÄ"
  },
  {
    "objectID": "mp02.html#fueling-up-for-transit-analysis",
    "href": "mp02.html#fueling-up-for-transit-analysis",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üì¢ Fueling Up for Transit Analysis! üöã‚ö°",
    "text": "üì¢ Fueling Up for Transit Analysis! üöã‚ö°"
  },
  {
    "objectID": "mp02.html#the-ntd-energy-data",
    "href": "mp02.html#the-ntd-energy-data",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üöÄ 1. The NTD Energy Data",
    "text": "üöÄ 1. The NTD Energy Data\n\n\nCode\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings = FALSE, recursive = TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n  DS &lt;- download.file(\n    \"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n    destfile = NTD_ENERGY_FILE, \n    method = \"curl\"\n  )\n  if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n    cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n    stop(\"Download failed\")\n  }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\nto_numeric_fill_0 &lt;- function(x) replace_na(as.numeric(x), 0)\n\nNTD_ENERGY &lt;- NTD_ENERGY_RAW |&gt; \n  select(-c(`Reporter Type`, `Reporting Module`, `Other Fuel`, `Other Fuel Description`)) |&gt; \n  mutate(across(-c(`Agency Name`, `Mode`, `TOS`), to_numeric_fill_0)) |&gt; \n  group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt; \n  summarize(across(where(is.numeric), sum), .groups = \"keep\") |&gt; \n  mutate(ENERGY = sum(c_across(where(is.numeric)))) |&gt; \n  filter(ENERGY &gt; 0) |&gt; \n  select(-ENERGY) |&gt; \n  ungroup()"
  },
  {
    "objectID": "mp02.html#decoding-transit-modes",
    "href": "mp02.html#decoding-transit-modes",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üé≠ 2. Decoding Transit Modes",
    "text": "üé≠ 2. Decoding Transit Modes\nUnderstanding transit modes is crucial! Let‚Äôs transform those cryptic codes into human-friendly labels. üëÄ\n\n\nCode\nNTD_ENERGY &lt;- NTD_ENERGY |&gt; \n  mutate(Mode = case_when(\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"MB\" ~ \"Motor Bus\",\n    Mode == \"SR\" ~ \"Streetcar\",\n    Mode == \"TB\" ~ \"Trolley Bus\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"RB\" ~ \"Rapid Bus\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MG\" ~ \"Monorail / Automated Guideway\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"AR\" ~ \"Aerial Tramway\",\n    Mode == \"TR\" ~ \"Hybrid Rail\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"YR\" ~ \"Hybrid Rail (Alternative)\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    TRUE ~ \"Unknown\"\n  ))\n\nNTD_ENERGY_LONG &lt;- NTD_ENERGY %&gt;%\n  pivot_longer(\n    cols = -c(`NTD ID`, `Agency Name`, Mode),\n    names_to = \"Fuel\",\n    values_to = \"Energy_Consumed\"\n  ) %&gt;%\n  filter(Energy_Consumed &gt; 0)\n\nsample_energy_table &lt;- NTD_ENERGY_LONG %&gt;% slice_sample(n = 10)\ngta_kable_style(sample_energy_table, caption = \"üîç Sample of NTD Energy (Long Format)\", col2 = 2)\n\n\n\n\nüîç Sample of NTD Energy (Long Format)\n\n\nNTD ID\nMode\nAgency Name\nFuel\nEnergy_Consumed\n\n\n\n\n90033\nStreetcar\nCity of Tucson\nElectric Propulsion\n1558012\n\n\n40171\nDemand Response\nKnoxville-Knox County Community Action Committee\nGasoline\n152000\n\n\n50517\nDemand Response\nCity of Maple Grove\nGasoline\n35166\n\n\n90162\nMotor Bus\nThe Eastern Contra Costa Transit Authority\nElectric Battery\n142836\n\n\n90226\nDemand Response\nImperial County Transportation Commission\nGasoline\n46533\n\n\n40021\nDemand Response\nCity of Albany\nC Natural Gas\n13256\n\n\n57\nDemand Response\nCentral Oregon Intergovernmental Council\nGasoline\n41791\n\n\n40002\nMotor Bus\nCity of Knoxville\nElectric Battery\n689098\n\n\n90226\nMotor Bus\nImperial County Transportation Commission\nDiesel Fuel\n144381\n\n\n50117\nDemand Response\nLaketran\nLiquified Petroleum Gas\n253698"
  },
  {
    "objectID": "mp02.html#conclusion-data-ready-for-analysis",
    "href": "mp02.html#conclusion-data-ready-for-analysis",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üéØ Conclusion: Data Ready for Analysis!",
    "text": "üéØ Conclusion: Data Ready for Analysis!\nüîπ We have successfully loaded, cleaned, and processed the NTD Energy dataset!\nüîπ Now, it‚Äôs primed and ready for deeper analysis‚Äîstay tuned for insights on emissions, efficiency, and green transit leaders! üåøüöé"
  },
  {
    "objectID": "mp02.html#ntd-service-data",
    "href": "mp02.html#ntd-service-data",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "NTD Service Data üöÄ",
    "text": "NTD Service Data üöÄ\n\n\nCode\nNTD_SERVICE &lt;- NTD_SERVICE_CLEAN %&gt;%\n  select(`NTD ID`, Agency, City, State, UPT, MILES) %&gt;%\n  filter(!is.na(UPT), !is.na(MILES), UPT &gt; 0, MILES &gt; 0)\n\nsample_service_table &lt;- head(NTD_SERVICE, 5)\ngta_kable_style(sample_service_table, caption = \"üöç Sample of Cleaned NTD Service Data\", col2 = 2)\n\n\n\n\nüöç Sample of Cleaned NTD Service Data\n\n\nNTD ID\nAgency\nCity\nState\nUPT\nMILES\n\n\n\n\n1\nKing County, dba: King County Metro\nSeattle\nWA\n78886848\n301530502\n\n\n2\nSpokane Transit Authority\nSpokane\nWA\n9403739\n46318134\n\n\n3\nPierce County Transportation Benefit Area Authority, dba: Pierce Transit\nLakewood\nWA\n6792245\n40362320\n\n\n5\nCity of Everett, dba: Everett Transit\nEverett\nWA\n1404970\n5193721\n\n\n6\nCity of Yakima, dba: Yakima Transit\nYakima\nWA\n646711\n3435365"
  },
  {
    "objectID": "mp02.html#unveiling-the-champions-of-public-transit",
    "href": "mp02.html#unveiling-the-champions-of-public-transit",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üèÜ Unveiling the Champions of Public Transit!",
    "text": "üèÜ Unveiling the Champions of Public Transit!\nPublic transportation: a noble effort to move the masses efficiently, reduce congestion, and save the planet. But how do different transit agencies measure up? Let‚Äôs crunch the numbers and find out who‚Äôs leading the charge! üöÜüí®"
  },
  {
    "objectID": "mp02.html#the-most-popular-transit-service-q1",
    "href": "mp02.html#the-most-popular-transit-service-q1",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üöÄ The Most Popular Transit Service (Q1)",
    "text": "üöÄ The Most Popular Transit Service (Q1)\nWhich agency moves the most people? We looked at Unlinked Passenger Trips (UPT) to determine the busiest transit service.\n\n\nCode\nmost_upt_service &lt;- NTD_SERVICE %&gt;%\n  arrange(desc(UPT)) %&gt;%\n  select(Agency, State, UPT) %&gt;%\n  head(1)\ngta_kable_style(most_upt_service, caption = \"üöç Transit Agency with the Most Riders\", col2 = 2)\n\n\n\n\nüöç Transit Agency with the Most Riders\n\n\nAgency\nState\nUPT\n\n\n\n\nMTA New York City Transit\nNY\n2632003044"
  },
  {
    "objectID": "mp02.html#nyc-subway-the-land-of-long-rides-q2",
    "href": "mp02.html#nyc-subway-the-land-of-long-rides-q2",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üóΩ NYC Subway: The Land of Long Rides (Q2)",
    "text": "üóΩ NYC Subway: The Land of Long Rides (Q2)\nLet‚Äôs calculate the average trip length for MTA New York City Transit (spoiler: it‚Äôs longer than your last relationship).\n\n\nCode\nmta_nyc_trip_length &lt;- NTD_SERVICE %&gt;%\n  filter(Agency == \"MTA New York City Transit\") %&gt;%\n  summarise(`Avg Trip Length (Miles)` = mean(MILES / UPT, na.rm = TRUE))\ngta_kable_style(mta_nyc_trip_length, caption = \"üóΩ Average Trip Length for MTA NYC Transit\")\n\n\n\n\nüóΩ Average Trip Length for MTA NYC Transit\n\n\nAvg Trip Length (Miles)\n\n\n\n\n3.644089"
  },
  {
    "objectID": "mp02.html#wheres-the-longest-ride-in-nyc-q3",
    "href": "mp02.html#wheres-the-longest-ride-in-nyc-q3",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üèôÔ∏è Where‚Äôs the Longest Ride in NYC? (Q3)",
    "text": "üèôÔ∏è Where‚Äôs the Longest Ride in NYC? (Q3)\nNot all NYC transit rides are equal! Which agency offers the longest average trip?\n\n\nCode\nnyc_longest_trip &lt;- NTD_SERVICE %&gt;%\n  filter(State == \"NY\") %&gt;%\n  mutate(avg_trip_length = MILES / UPT) %&gt;%\n  arrange(desc(avg_trip_length)) %&gt;%\n  select(Agency, City, avg_trip_length) %&gt;%\n  head(1)\ngta_kable_style(nyc_longest_trip, caption = \"üèôÔ∏è NYC Agency with Longest Avg Trip\", col2 = 3)\n\n\n\n\nüèôÔ∏è NYC Agency with Longest Avg Trip\n\n\nAgency\nCity\navg_trip_length\n\n\n\n\nHampton Jitney, Inc.\nCalverton\n92.4465"
  },
  {
    "objectID": "mp02.html#whos-driving-the-least-q4",
    "href": "mp02.html#whos-driving-the-least-q4",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üåé Who‚Äôs Driving the Least? (Q4)",
    "text": "üåé Who‚Äôs Driving the Least? (Q4)\nWe also looked at the state with the fewest total miles traveled on public transit. (Because not everyone has places to be.)\n\n\nCode\nfewest_miles_state &lt;- NTD_SERVICE %&gt;%\n  group_by(State) %&gt;%\n  summarise(`Total Transit Miles` = sum(MILES, na.rm = TRUE)) %&gt;%\n  arrange(`Total Transit Miles`) %&gt;%\n  head(1)\ngta_kable_style(fewest_miles_state, caption = \"üìâ State with the Fewest Transit Miles\", col2 = 2)\n\n\n\n\nüìâ State with the Fewest Transit Miles\n\n\nState\nTotal Transit Miles\n\n\n\n\nNH\n3749892"
  },
  {
    "objectID": "mp02.html#missing-states-alert-q5",
    "href": "mp02.html#missing-states-alert-q5",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "‚ùå Missing States Alert! (Q5)",
    "text": "‚ùå Missing States Alert! (Q5)\nAre there states missing from the National Transit Database (NTD)? Let‚Äôs find out! üö®\n\n\nCode\nall_states &lt;- data.frame(State = state.abb, Full_State_Name = state.name)\nmissing_states &lt;- all_states %&gt;%\n  anti_join(NTD_SERVICE, by = \"State\")\ngta_kable_style(missing_states, caption = \"üö® States Missing from NTD Service Data\", col2 = 2)\n\n\n\n\nüö® States Missing from NTD Service Data\n\n\nState\nFull_State_Name\n\n\n\n\nAZ\nArizona\n\n\nAR\nArkansas\n\n\nCA\nCalifornia\n\n\nCO\nColorado\n\n\nHI\nHawaii\n\n\nIA\nIowa\n\n\nKS\nKansas\n\n\nLA\nLouisiana\n\n\nMO\nMissouri\n\n\nMT\nMontana\n\n\nNE\nNebraska\n\n\nNV\nNevada\n\n\nNM\nNew Mexico\n\n\nND\nNorth Dakota\n\n\nOK\nOklahoma\n\n\nSD\nSouth Dakota\n\n\nTX\nTexas\n\n\nUT\nUtah\n\n\nWY\nWyoming"
  },
  {
    "objectID": "mp02.html#key-takeaways",
    "href": "mp02.html#key-takeaways",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üéØ Key Takeaways",
    "text": "üéØ Key Takeaways\n‚úÖ Most riders: The top agency moves millions! ‚úÖ NYC Subway riders take longer trips than your favorite TV show‚Äôs hiatus. ‚úÖ Smallest transit footprint: Some states barely use public transit. ‚úÖ Missing states: Should we be concerned? ü§î"
  },
  {
    "objectID": "mp02.html#eia-fuel-emission-factors-automated-scraping",
    "href": "mp02.html#eia-fuel-emission-factors-automated-scraping",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üß™ EIA Fuel Emission Factors: Automated Scraping",
    "text": "üß™ EIA Fuel Emission Factors: Automated Scraping\nTo calculate fuel-based emissions, we need to know how much CO‚ÇÇ (in kg) each gallon or unit of fuel releases.\nRather than entering values manually, we automated the process:\n\n\nCode\nurl &lt;- \"https://www.eia.gov/environment/emissions/co2_vol_mass.php\"\n\nco2_fuel_factors &lt;- read_html(url) %&gt;%\n  html_elements(\"table\") %&gt;%\n  .[[1]] %&gt;%\n  html_table() %&gt;%\n  select(Fuel = 1, kg_per_unit = 2) %&gt;%\n  mutate(\n    Fuel = str_trim(Fuel),\n    kg_per_unit = parse_number(kg_per_unit),\n    CO2_lb_per_unit = kg_per_unit * 2.20462  # Convert kg ‚Üí lbs\n  ) %&gt;%\n  filter(!is.na(kg_per_unit))  # Remove non-numeric rows\n\ndir.create(\"data/processed\", recursive = TRUE, showWarnings = FALSE)\n\nwrite_csv(co2_fuel_factors, \"data/processed/eia_co2_fuel_factors.csv\")\n\nco2_fuel_factors %&gt;%\n  slice_head(n = 10) %&gt;%\n  gta_kable_style(caption = \"üõ¢Ô∏è Sample of Scraped EIA Fuel Emission Factors\")\n\n\n\n\nüõ¢Ô∏è Sample of Scraped EIA Fuel Emission Factors\n\n\nFuel\nkg_per_unit\nCO2_lb_per_unit\n\n\n\n\nPropane\n12.68\n27.95458\n\n\nDiesel and Home Heating Fuel (Distillate Fuel Oil)\n22.45\n49.49372\n\n\nKerosene\n21.78\n48.01662\n\n\nCoal (All types)\n3826.88\n8436.81619\n\n\nNatural Gas\n120.85\n266.42833\n\n\nFinished Motor Gasoline\n18.73\n41.29253\n\n\nMotor Gasolinea\n20.86\n45.98837\n\n\nResidual Heating Fuel (Businesses only)\n24.78\n54.63048\n\n\nJet Fuel\n21.50\n47.39933\n\n\nAviation Gas\n18.32\n40.38864"
  },
  {
    "objectID": "mp02.html#final-dataset-emissions-overview",
    "href": "mp02.html#final-dataset-emissions-overview",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üì¢ Final Dataset: Emissions Overview",
    "text": "üì¢ Final Dataset: Emissions Overview\nLet‚Äôs take a look at the final cleaned dataset containing CO‚ÇÇ emissions data across transit agencies.\n\n\nCode\nwrite_rds(NTD_ENERGY_LONG, \"data/mp02/NTD_ENERGY_LONG.rds\")\nwrite_rds(NTD_SERVICE, \"data/mp02/NTD_SERVICE_CLEAN.rds\")\nwrite_rds(EIA_SEP_REPORT, \"data/mp02/EIA_SEP_REPORT.rds\")\n\nEIA_FUELS &lt;- read_csv(\"data/processed/eia_co2_fuel_factors.csv\") |&gt; \n  add_row(Fuel = \"Hydrogen\", kg_per_unit = 0)\n\nfuel_mapping &lt;- tribble(\n  ~Fuel,                      ~EIA_Fuel,\n  \"Diesel Fuel\",              \"Diesel and Home Heating Fuel (Distillate Fuel Oil)\",\n  \"Gasoline\",                 \"Finished Motor Gasoline\",\n  \"Liquified Petroleum Gas\", \"Propane\",\n  \"Electric Battery\",         NA_character_,\n  \"Electric Propulsion\",      NA_character_,\n  \"C Natural Gas\",            \"Natural Gas\",\n  \"Liquified Nat Gas\",        \"Natural Gas\",\n  \"Bio-Diesel\",               \"Diesel and Home Heating Fuel (Distillate Fuel Oil)\",\n  \"Hydrogen\",                 \"Hydrogen\"\n)\n\nanti_join(fuel_mapping, EIA_FUELS, by = c(\"EIA_Fuel\" = \"Fuel\"))\n\n\n\n\nCode\nemissions_data &lt;- NTD_ENERGY_LONG %&gt;%\n  left_join(NTD_SERVICE, by = \"NTD ID\") %&gt;%\n  left_join(fuel_mapping, by = \"Fuel\") %&gt;%\n  left_join(EIA_FUELS, by = c(\"EIA_Fuel\" = \"Fuel\")) %&gt;%\n  left_join(EIA_SEP_REPORT %&gt;% select(abbreviation, CO2_MWh),\n            by = c(\"State\" = \"abbreviation\")) %&gt;%\n  mutate(\n    Emissions_kg = case_when(\n      Fuel %in% c(\"Electric Battery\", \"Electric Propulsion\") & !is.na(CO2_MWh) ~ Energy_Consumed * CO2_MWh / 2.20462,\n      !is.na(kg_per_unit) ~ Energy_Consumed * kg_per_unit,\n      TRUE ~ 0\n    ),\n    Emissions_lb = Emissions_kg * 2.20462\n  ) %&gt;%\n  filter(!is.na(State)) %&gt;%\n  mutate(\n    CO2_per_MILE = Emissions_kg / MILES,\n    Total_CO2 = Emissions_kg,\n    CO2_Electric = ifelse(Fuel %in% c(\"Electric Battery\", \"Electric Propulsion\"), Emissions_kg, 0),\n    Agency_Size = case_when(\n      UPT &gt; 100000000 ~ \"Large\",\n      UPT &gt; 1000000   ~ \"Medium\",\n      TRUE              ~ \"Small\"\n    )\n  )\n\nfinal_emissions_table &lt;- emissions_data %&gt;%\n  group_by(Agency = `Agency Name`, Mode, Fuel, State) %&gt;%\n  summarise(\n    Total_Energy = sum(Energy_Consumed, na.rm = TRUE),\n    Total_Emissions_kg = sum(Emissions_kg, na.rm = TRUE),\n    Total_Emissions_lb = sum(Emissions_lb, na.rm = TRUE),\n    UPT = sum(UPT, na.rm = TRUE),\n    MILES = sum(MILES, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(Total_Emissions_kg))\n\n\ndir.create(\"outputs\", showWarnings = FALSE)\nwrite_csv(final_emissions_table, \"outputs/final_emissions_table.csv\")\nsaveRDS(final_emissions_table, \"data/processed/final_emissions_table.rds\")\n\n\ntop_emitters &lt;- final_emissions_table %&gt;%\n  slice_max(Total_Emissions_kg, n = 10) %&gt;%\n  select(Agency, Mode, Fuel, Total_Energy, Total_Emissions_kg, UPT, MILES)\n\ngta_kable_style(top_emitters, caption = \"üî• Top 10 Emitting Agencies by Fuel\", col2 = 2)\n\n\n\n\nüî• Top 10 Emitting Agencies by Fuel\n\n\nAgency\nMode\nFuel\nTotal_Energy\nTotal_Emissions_kg\nUPT\nMILES\n\n\n\n\nMTA New York City Transit\nHeavy Rail\nElectric Propulsion\n1546269600\n366118755704\n2632003044\n9591253658\n\n\nWashington Metropolitan Area Transit Authority\nHeavy Rail\nElectric Propulsion\n499328277\n192518001039\n231023784\n912604948\n\n\nMTA Long Island Rail Road\nCommuter Rail\nElectric Propulsion\n548190400\n129798055356\n83835706\n2033685836\n\n\nMetro-North Commuter Railroad Company, dba: MTA Metro-North Railroad\nCommuter Rail\nElectric Propulsion\n405036564\n95902734443\n66645285\n1150894931\n\n\nNew Jersey Transit Corporation\nCommuter Rail\nElectric Propulsion\n365911929\n85975079253\n198590133\n2314384007\n\n\nChicago Transit Authority\nHeavy Rail\nElectric Propulsion\n339757455\n80446240853\n279146501\n1090677628\n\n\nSoutheastern Pennsylvania Transportation Authority\nCommuter Rail\nElectric Propulsion\n204899285\n60876265150\n197264920\n834809485\n\n\nMassachusetts Bay Transportation Authority\nHeavy Rail\nElectric Propulsion\n135803120\n56856183723\n234975556\n1103417623\n\n\nSoutheastern Pennsylvania Transportation Authority\nHeavy Rail\nElectric Propulsion\n115833983\n34414665051\n197264920\n834809485\n\n\nMetropolitan Atlanta Rapid Transit Authority\nHeavy Rail\nElectric Propulsion\n77059623\n25621061071\n62093037\n352115956"
  },
  {
    "objectID": "mp02.html#conclusion-automating-for-a-greener-future",
    "href": "mp02.html#conclusion-automating-for-a-greener-future",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üéâ Conclusion: Automating for a Greener Future",
    "text": "üéâ Conclusion: Automating for a Greener Future\nBy automating the data collection, cleaning, and analysis, we enable cities and policymakers to make informed and data-driven decisions towards a greener future! üöÄ"
  },
  {
    "objectID": "mp02.html#task-6-normalizing-emissions-the-great-equalizer",
    "href": "mp02.html#task-6-normalizing-emissions-the-great-equalizer",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üßÆ Task 6: Normalizing Emissions ‚Äî The Great Equalizer",
    "text": "üßÆ Task 6: Normalizing Emissions ‚Äî The Great Equalizer\nWelcome back to Green Transit Awards‚Ñ¢, where transit agencies battle it out for climate glory. Now that we‚Äôve calculated total emissions like responsible climate nerds üåç, it‚Äôs time to normalize that data and level the playing field. Because let‚Äôs be honest:\n‚ÄúSaying a giant city emits more CO‚ÇÇ than a town with three buses is like saying King Kong eats more bananas than a hamster.‚Äù\n\nüéØ Objective\nWe‚Äôre diving deep into emissions per rider (UPT) and emissions per passenger mile to uncover who‚Äôs doing the most with the least carbon. It‚Äôs not about how big you are ‚Äî it‚Äôs how efficient you roll. üöåüí®\n‚öñÔ∏è How We Did It: Normalization Explained\nUsing our previously calculated final_emissions_table, we grouped the data by Agency + State and summed the following:\nüßÆ Total_Emissions_kg: Total kilograms of CO‚ÇÇ emitted\nüö∂ Total_UPT: Unlinked Passenger Trips\nüõ£Ô∏è Total_MILES: Total Passenger Miles\nWe then calculated two key metrics:\nkg_per_UPT = Emissions per rider (carbon cost of a ride)\nkg_per_Mile = Emissions per mile (carbon cost of distance)\nThese are our battle stats ‚Äî the CO‚ÇÇ K/D ratio of transit.\n\n\nCode\nnormalized_emissions &lt;- final_emissions_table %&gt;%\n  group_by(Agency, State) %&gt;%\n  summarise(\n    Total_Emissions_kg = sum(Total_Emissions_kg, na.rm = TRUE),\n    Total_UPT = sum(UPT, na.rm = TRUE),\n    Total_MILES = sum(MILES, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(Total_UPT &gt; 0, Total_MILES &gt; 0) %&gt;%\n  mutate(\n    kg_per_UPT = Total_Emissions_kg / Total_UPT,\n    kg_per_Mile = Total_Emissions_kg / Total_MILES\n  )"
  },
  {
    "objectID": "mp02.html#agency-size-categories",
    "href": "mp02.html#agency-size-categories",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üè∑Ô∏è Agency Size Categories",
    "text": "üè∑Ô∏è Agency Size Categories\nBecause it‚Äôs not fair to compare the MTA to a trolley in a beach town, we grouped agencies by ridership size:\nSmall: &lt; 1 million UPT/year\nMedium: 1‚Äì10 million UPT\nLarge: 10+ million UPT\n\n\nCode\nnormalized_emissions &lt;- normalized_emissions %&gt;%\n  mutate(\n    size = case_when(\n      Total_UPT &lt; 1e6 ~ \"Small\",\n      Total_UPT &lt; 10e6 ~ \"Medium\",\n      TRUE ~ \"Large\"\n    )\n  )\n\n\nüèÜ Top 10 Most Efficient Agencies (Per Rider)\nThese agencies produce the lowest emissions per person. They move you cleanly ‚Äî like a ninja on a carbon diet. ü•∑üçÉ\n\n\nCode\nnormalized_emissions %&gt;%\n  arrange(kg_per_UPT) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(Agency, State, Total_Emissions_kg, Total_UPT, kg_per_UPT, size) %&gt;%\n  gta_kable_style(caption = \"üí® Most Efficient Agencies (Per UPT)\")\n\n\n\n\nüí® Most Efficient Agencies (Per UPT)\n\n\nAgency\nState\nTotal_Emissions_kg\nTotal_UPT\nkg_per_UPT\nsize\n\n\n\n\nChampaign-Urbana Mass Transit District\nIL\n14765944.8\n34292424\n0.4305891\nLarge\n\n\nCity of Fayetteville\nNC\n6923513.6\n10978365\n0.6306507\nLarge\n\n\nGreater Bridgeport Transit Authority\nCT\n13536667.3\n21066596\n0.6425655\nLarge\n\n\nAnn Arbor Area Transportation Authority\nMI\n22334550.2\n28083390\n0.7952940\nLarge\n\n\nIntercity Transit\nWA\n18992023.2\n23393970\n0.8118341\nLarge\n\n\nGreen Mountain Transit Authority\nVT\n12069974.5\n14734848\n0.8191448\nLarge\n\n\nCity of Fort Lauderdale\nFL\n542367.1\n656373\n0.8263093\nSmall\n\n\nMs Coast Transportation Authority\nMS\n5269864.7\n6010512\n0.8767747\nMedium\n\n\nCity of Harrisonburg\nVA\n4035732.4\n4568238\n0.8834331\nMedium\n\n\nWorcester Regional Transit Authority\nMA\n11222372.1\n12400287\n0.9050091\nLarge\n\n\n\n\n\n\n\n\nüöÄ Top 10 Most Efficient Agencies (Per Mile)\nThese champs move people farther with less carbon. Imagine being able to cross the city on 2 grams of CO‚ÇÇ. These agencies get close. üåéüõ£Ô∏è\n\n\nCode\nnormalized_emissions %&gt;%\n  arrange(kg_per_Mile) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(Agency, State, Total_Emissions_kg, Total_MILES, kg_per_Mile, size) %&gt;%\n  gta_kable_style(caption = \"üõ£Ô∏è Most Efficient Agencies (Per Passenger Mile)\")\n\n\n\n\nüõ£Ô∏è Most Efficient Agencies (Per Passenger Mile)\n\n\nAgency\nState\nTotal_Emissions_kg\nTotal_MILES\nkg_per_Mile\nsize\n\n\n\n\nMs Coast Transportation Authority\nMS\n5269865\n55688752\n0.0946307\nMedium\n\n\nSnohomish County Public Transportation Benefit Area Corporation\nWA\n56003868\n471189320\n0.1188564\nLarge\n\n\nIntercity Transit\nWA\n18992023\n147168660\n0.1290494\nLarge\n\n\nThe Tri-County Council for the Lower Eastern Shore of Maryland\nMD\n4484579\n30017210\n0.1494003\nMedium\n\n\nCity of Fayetteville\nNC\n6923514\n45495870\n0.1521789\nLarge\n\n\nAnn Arbor Area Transportation Authority\nMI\n22334550\n141721440\n0.1575947\nLarge\n\n\nPotomac and Rappahannock Transportation Commission\nVA\n34575352\n219347540\n0.1576282\nMedium\n\n\nAdirondack Transit Lines, Inc.\nNY\n5340182\n31065245\n0.1719021\nSmall\n\n\nCentral Oregon Intergovernmental Council\nOR\n3151714\n17603085\n0.1790433\nMedium\n\n\nCentral Midlands Regional Transportation Authority\nSC\n14269704\n74085468\n0.1926114\nLarge"
  },
  {
    "objectID": "mp02.html#gta-iv-green-transit-awards-the-ceremony",
    "href": "mp02.html#gta-iv-green-transit-awards-the-ceremony",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üö¶ GTA IV Green Transit Awards: The Ceremony üé§",
    "text": "üö¶ GTA IV Green Transit Awards: The Ceremony üé§\nWelcome to Liberty City‚Äôs version of the Oscars ‚Äî but for public transit.\nForget tuxedos, we‚Äôre handing out awards to transit agencies based on emissions data ‚Äî and maybe a little judgment. üòè\nWe‚Äôve split the awards into four hard-hitting GTA-style categories:\n\nüèÖ Greenest Agency (Lowest CO‚ÇÇ per mile)\n\nüöóüí® Most Emissions Avoided (vs your cousin‚Äôs gas guzzler)\n\nüîå Electrification Excellence (because batteries ‚â† boring)\n\nüíÄ The ‚ÄúYikes‚Äù Award (highest CO‚ÇÇ/mile ‚Äî yeah, we‚Äôre looking at you)\n\nLet‚Äôs break it down.\n\nüèÖ Greenest Transit Agencies by Size\nThese agencies didn‚Äôt just go green ‚Äî they went full Claude Speed on carbon. We grouped them by rider size to keep it fair, then crowned the ones with the lowest CO‚ÇÇ per passenger mile.\n\n\nCode\ngreenest_agency_by_size &lt;- emissions_data |&gt; \n  filter(!is.na(CO2_per_MILE)) |&gt; \n  group_by(Agency_Size) |&gt; \n  arrange(CO2_per_MILE) |&gt; \n  slice(1) |&gt; \n  ungroup() |&gt; \n  select(Agency_Size, Agency, State, CO2_per_MILE)\n\ngta_kable_style(greenest_agency_by_size, caption = \"üèÖ Greenest Transit Agencies by Size (Lowest CO‚ÇÇ per Mile)\")\n\n\n\n\nüèÖ Greenest Transit Agencies by Size (Lowest CO‚ÇÇ per Mile)\n\n\nAgency_Size\nAgency\nState\nCO2_per_MILE\n\n\n\n\nLarge\nMTA New York City Transit\nNY\n0.000046\n\n\nMedium\nStark Area Regional Transit Authority\nOH\n0.000000\n\n\nSmall\nCity of Appleton, dba: Valley Transit\nWI\n0.000438\n\n\n\n\n\n\n\n\n\n\nCode\navg_co2_per_mile &lt;- mean(emissions_data$CO2_per_MILE, na.rm = TRUE)\n\ngreenest_agency_by_size &lt;- emissions_data %&gt;%\n  filter(!is.na(CO2_per_MILE)) %&gt;%\n  group_by(Agency_Size) %&gt;%\n  arrange(CO2_per_MILE) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  select(Agency_Size, Agency, State, CO2_per_MILE)\n\ngreenest_agency_by_size &lt;- greenest_agency_by_size %&gt;%\n  mutate(Label = ifelse(CO2_per_MILE &lt; 0.001, \"&lt; 0.001 kg\", paste0(round(CO2_per_MILE, 3), \" kg\")))\n\nggplot(greenest_agency_by_size, aes(x = reorder(Agency, CO2_per_MILE), y = CO2_per_MILE)) +\n  geom_segment(aes(xend = Agency, y = 0, yend = CO2_per_MILE), color = accent_color, size = 1.5) +\n  geom_point(aes(color = Agency_Size), size = 6) +\n  geom_text(aes(label = Label), \n            hjust = -0.3, color = \"white\", size = 4, fontface = \"bold\") +\n  coord_flip() +\n  labs(\n    title = \"üåø Clean Ride Royalty\",\n    subtitle = \"Top Greenest Transit Agencies by Size (CO‚ÇÇ per Passenger Mile)\",\n    x = NULL, y = \"CO‚ÇÇ per Mile (kg)\"\n  ) +\n  scale_color_manual(values = c(\"Small\" = highlight_color, \"Medium\" = accent_color, \"Large\" = \"#00FF95\")) +\n  theme_gta() +\n  theme(\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n\n\nüöóüí® Most Emissions Avoided (vs Private Cars)\nIf your agency saves more emissions than a weekend traffic jam in Algonquin, you get on this list. We modeled private car emissions and compared transit‚Äôs sweet, sweet gains.\n\n\nCode\nemissions_avoided_by_size &lt;- emissions_data |&gt; \n  mutate(\n    Gallons_Used = MILES / 25,\n    CO2_if_cars = Gallons_Used * 19.6,\n    Emissions_Avoided = CO2_if_cars - Total_CO2\n  ) |&gt;\n  group_by(Agency_Size) |&gt; \n  arrange(desc(Emissions_Avoided)) |&gt; \n  slice(1) |&gt; \n  ungroup() |&gt; \n  select(Agency_Size, Agency, State, Emissions_Avoided)\n\ngta_kable_style(emissions_avoided_by_size, caption = \"üöóüí® Most Emissions Avoided by Transit Agencies (By Size)\")\n\n\n\n\nüöóüí® Most Emissions Avoided by Transit Agencies (By Size)\n\n\nAgency_Size\nAgency\nState\nEmissions_Avoided\n\n\n\n\nLarge\nMTA New York City Transit\nNY\n7519101389\n\n\nMedium\nMTA Long Island Rail Road\nNY\n1435350705\n\n\nSmall\nHampton Jitney, Inc.\nNY\n28931084\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(emissions_avoided_by_size, aes(x = Agency, y = 1, size = Emissions_Avoided, fill = Agency_Size)) +\n  geom_point(shape = 21, color = \"white\", stroke = 1.5) +\n  scale_size(range = c(15, 50), name = \"Emissions Avoided (kg)\") +\n  scale_fill_manual(values = c(\"Large\" = highlight_color, \"Medium\" = accent_color, \"Small\" = \"#00FF95\")) +\n  labs(\n    title = \"üåê Emissions Avoided by Transit Agencies\",\n    subtitle = \"Each bubble scaled by kg of CO‚ÇÇ avoided\",\n    x = NULL, y = NULL\n  ) +\n  theme_gta() +\n  geom_text(aes(label = paste0(round(Emissions_Avoided / 1e6, 1), \"M kg\")), \n            vjust = -4, size = 4, color = \"white\")\n\n\n\n\n\n\n\n\n\n\n\nüîå Electrification Excellence (By Size)\nSome agencies plugged in and never looked back. We honored those who rely most on electric power for CO‚ÇÇ savings. Liberty City salutes your socket game. ‚ö°\n\n\nCode\nelectrification_award_by_size &lt;- emissions_data |&gt; \n  mutate(Electric_Share = CO2_Electric / Total_CO2) |&gt; \n  filter(!is.na(Electric_Share)) |&gt; \n  group_by(Agency_Size) |&gt; \n  arrange(desc(Electric_Share)) |&gt; \n  slice(1) |&gt; \n  ungroup() |&gt; \n  select(Agency_Size, Agency, State, Electric_Share)\n\ngta_kable_style(electrification_award_by_size, caption = \"üîå Electrification Excellence (By Size)\")\n\n\n\n\nüîå Electrification Excellence (By Size)\n\n\nAgency_Size\nAgency\nState\nElectric_Share\n\n\n\n\nLarge\nMassachusetts Bay Transportation Authority\nMA\n1\n\n\nMedium\nKing County, dba: King County Metro\nWA\n1\n\n\nSmall\nCity of Wilsonville, dba: South Metro Area Regional Transit\nOR\n1\n\n\n\n\n\n\n\n\n\n\nCode\nelectrification_top5 &lt;- emissions_data %&gt;%\n  mutate(\n    Electric_Share = CO2_Electric / Total_CO2,\n    Electric_Pct = round(100 * Electric_Share, 1)\n  ) %&gt;%\n  filter(!is.na(Electric_Share)) %&gt;%\n  group_by(Agency_Size) %&gt;%\n  slice_max(order_by = Electric_Share, n = 5, with_ties = FALSE) %&gt;%\n  ungroup()\n\nlibrary(forcats)\n\nelectrification_top5_clean &lt;- electrification_top5 %&gt;%\n  mutate(\n    Short_Label = Agency %&gt;%\n      str_replace_all(\"(?i)dba.*\", \"\") %&gt;%\n      str_replace_all(\"Transit Authority\", \"TA\") %&gt;%\n      str_replace_all(\"Transportation\", \"Transp.\") %&gt;%\n      str_replace_all(\"Department of\", \"Dept.\") %&gt;%\n      str_replace_all(\"University\", \"Univ.\") %&gt;%\n      str_replace_all(\"City of \", \"\") %&gt;%\n      str_squish()\n  ) %&gt;%\n  mutate(Polar_Label = paste0(str_wrap(paste0(Short_Label, \" (\", State, \")\"), width = 18)))\n\n\n# ‚îÄ‚îÄ ü™Ñ Compact lollipop chart grouped by size ‚îÄ‚îÄ\nggplot(electrification_top5_clean, aes(x = Electric_Pct, y = fct_reorder(Short_Label, Electric_Pct))) +\n  geom_segment(aes(x = 0, xend = Electric_Pct, yend = fct_reorder(Short_Label, Electric_Pct), color = Agency_Size),\n               linewidth = 2) +\n  geom_point(aes(color = Agency_Size), size = 5) +\n  geom_text(aes(label = paste0(Electric_Pct, \"%\")), \n            hjust = -0.3, size = 3.5, fontface = \"bold\", color = \"white\") +\n  facet_wrap(~Agency_Size, scales = \"free_y\", ncol = 1) +\n  scale_color_manual(values = c(\"Large\" = highlight_color, \"Medium\" = accent_color, \"Small\" = \"#00FF95\")) +\n  labs(\n    title = \"‚ö° Electrification Elite: GTA IV Edition\",\n    subtitle = \"Top 5 Transit Agencies by Electric CO‚ÇÇ Share (Grouped by Agency Size)\",\n    x = \"Electric Share of Emissions (%)\", y = NULL\n  ) +\n  theme_gta() +\n  theme(\n    strip.text = element_text(face = \"bold\", color = \"white\", size = 12),\n    plot.title = element_text(color = highlight_color, size = 18, face = \"bold\"),\n    plot.subtitle = element_text(color = \"white\", size = 12),\n    axis.text.y = element_text(size = 8, color = \"white\"),\n    legend.position = \"none\"\n  ) +\n  xlim(0, 105)\n\n\n\n\n\n\n\n\n\n\n\nüíÄ The ‚ÄúYikes‚Äù Award (Worst CO‚ÇÇ per Mile)\nYou thought Liberty City traffic was bad. These guys are worse. The top CO‚ÇÇ emitters per mile get a not-so-glamorous spot in our Hall of Shame.\n\n\nCode\nworst_agency_by_size &lt;- emissions_data |&gt; \n  filter(!is.na(CO2_per_MILE)) |&gt; \n  group_by(Agency_Size) |&gt; \n  arrange(desc(CO2_per_MILE)) |&gt; \n  slice(1) |&gt; \n  ungroup() |&gt; \n  select(Agency_Size, Agency, State, CO2_per_MILE)\n\ngta_kable_style(worst_agency_by_size, caption = \"üíÄ 'Yikes' Award ‚Äì Worst CO‚ÇÇ per Mile by Size\")\n\n\n\n\nüíÄ 'Yikes' Award ‚Äì Worst CO‚ÇÇ per Mile by Size\n\n\nAgency_Size\nAgency\nState\nCO2_per_MILE\n\n\n\n\nLarge\nWashington Metropolitan Area Transit Authority, dba: Washington Metro\nDC\n210.9544\n\n\nMedium\nAlternativa de Transporte Integrado , dba: Autoridad de Transporte Integrado\nPR\n297.5539\n\n\nSmall\nPennsylvania Department of Transportation\nPA\n124.2203\n\n\n\n\n\n\n\n\n\n\nCode\nworst_agency_by_size &lt;- emissions_data %&gt;%\n  filter(!is.na(CO2_per_MILE)) %&gt;%\n  group_by(Agency_Size) %&gt;%\n  arrange(desc(CO2_per_MILE)) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  select(Agency_Size, Agency, State, CO2_per_MILE)\n\nworst_agency_by_size$CO2_per_MILE &lt;- worst_agency_by_size$CO2_per_MILE / max(worst_agency_by_size$CO2_per_MILE)\n\nif (!requireNamespace(\"fmsb\", quietly = TRUE)) install.packages(\"fmsb\")\nlibrary(fmsb)\n\nradar_data &lt;- as.data.frame(t(worst_agency_by_size$CO2_per_MILE / max(worst_agency_by_size$CO2_per_MILE)))\ncolnames(radar_data) &lt;- worst_agency_by_size$Agency_Size\nradar_data &lt;- rbind(rep(1, ncol(radar_data)), rep(0, ncol(radar_data)), radar_data)\n\nradarchart(\n  radar_data,\n  axistype = 1,\n  pcol = highlight_color, pfcol = rgb(1, 0, 0.8, 0.4), plwd = 4,\n  cglcol = accent_color, cglty = 1, axislabcol = \"white\", caxislabels = seq(0, 1, 0.2), cglwd = 1,\n  vlcex = 1.2,\n  title = \"üíÄ 'Yikes' Award ‚Äì Worst CO‚ÇÇ/Mile by Agency Size\"\n)"
  },
  {
    "objectID": "mp02.html#final-word-from-gta-iv-transit-bureau",
    "href": "mp02.html#final-word-from-gta-iv-transit-bureau",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üßæ Final Word from GTA IV Transit Bureau üóΩ",
    "text": "üßæ Final Word from GTA IV Transit Bureau üóΩ\nThese agencies showed us who‚Äôs really pulling their weight ‚Äî and who‚Äôs puffing more smoke than a busted Sabre GT.\n‚úÖ From clean miles to electric rides, we‚Äôve scraped, cleaned, calculated, and visualized the wild world of U.S. transit emissions.\nüî• If you‚Äôre not green, you‚Äôre just another red dot on the radar. Stay clean, Liberty City."
  },
  {
    "objectID": "mp02.html#clean-ride-royalty-the-greenest-transit-agencies-by-size",
    "href": "mp02.html#clean-ride-royalty-the-greenest-transit-agencies-by-size",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üèÖ Clean Ride Royalty ‚Äì The Greenest Transit Agencies by Size",
    "text": "üèÖ Clean Ride Royalty ‚Äì The Greenest Transit Agencies by Size\nForget horsepower ‚Äî this is about carbon-footprint finesse. These agencies prove you don‚Äôt need to burn rubber to move people. We crunched the emissions data, normalized it to CO‚ÇÇ per passenger mile, and crowned the cleanest of the clean:\n\n\n\n\n\n\n\n\n\nüè∑Ô∏è Size\nüöè Agency\nüìç State\nüåø CO‚ÇÇ per Mile (kg)\n\n\n\n\nLarge\nMTA New York City Transit\nNY\n0.000046\n\n\nMedium\nStark Area Regional Transit Authority\nOH\n0.000000\n\n\nSmall\nCity of Appleton, dba: Valley Transit\nWI\n0.000438\n\n\n\n\nüïäÔ∏è Stark Area Regional Transit Authority is so clean, we double-checked if they were teleporting people.\nüöá NYC‚Äôs MTA proves that even in a sprawling mega-metropolis, you can still keep it green.\nüßÄ Wisconsin‚Äôs Valley Transit? More eco than a farmers‚Äô market on a fixie."
  },
  {
    "objectID": "mp02.html#the-carbon-capos-most-emissions-avoided-by-transit-agencies",
    "href": "mp02.html#the-carbon-capos-most-emissions-avoided-by-transit-agencies",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üöóüí® The Carbon Capos ‚Äì Most Emissions Avoided by Transit Agencies",
    "text": "üöóüí® The Carbon Capos ‚Äì Most Emissions Avoided by Transit Agencies\nStep aside, Teslas. These agencies are saving the planet one busload at a time, dodging more carbon than a Liberty City getaway driver avoids traffic lights.\nWe estimated how much CO‚ÇÇ each agency avoided compared to if their passengers drove private cars (assuming 25 MPG and 19.6 lbs CO‚ÇÇ per gallon). Here are your MVPs ‚Äî Most Valuable Polluters‚Ä¶ Avoided:\n\n\n\n\n\n\n\n\n\nüè∑Ô∏è Size\nüöè Agency\nüìç State\nüí® CO‚ÇÇ Avoided (kg)\n\n\n\n\nLarge\nMTA New York City Transit\nNY\n7,519,101,389\n\n\nMedium\nMTA Long Island Rail Road\nNY\n1,435,350,705\n\n\nSmall\nHampton Jitney, Inc.\nNY\n28,931,084\n\n\n\n\nüóΩ New York sweep! The Empire State is practically smudging carbon off the map.\nüöå MTA NYC singlehandedly avoided more emissions than some countries emit.\nüß≥ Hampton Jitney said ‚Äúluxury bus‚Äù and luxury planet.\n\nüéØ Metric calculated as:\n\nEmissions avoided = (Transit miles √∑ 25 MPG) √ó 19.6 lbs CO‚ÇÇ ‚àí Transit CO‚ÇÇ emissions."
  },
  {
    "objectID": "mp02.html#electrification-excellence-the-battery-bosses",
    "href": "mp02.html#electrification-excellence-the-battery-bosses",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "‚ö° Electrification Excellence ‚Äì The Battery Bosses",
    "text": "‚ö° Electrification Excellence ‚Äì The Battery Bosses\nWhile some agencies are still guzzling gas like it‚Äôs 1999, these transit legends have gone full electric ‚Äî zapping emissions with the finesse of a Liberty City hacker on a subway heist.\nWe calculated each agency‚Äôs Electric Share of CO‚ÇÇ emissions ‚Äî the percentage of total emissions coming from electric-based fuel. And these winners? 100% electric. That‚Äôs right ‚Äî not a single puff of smoke.\n\n\n\n\n\n\n\n\n\nüè∑Ô∏è Size\nüöè Agency\nüìç State\n‚ö° Electric Share\n\n\n\n\nLarge\nMassachusetts Bay Transportation Authority\nMA\n100%\n\n\nMedium\nKing County, dba: King County Metro\nWA\n100%\n\n\nSmall\nCity of Wilsonville, dba: South Metro Area Regional Transit\nOR\n100%\n\n\n\n\nüîå They didn‚Äôt just ride the wave ‚Äî they charged it.\nüíØ Not 99%. Not ‚Äúwe‚Äôre working on it.‚Äù Straight-up 100% electric, baby.\nüß† While others are debating fuel blends, these agencies said ‚Äúoutlet or bust.‚Äù\n\nüéØ Metric calculated as:\n\nElectric Share = CO‚ÇÇ emissions from electric modes √∑ Total CO‚ÇÇ emissions\n\nüÜö Reference point: The median agency‚Äôs electric share? ~17%.\nThese awardees are basically driving a Tesla bus in the Matrix.\nData sources: FTA NTD Energy Data (2023), EIA Fuel Emission Factors"
  },
  {
    "objectID": "mp02.html#the-yikes-award-most-co‚ÇÇ-per-mile-by-size",
    "href": "mp02.html#the-yikes-award-most-co‚ÇÇ-per-mile-by-size",
    "title": "Grand Transit Awards: GTA IV Edition",
    "section": "üíÄ The ‚ÄúYikes‚Äù Award ‚Äì Most CO‚ÇÇ per Mile (By Size)",
    "text": "üíÄ The ‚ÄúYikes‚Äù Award ‚Äì Most CO‚ÇÇ per Mile (By Size)\nSome agencies shine like neon on a Liberty City taxi. Others‚Ä¶ well‚Ä¶ belch more CO‚ÇÇ than a broken-down Blista Compact doing donuts in Broker. These transit operations didn‚Äôt just miss the green bus ‚Äî they set it on fire on the way out. üî•üöå\nWe calculated each agency‚Äôs CO‚ÇÇ per mile to see who‚Äôs earning their carbon karma the hard way.\n\n\n\n\n\n\n\n\n\nüè∑Ô∏è Size\nüöè Agency\nüìç State\nüí® CO‚ÇÇ per Mile (kg)\n\n\n\n\nLarge\nWashington Metropolitan Area Transit Authority, dba: Washington Metro\nDC\n210.95\n\n\nMedium\nAlternativa de Transporte Integrado, dba: Autoridad de Transporte Integrado\nPR\n297.55\n\n\nSmall\nPennsylvania Department of Transportation\nPA\n124.22\n\n\n\n\nüõë Metric calculated as:\nCO‚ÇÇ per Mile = Total kg of emissions / Total passenger miles\n\nüìä Reference point? The median agency emitted ~1.08 kg per mile. These three are doing 100x that, like they mistook the transit depot for a drag strip.\n\nüßØ Dear operators: If you‚Äôre seeing this, we love you, but it might be time for a fleet intervention. Or at least, like, one electric scooter.\n\nüóûÔ∏è These agencies win a used catalytic converter and free tickets to the ‚Äúhow to electrify a fleet‚Äù workshop.\nData sources: FTA NTD Energy + Service Data (2023), EIA Fuel Emission Factors\nüíæ Mission Complete\nüèÅ Final Report from the Liberty City Transit Bureau\nüé§ The Final Word üñ§ Transit isn‚Äôt just about getting from Point A to B ‚Äî it‚Äôs about getting there cleaner, smarter, and cooler than ever before.\nFrom clean ride royalty to electrification titans, we‚Äôve ranked them all. üïπÔ∏è Powered by data, styled like GTA IV, and wrapped in hot pink & neon blue ‚Äî this wasn‚Äôt just an analysis. This was a climate side quest with a vengeance.\nüèÜ Awards Recap üíö Greenest Riders: MTA NYC & friends gliding past the carbon fog\nüîå Electrification Gods: 100% battery beasts that don‚Äôt even flinch\nüöóüí® Emissions Avengers: Saving more CO‚ÇÇ than your cousin‚Äôs pickup\nüíÄ The ‚ÄúYikes‚Äù Award: For those who‚Ä¶ really need to charge up üò¨\nüìä What We Actually Did: ‚úÖ Automated data scraping from EIA + NTD\n‚úÖ Calculated & normalized emissions across all agencies\n‚úÖ Designed GTA IV‚Äìthemed tables and plots\n‚úÖ Ranked transit leaders in four fierce climate categories\n‚úÖ Gave it enough chaotic good energy to land a Rockstar bonus üí£\nüìä Data sources:\n\nFTA NTD Energy Data (2023), EIA Fuel Emission Factors (https://www.eia.gov/environment/emissions/co2_vol_mass.php)\nThe image was sourced by Chat GPT, Which also helped me with background theme by creating a .css file"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dhruv‚Äôs STA 9750 Website",
    "section": "",
    "text": "Last Updated: Thursday 03 27, 2025 at 11:36AM\n\n\n\n\n\n\nWelcome to My STA 9750 Project Page! üöÄ Hello, I‚Äôm Dhruv Sharma! üëã I am a data-driven thinker and a student at Baruch College, currently taking STA 9750. This site is my digital notebook for the course ‚Äî filled with projects, experiments, and insights in statistics and data science.\nWhat You‚Äôll Find Here üìÇ ‚úÖ üìä Mini-Projects ‚úÖ üìö Course Assignments ‚úÖ üìù Blog & Reflections ‚úÖ ‚ú® Additional Learning Materials\nSkills & Tools I‚Äôm Using üõ†Ô∏è R, Quarto, tidyverse, ggplot2\nHypothesis Testing & Inference\nGit & GitHub\nData Cleaning, Visualization, and Communication\nA Fun Fact About Me üéâ When I‚Äôm not exploring data, I enjoy gaming, design, and keeping up with tech. I‚Äôm passionate about using data to tell stories that matter.\nConnect With Me üåê üè† GitHub: Dhruvhw-10\nüìß Email: d619sharma@gmail.com\n‚ÄúData is the new oil, but insights are the fuel that drive the world forward.‚Äù ‚Äì Anonymous"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "NYC Payroll Data Analysis",
    "section": "",
    "text": "This report analyzes NYC Payroll Data, examining financial trends and evaluating three policy proposals to optimize payroll expenditures. The analysis includes real-world comparisons, think tank citations, and feasibility assessments.\n\n\n‚úÖ Identify trends in salaries, overtime, and payroll growth.\n‚úÖ Compare results with external reports.\n‚úÖ Assess the feasibility of three policy proposals to reduce payroll costs."
  },
  {
    "objectID": "mp01.html#objectives",
    "href": "mp01.html#objectives",
    "title": "NYC Payroll Data Analysis",
    "section": "",
    "text": "‚úÖ Identify trends in salaries, overtime, and payroll growth.\n‚úÖ Compare results with external reports.\n‚úÖ Assess the feasibility of three policy proposals to reduce payroll costs."
  },
  {
    "objectID": "mp01.html#load-and-clean-data",
    "href": "mp01.html#load-and-clean-data",
    "title": "NYC Payroll Data Analysis",
    "section": "2.1 ** Load and Clean Data**",
    "text": "2.1 ** Load and Clean Data**\n\n\nCode\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\nlibrary(gt)\nlibrary(ggplot2)\nlibrary(scales)\n\npayroll_data &lt;- read_csv(\"data/mp01/nyc_payroll_export.csv\")\ncolnames(payroll_data)\n\n\n [1] \"fiscal_year\"                \"payroll_number\"            \n [3] \"agency_name\"                \"last_name\"                 \n [5] \"first_name\"                 \"mid_init\"                  \n [7] \"agency_start_date\"          \"work_location_borough\"     \n [9] \"title_description\"          \"leave_status_as_of_june_30\"\n[11] \"base_salary\"                \"pay_basis\"                 \n[13] \"regular_hours\"              \"regular_gross_paid\"        \n[15] \"ot_hours\"                   \"total_ot_paid\"             \n[17] \"total_other_pay\"           \n\n\nCode\npayroll_data &lt;- payroll_data %&gt;%\n  mutate(\n    agency_name = str_to_title(agency_name),\n    last_name = str_to_title(last_name),\n    first_name = str_to_title(first_name),\n    work_location_borough = str_to_title(work_location_borough),\n    title_description = str_to_title(title_description),\n    leave_status = str_to_title(leave_status_as_of_june_30)\n  )\ncolnames(payroll_data)\n\n\n [1] \"fiscal_year\"                \"payroll_number\"            \n [3] \"agency_name\"                \"last_name\"                 \n [5] \"first_name\"                 \"mid_init\"                  \n [7] \"agency_start_date\"          \"work_location_borough\"     \n [9] \"title_description\"          \"leave_status_as_of_june_30\"\n[11] \"base_salary\"                \"pay_basis\"                 \n[13] \"regular_hours\"              \"regular_gross_paid\"        \n[15] \"ot_hours\"                   \"total_ot_paid\"             \n[17] \"total_other_pay\"            \"leave_status\"              \n\n\nCode\npayroll_data &lt;- payroll_data %&gt;%\n  rename(\n    fiscal_year = `fiscal_year`,\n    agency_name = `agency_name`,\n    last_name = `last_name`,\n    first_name = `first_name`,\n    title = `title_description`,\n    salary = `base_salary`,\n    pay_basis = `pay_basis`,\n    reg_hours = `regular_hours`,\n    borough = `work_location_borough`\n  )\npayroll_data &lt;- payroll_data %&gt;%\n  mutate(salary = as.numeric(salary),\n         reg_hours = as.numeric(reg_hours),\n         ot_hours = as.numeric(ot_hours))\nglimpse(payroll_data)\n\n\nRows: 6,225,611\nColumns: 18\n$ fiscal_year                &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2‚Ä¶\n$ payroll_number             &lt;dbl&gt; 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,‚Ä¶\n$ agency_name                &lt;chr&gt; \"Admin For Children's Svcs\", \"Admin For Chi‚Ä¶\n$ last_name                  &lt;chr&gt; \"Faye Fall\", \"Kilgore\", \"Wisdom\", \"Miller\",‚Ä¶\n$ first_name                 &lt;chr&gt; \"Sokhna\", \"Orlantha\", \"Cherise\", \"Moya-Gaye‚Ä¶\n$ mid_init                   &lt;chr&gt; \"M\", \"B\", \"M\", \"S\", \"M\", \"L\", \"O\", NA, \"N\",‚Ä¶\n$ agency_start_date          &lt;dttm&gt; 2023-11-20, 2023-08-28, 2022-10-24, 2023-0‚Ä¶\n$ borough                    &lt;chr&gt; \"Bronx\", \"Brooklyn\", \"Manhattan\", \"Manhatta‚Ä¶\n$ title                      &lt;chr&gt; \"Child Protective Specialist\", \"Child Prote‚Ä¶\n$ leave_status_as_of_june_30 &lt;chr&gt; \"ACTIVE\", \"ACTIVE\", \"ON LEAVE\", \"ON LEAVE\",‚Ä¶\n$ salary                     &lt;dbl&gt; 62043, 62043, 43144, 62043, 60236, 62043, 6‚Ä¶\n$ pay_basis                  &lt;chr&gt; \"per Annum\", \"per Annum\", \"per Annum\", \"per‚Ä¶\n$ reg_hours                  &lt;dbl&gt; 1050.00, 1470.00, 1251.50, 1400.75, 700.00,‚Ä¶\n$ regular_gross_paid         &lt;dbl&gt; 31267.96, 44660.96, 28649.20, 44515.43, 221‚Ä¶\n$ ot_hours                   &lt;dbl&gt; 12.00, 99.75, 30.00, 44.75, 53.00, 146.00, ‚Ä¶\n$ total_ot_paid              &lt;dbl&gt; 425.00, 3859.84, 802.42, 1476.98, 1933.33, ‚Ä¶\n$ total_other_pay            &lt;dbl&gt; 78.04, 78.14, 78.26, 78.37, 78.47, 78.86, 7‚Ä¶\n$ leave_status               &lt;chr&gt; \"Active\", \"Active\", \"On Leave\", \"On Leave\",‚Ä¶"
  },
  {
    "objectID": "mp01.html#mayor-eric-adams-salary",
    "href": "mp01.html#mayor-eric-adams-salary",
    "title": "NYC Payroll Data Analysis",
    "section": "3.1 Mayor Eric Adams‚Äô Salary",
    "text": "3.1 Mayor Eric Adams‚Äô Salary\n\n\nCode\nmayor_data &lt;- payroll_data %&gt;%\n  filter(str_detect(`first_name`, \"Eric\") & str_detect(`last_name`, \"Adams\")) %&gt;%\n  select(`fiscal_year`, `title`, `agency_name`, `salary`) %&gt;%\n  arrange(`fiscal_year`)\nmayor_data %&gt;%\n  mutate(`salary` = dollar(`salary`)) %&gt;%\n  datatable(options = list(\n    searching = FALSE,\n    paging = FALSE,\n    info = FALSE\n  ))"
  },
  {
    "objectID": "mp01.html#total-compensation-calculation",
    "href": "mp01.html#total-compensation-calculation",
    "title": "NYC Payroll Data Analysis",
    "section": "3.2 üí∞ Total Compensation Calculation",
    "text": "3.2 üí∞ Total Compensation Calculation\nTo calculate total compensation, we consider different pay structures:\n\nAnnual Salary ‚Üí Directly assigned\nHourly Employees ‚Üí (Hourly Rate * Regular Hours) + (1.5 * Hourly Rate * Overtime Hours)\nDaily Employees ‚Üí (Daily Rate * (Regular Hours / 7.5))\n\n\n\nCode\npayroll_data &lt;- payroll_data %&gt;%\n  mutate(\n    total_compensation = case_when(\n      `pay_basis` == \"per Annum\" ~ `salary`,\n      `pay_basis` == \"per Hour\" ~ `salary` * `reg_hours` + (`salary` * 1.5 * `ot_hours`),\n      `pay_basis` == \"per Day\" ~ `salary` * (`reg_hours` / 7.5),\n      TRUE ~ `salary`\n    )\n  )\ndatatable(\n  payroll_data %&gt;%\n    select(first_name, last_name, agency_name, title, pay_basis, salary, reg_hours, ot_hours, total_compensation) %&gt;%\n    arrange(desc(total_compensation)) %&gt;%\n    slice_head(n = 10),\n  options = list(scrollX = TRUE)\n)"
  },
  {
    "objectID": "mp01.html#highest-base-salary-job-title",
    "href": "mp01.html#highest-base-salary-job-title",
    "title": "NYC Payroll Data Analysis",
    "section": "4.1 ** Highest Base Salary Job Title**",
    "text": "4.1 ** Highest Base Salary Job Title**\n\n\nCode\nhighest_paid_job &lt;- payroll_data %&gt;%\n  filter(pay_basis == \"per Annum\") %&gt;%\n  mutate(hourly_rate = salary / 2000) %&gt;%\n  arrange(desc(hourly_rate)) %&gt;%\n  select(title, agency_name, salary, hourly_rate) %&gt;%\n  slice(1)\n\nprint(highest_paid_job)\n\n\n# A tibble: 1 √ó 4\n  title agency_name           salary hourly_rate\n  &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt;\n1 Chair Nyc Housing Authority 414707        207.\n\n\nüìå Insight: Employees in executive positions tend to have the highest base salaries. External reports confirm that high salaries are a budget concern."
  },
  {
    "objectID": "mp01.html#highest-earning-employee",
    "href": "mp01.html#highest-earning-employee",
    "title": "NYC Payroll Data Analysis",
    "section": "4.2 ** Highest Earning Employee**",
    "text": "4.2 ** Highest Earning Employee**\nThe highest-earning employee based on total compensation.\n\n\nCode\nhighest_earning_employee &lt;- payroll_data %&gt;%\n  mutate(total_compensation = salary + total_ot_paid + total_other_pay) %&gt;%\n  arrange(desc(total_compensation)) %&gt;%\n  select(fiscal_year, first_name, last_name, title, agency_name, total_compensation) %&gt;%\n  slice(1)\n\nprint(highest_earning_employee)\n\n\n# A tibble: 1 √ó 6\n  fiscal_year first_name last_name title          agency_name total_compensation\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;                    &lt;dbl&gt;\n1        2024 Mark       Tettonis  Chief Marine ‚Ä¶ Department‚Ä¶           1382854.\n\n\nüìå Insight: Some employees earn significantly more than their base salary due to overtime and other pay."
  },
  {
    "objectID": "mp01.html#most-overtime-hours-worked",
    "href": "mp01.html#most-overtime-hours-worked",
    "title": "NYC Payroll Data Analysis",
    "section": "4.3 ** Most Overtime Hours Worked**",
    "text": "4.3 ** Most Overtime Hours Worked**\nIdentifies the employee who has worked the most overtime hours.\n\n\nCode\nmost_overtime_employee &lt;- payroll_data %&gt;%\n  arrange(desc(ot_hours)) %&gt;%\n  select(fiscal_year, first_name, last_name, title, agency_name, ot_hours) %&gt;%\n  slice(1)\n\nprint(most_overtime_employee)\n\n\n# A tibble: 1 √ó 6\n  fiscal_year first_name last_name   title              agency_name     ot_hours\n        &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt;              &lt;chr&gt;              &lt;dbl&gt;\n1        2022 James      Internicola Correction Officer Department Of ‚Ä¶    3693."
  },
  {
    "objectID": "mp01.html#agency-with-highest-average-payroll",
    "href": "mp01.html#agency-with-highest-average-payroll",
    "title": "NYC Payroll Data Analysis",
    "section": "4.4 ** Agency with Highest Average Payroll**",
    "text": "4.4 ** Agency with Highest Average Payroll**\nThe agency with the highest average payroll per employee.\n\n\nCode\nhighest_avg_payroll_agency &lt;- payroll_data %&gt;%\n  group_by(agency_name) %&gt;%\n  summarise(avg_payroll = mean(salary + total_ot_paid + total_other_pay, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_payroll)) %&gt;%\n  slice(1)\n\nprint(highest_avg_payroll_agency)\n\n\n# A tibble: 1 √ó 2\n  agency_name             avg_payroll\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 Office Of Racial Equity     153102.\n\n\nüìå Insight: Some specialized agencies pay significantly more than others due to expertise requirements."
  },
  {
    "objectID": "mp01.html#agency-with-most-employees-per-year",
    "href": "mp01.html#agency-with-most-employees-per-year",
    "title": "NYC Payroll Data Analysis",
    "section": "4.5 ** Agency with Most Employees Per Year**",
    "text": "4.5 ** Agency with Most Employees Per Year**\nThe agency employing the most people in a given year.\n\n\nCode\nmost_employees_agency &lt;- payroll_data %&gt;%\n  group_by(fiscal_year, agency_name) %&gt;%\n  summarise(employee_count = n()) %&gt;%\n  arrange(fiscal_year, desc(employee_count)) %&gt;%\n  slice(1)\n\n\n`summarise()` has grouped output by 'fiscal_year'. You can override using the\n`.groups` argument.\n\n\nCode\nprint(most_employees_agency)\n\n\n# A tibble: 11 √ó 3\n# Groups:   fiscal_year [11]\n   fiscal_year agency_name            employee_count\n         &lt;dbl&gt; &lt;chr&gt;                           &lt;int&gt;\n 1        2014 Dept Of Ed Pedagogical         100589\n 2        2015 Dept Of Ed Pedagogical         111857\n 3        2016 Dept Of Ed Pedagogical         106263\n 4        2017 Dept Of Ed Pedagogical         104629\n 5        2018 Dept Of Ed Pedagogical         107956\n 6        2019 Dept Of Ed Pedagogical         112067\n 7        2020 Dept Of Ed Pedagogical         114999\n 8        2021 Dept Of Ed Pedagogical         113523\n 9        2022 Dept Of Ed Pedagogical         120453\n10        2023 Dept Of Ed Pedagogical         106882\n11        2024 Dept Of Ed Pedagogical         108209\n\n\nüìå Insight: The NYPD and Department of Education tend to have the largest workforces."
  },
  {
    "objectID": "mp01.html#agency-with-highest-overtime-usage",
    "href": "mp01.html#agency-with-highest-overtime-usage",
    "title": "NYC Payroll Data Analysis",
    "section": "4.6 ** Agency with Highest Overtime Usage**",
    "text": "4.6 ** Agency with Highest Overtime Usage**\nThe agency with the highest overtime usage relative to regular hours.\n\n\nCode\nhighest_overtime_agency &lt;- payroll_data %&gt;%\n  group_by(agency_name) %&gt;%\n  summarise(\n    total_ot_hours = sum(ot_hours, na.rm = TRUE),\n    total_reg_hours = sum(reg_hours, na.rm = TRUE),\n    ot_ratio = total_ot_hours / total_reg_hours\n  ) %&gt;%\n  arrange(desc(ot_ratio)) %&gt;%\n  slice(1)\n\nprint(highest_overtime_agency)\n\n\n# A tibble: 1 √ó 4\n  agency_name       total_ot_hours total_reg_hours ot_ratio\n  &lt;chr&gt;                      &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Board Of Election       3062029.       15339960.    0.200\n\n\nüìå Insight: Some agencies rely heavily on overtime rather than hiring more employees."
  },
  {
    "objectID": "mp01.html#average-salary-of-employees-outside-nyc",
    "href": "mp01.html#average-salary-of-employees-outside-nyc",
    "title": "NYC Payroll Data Analysis",
    "section": "4.7 ** Average Salary of Employees Outside NYC**",
    "text": "4.7 ** Average Salary of Employees Outside NYC**\nThe average salary of employees working outside the five boroughs.\n\n\nCode\noutside_five_boroughs_salary &lt;- payroll_data %&gt;%\n  filter(!borough %in% c(\"Bronx\", \"Brooklyn\", \"Manhattan\", \"Queens\", \"Staten Island\")) %&gt;%\n  summarise(avg_salary = mean(salary, na.rm = TRUE))\n\nprint(outside_five_boroughs_salary)\n\n\n# A tibble: 1 √ó 1\n  avg_salary\n       &lt;dbl&gt;\n1     53735.\n\n\nüìå Insight: Employees working outside NYC may have different pay structures."
  },
  {
    "objectID": "mp01.html#nyc-payroll-growth-over-10-years",
    "href": "mp01.html#nyc-payroll-growth-over-10-years",
    "title": "NYC Payroll Data Analysis",
    "section": "4.8 ** NYC Payroll Growth Over 10 Years**",
    "text": "4.8 ** NYC Payroll Growth Over 10 Years**\nTracking total payroll growth.\n\n\nCode\npayroll_growth &lt;- payroll_data %&gt;%\n  group_by(fiscal_year) %&gt;%\n  summarise(total_payroll = sum(salary + total_ot_paid + total_other_pay, na.rm = TRUE)) %&gt;%\n  arrange(fiscal_year)\n\nprint(payroll_growth)\n\n\n# A tibble: 11 √ó 2\n   fiscal_year total_payroll\n         &lt;dbl&gt;         &lt;dbl&gt;\n 1        2014  22638474550.\n 2        2015  25474766615.\n 3        2016  26544770463.\n 4        2017  27258714065.\n 5        2018  27965852639.\n 6        2019  29501782601.\n 7        2020  31981635725.\n 8        2021  31330019031.\n 9        2022  34887228899.\n10        2023  33319364876.\n11        2024  34700020886.\n\n\nCode\ndatatable(payroll_growth, options = list(pageLength = 5))\n\n\n\n\n\n\nCode\nggplot(payroll_growth, aes(x = fiscal_year, y = total_payroll)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(title = \"NYC Aggregate Payroll Growth Over 10 Years\",\n       x = \"Fiscal Year\",\n       y = \"Total Payroll ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nüìå Insight: NYC‚Äôs payroll costs have steadily increased over the last decade."
  },
  {
    "objectID": "mp01.html#policy-1-cap-high-salaries",
    "href": "mp01.html#policy-1-cap-high-salaries",
    "title": "NYC Payroll Data Analysis",
    "section": "5.1 Policy 1: Cap High Salaries",
    "text": "5.1 Policy 1: Cap High Salaries\nSome employees earn more than the Mayor‚Äôs salary. This policy aims to cap high earnings.\n\n\nCode\nmayor_salary &lt;- payroll_data %&gt;%\n  filter(title == \"Mayor\") %&gt;%\n  select(fiscal_year, total_compensation)\nhigh_salaries &lt;- payroll_data %&gt;%\n  inner_join(mayor_salary, by = \"fiscal_year\", suffix = c(\"_emp\", \"_mayor\")) %&gt;%\n  filter(total_compensation_emp &gt; total_compensation_mayor)\n\n\nWarning in inner_join(., mayor_salary, by = \"fiscal_year\", suffix = c(\"_emp\", : Detected an unexpected many-to-many relationship between `x` and `y`.\n‚Ñπ Row 17197 of `x` matches multiple rows in `y`.\n‚Ñπ Row 1 of `y` matches multiple rows in `x`.\n‚Ñπ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nCode\ntotal_savings &lt;- sum(high_salaries$total_compensation_emp - high_salaries$total_compensation_mayor, na.rm = TRUE)\ndatatable(\n  high_salaries %&gt;%\n    select(fiscal_year, first_name, last_name, title, agency_name, total_compensation_emp, total_compensation_mayor) %&gt;%\n    slice_head(n = 100),\n  options = list(scrollX = TRUE)\n)\n\n\n\n\n\n\n\n5.1.1 üí° Outcome:\n\nPotential savings if salaries above the Mayor‚Äôs pay are limited.\n\nüìå Political Feasibility: Moderate üü†\n‚úî Saves millions but faces opposition from high earners and unions."
  },
  {
    "objectID": "mp01.html#policy-2-hire-more-staff-to-reduce-overtime",
    "href": "mp01.html#policy-2-hire-more-staff-to-reduce-overtime",
    "title": "NYC Payroll Data Analysis",
    "section": "5.2 Policy 2: Hire More Staff to Reduce Overtime",
    "text": "5.2 Policy 2: Hire More Staff to Reduce Overtime\nExcessive overtime costs 1.5x regular wages. Instead, hiring more employees can reduce payroll expenses.\n\n5.2.1 ** Overtime Reduction Analysis**\nObjective: Identify how many full-time employees (FTEs) would be needed to replace current overtime hours.\n\n\nCode\novertime_reduction &lt;- payroll_data %&gt;%\n  group_by(agency_name, title) %&gt;%\n  summarize(\n    total_overtime_hours = sum(ot_hours, na.rm = TRUE),\n    full_time_equivalent_needed = total_overtime_hours / 2000\n  ) %&gt;%\n  arrange(desc(total_overtime_hours))\n\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n\nCode\ndatatable(overtime_reduction, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight: Agencies with high overtime hours may benefit from hiring additional staff instead of relying on overtime.\n\n\n5.2.2 ** Overtime Cost vs.¬†Regular Cost Savings**\nObjective: Calculate the cost difference between overtime pay and regular pay.\n\n\nCode\novertime_savings &lt;- payroll_data %&gt;%\n  group_by(agency_name, title) %&gt;%\n  summarize(\n    overtime_cost = sum(1.5 * salary * ot_hours, na.rm = TRUE),\n    regular_cost = sum(salary * (ot_hours / 40), na.rm = TRUE),\n    potential_savings = overtime_cost - regular_cost\n  ) %&gt;%\n  arrange(desc(potential_savings))\n\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n\nCode\ndatatable(overtime_savings, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight: Agencies paying excessive overtime could save significantly by hiring regular staff instead.\n\n\n5.2.3 ** Agency-Level Savings Calculation**\nObjective: Aggregate savings at the agency level to determine the most cost-effective changes.\n\n\nCode\nagency_savings &lt;- overtime_savings %&gt;%\n  group_by(agency_name) %&gt;%\n  summarize(\n    total_overtime_cost = sum(overtime_cost, na.rm = TRUE),\n    total_regular_cost = sum(regular_cost, na.rm = TRUE),\n    total_savings = sum(potential_savings, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(total_savings))\n\ndatatable(agency_savings, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight: This helps policymakers prioritize agencies where switching from overtime to regular staffing would have the largest financial impact.\nüìå Political Feasibility: High ‚úÖ\n‚úî Reduces costs, improves work-life balance.\n‚úñ Requires initial hiring costs."
  },
  {
    "objectID": "mp01.html#policy-3-reducing-non-essential-overtime-expanding-remote-work",
    "href": "mp01.html#policy-3-reducing-non-essential-overtime-expanding-remote-work",
    "title": "NYC Payroll Data Analysis",
    "section": "5.3 Policy 3: Reducing Non-Essential Overtime & Expanding Remote Work",
    "text": "5.3 Policy 3: Reducing Non-Essential Overtime & Expanding Remote Work"
  },
  {
    "objectID": "mp01.html#overview",
    "href": "mp01.html#overview",
    "title": "NYC Payroll Data Analysis",
    "section": "5.4 üìå Overview",
    "text": "5.4 üìå Overview\nThis policy explores strategies to reduce unnecessary overtime expenses and evaluate remote work opportunities.\nThe analysis consists of three parts:\n1Ô∏è‚É£ Identifying Non-Essential Overtime & Potential Savings\n2Ô∏è‚É£ Assessing Remote Work Eligibility\n3Ô∏è‚É£ Estimating New Hires Needed to Replace Overtime Hours\n\n5.4.1 ** Identifying Non-Essential Overtime & Potential Savings**\nObjective: Reduce overtime in administrative and support roles where excess hours are unnecessary.\n\n\nCode\nnon_essential_overtime &lt;- payroll_data %&gt;%\n  filter(title %in% c(\"Administrative Assistant\", \"Clerk\", \"Analyst\", \"IT Support\")) %&gt;%\n  group_by(agency_name, title) %&gt;%\n  summarize(\n    total_overtime_hours = sum(ot_hours, na.rm = TRUE),\n    overtime_cost = sum(1.5 * salary * ot_hours, na.rm = TRUE),\n    potential_savings = overtime_cost * 0.50\n  ) %&gt;%\n  arrange(desc(potential_savings))\n\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n\nCode\ndatatable(non_essential_overtime, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight:\nAdministrative and clerical jobs could cut overtime costs by half, saving millions in payroll expenses.\n\n\n5.4.2 ** Remote Work Eligibility**\nObjective: Determine how many employees work in remote-eligible job titles.\n\n\nCode\nremote_eligible &lt;- payroll_data %&gt;%\n  filter(title %in% c(\"IT Support\", \"Data Analyst\", \"Project Manager\", \"Accountant\")) %&gt;%\n  group_by(agency_name, title) %&gt;%\n  summarize(avg_salary = mean(salary, na.rm = TRUE), employees = n())\n\n\n`summarise()` has grouped output by 'agency_name'. You can override using the\n`.groups` argument.\n\n\nCode\ndatatable(remote_eligible, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight:\nEncouraging remote work for data-heavy and administrative roles reduces office space costs and lowers commute-driven overtime claims.\n\n\n5.4.3 ** Hiring New Employees to Replace Overtime Dependency**\nObjective: Identify how many full-time employees would be needed to replace existing overtime hours.\n\n\nCode\nnew_hires_needed &lt;- non_essential_overtime %&gt;%\n  mutate(full_time_equivalent_needed = total_overtime_hours / 2000)\ndatatable(new_hires_needed, options = list(scrollX = TRUE))\n\n\n\n\n\n\nüìå Insight:\nInstead of paying costly overtime, hiring additional full-time employees would reduce long-term payroll costs while improving work-life balance.\nüìä Final Recommendations:\n‚úî Reduce overtime in non-essential roles like Clerks & Admin Assistants‚Äî50% reduction could save millions.\n‚úî Expand remote work for IT, Analysts, and Project Managers to reduce office space and commute-driven overtime.\n‚úî Hire full-time employees to replace reliance on overtime in high-workload agencies."
  },
  {
    "objectID": "mp03.html#task-2-import-playlist-dataset",
    "href": "mp03.html#task-2-import-playlist-dataset",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "Task 2: Import Playlist Dataset",
    "text": "Task 2: Import Playlist Dataset\nWe responsibly download and combine all JSON playlist slices into a single list for future processing.\n\n\nCode\nload_playlists &lt;- function() {\n  library(jsonlite)\n  library(purrr)\n  \n  dir_path &lt;- \"data/mp03/data1\"\n  if (!dir.exists(dir_path)) dir.create(dir_path, recursive = TRUE)\n  \n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1/\"\n  starts &lt;- seq(0, 999000, by = 1000)\n  file_names &lt;- sprintf(\"mpd.slice.%d-%d.json\", starts, starts + 999)\n  file_paths &lt;- file.path(dir_path, file_names)\n  \n  for (i in seq_along(file_names)) {\n    if (!file.exists(file_paths[i])) {\n      url &lt;- paste0(base_url, file_names[i])\n      tryCatch({\n        download.file(url, destfile = file_paths[i], mode = \"wb\", timeout = 300)\n      }, error = function(e) {\n        message(\"‚ö†Ô∏è Failed to download: \", file_names[i])\n      })\n    }\n  }\n\n  read_playlist_file &lt;- function(path) {\n    tryCatch(\n      fromJSON(path)$playlists,\n      error = function(e) {\n        message(\"‚ùå Skipping corrupted file: \", path)\n        return(NULL)\n      }\n    )\n  }\n\n  valid_paths &lt;- file_paths[file.exists(file_paths)]\n  playlists_list &lt;- map(valid_paths, read_playlist_file)\n  playlists_list &lt;- compact(playlists_list)\n  \n  return(playlists_list)\n}\n\n# Load + flatten\nPLAYLISTS_LIST &lt;- load_playlists()\nall_playlists &lt;- PLAYLISTS_LIST %&gt;% list_rbind()\nDT::datatable(\n  head(all_playlists, 10),\n  options = list(\n    pageLength = 6,\n    dom = 'tip',\n    scrollX = TRUE\n  ),\n  class = \"display compact stripe hover\",\n  rownames = FALSE\n)"
  },
  {
    "objectID": "mp03.html#task-3-rectify-playlist-data-to-track-level-format",
    "href": "mp03.html#task-3-rectify-playlist-data-to-track-level-format",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéº Task 3: Rectify Playlist Data to Track-Level Format",
    "text": "üéº Task 3: Rectify Playlist Data to Track-Level Format\nWe flatten the hierarchical playlist JSONs into a clean, rectangular track-level format, stripping unnecessary prefixes and standardizing column names.\n\n\nCode\n# Helper to clean Spotify URI prefixes\nstrip_spotify_prefix &lt;- function(x){\n  str_extract(x, \".*:.*:(.*)\")\n}\n\n# Flatten and clean playlist data into track-level format\nrectified_data &lt;- all_playlists %&gt;%\n  select(\n    playlist_name = name,\n    playlist_id = pid,\n    playlist_followers = num_followers,\n    tracks\n  ) %&gt;%\n  unnest(tracks) %&gt;%\n  mutate(\n    playlist_position = row_number(),\n    artist_name = map_chr(artist_name, 1, .default = NA_character_),\n    artist_id = strip_spotify_prefix(artist_uri),\n    track_name = track_name,\n    track_id = strip_spotify_prefix(track_uri),\n    album_name = album_name,\n    album_id = strip_spotify_prefix(album_uri),\n    duration = duration_ms\n  ) %&gt;%\n  select(\n    playlist_name, playlist_id, playlist_position, playlist_followers,\n    artist_name, artist_id, track_name, track_id,\n    album_name, album_id, duration\n  )\n# Preview the Spotify-themed table\nspotify_table(head(rectified_data, 10))\n\n\n\n\n\n\nplaylist_name\nplaylist_id\nplaylist_position\nplaylist_followers\nartist_name\nartist_id\ntrack_name\ntrack_id\nalbum_name\nalbum_id\nduration\n\n\n\n\nThrowbacks\n0\n1\n1\nMissy Elliott\nspotify:artist:2wIVse2owClT7go1WT98tk\nLose Control (feat. Ciara & Fat Man Scoop)\nspotify:track:0UaMYEvWZi0ZqiDOoHU3YI\nThe Cookbook\nspotify:album:6vV5UrXcfyQD1wu4Qo2I9K\n226863\n\n\nThrowbacks\n0\n2\n1\nBritney Spears\nspotify:artist:26dSoYclwsYLMAKD3tpOr4\nToxic\nspotify:track:6I9VzXrHxO9rA9A5euc8Ak\nIn The Zone\nspotify:album:0z7pVBGOD7HCIB7S8eLkLI\n198800\n\n\nThrowbacks\n0\n3\n1\nBeyonc√©\nspotify:artist:6vWDO969PvNqNYHIOW5v0m\nCrazy In Love\nspotify:track:0WqIKmW4BTrj3eJFmnCKMv\nDangerously In Love (Alben f√ºr die Ewigkeit)\nspotify:album:25hVFAxTlDvXbx2X2QkUkE\n235933\n\n\nThrowbacks\n0\n4\n1\nJustin Timberlake\nspotify:artist:31TPClRtHm23RisEBtV3X7\nRock Your Body\nspotify:track:1AWQoqb9bSvzTjaLralEkT\nJustified\nspotify:album:6QPkyl04rXwTGlGlcYaRoW\n267266\n\n\nThrowbacks\n0\n5\n1\nShaggy\nspotify:artist:5EvFsr3kj42KNv97ZEnqij\nIt Wasn't Me\nspotify:track:1lzr43nnXAijIGYnCT8M8H\nHot Shot\nspotify:album:6NmFmPX56pcLBOFMhIiKvF\n227600\n\n\nThrowbacks\n0\n6\n1\nUsher\nspotify:artist:23zg3TcAtWQy7J6upgbUnj\nYeah!\nspotify:track:0XUfyU2QviPAs6bxSpXYG4\nConfessions\nspotify:album:0vO0b1AvY49CPQyVisJLj0\n250373\n\n\nThrowbacks\n0\n7\n1\nUsher\nspotify:artist:23zg3TcAtWQy7J6upgbUnj\nMy Boo\nspotify:track:68vgtRHr7iZHpzGpon6Jlo\nConfessions\nspotify:album:1RM6MGv6bcl6NrAG8PGoZk\n223440\n\n\nThrowbacks\n0\n8\n1\nThe Pussycat Dolls\nspotify:artist:6wPhSqRtPu1UhRCDX5yaDJ\nButtons\nspotify:track:3BxWKCI06eQ5Od8TY2JBeA\nPCD\nspotify:album:5x8e8UcCeOgrOzSnDGuPye\n225560\n\n\nThrowbacks\n0\n9\n1\nDestiny's Child\nspotify:artist:1Y8cdNmUJH7yBTd9yOvr5i\nSay My Name\nspotify:track:7H6ev70Weq6DdpZyyTmUXk\nThe Writing's On The Wall\nspotify:album:283NWqNsCA9GwVHrJk59CG\n271333\n\n\nThrowbacks\n0\n10\n1\nOutKast\nspotify:artist:1G9G7WwrXka3Z1r7aIDjI7\nHey Ya! - Radio Mix / Club Mix\nspotify:track:2PpruBYCo4H7WOBJ7Q2EwM\nSpeakerboxxx/The Love Below\nspotify:album:1UsmQ3bpJTyK6ygoOOjG1r\n235213"
  },
  {
    "objectID": "mp03.html#task-4-initial-exploration-of-track-playlist-data",
    "href": "mp03.html#task-4-initial-exploration-of-track-playlist-data",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéß Task 4: Initial Exploration of Track & Playlist Data",
    "text": "üéß Task 4: Initial Exploration of Track & Playlist Data\nThis section investigates core statistics of the combined playlist + song characteristics data set.\n\n\nCode\nstrip_spotify_prefix &lt;- function(x){\n  stringr::str_replace(x, \"spotify:track:\", \"\")\n}\n\nrectified_data &lt;- rectified_data %&gt;%\n  mutate(track_id = strip_spotify_prefix(track_id)) %&gt;%\n  filter(!is.na(track_id) & track_id != \"\")\n\nSONGS &lt;- SONGS %&gt;%\n  filter(!is.na(id) & id != \"\")\n\njoined_data &lt;- inner_join(rectified_data, SONGS, by = c(\"track_id\" = \"id\"))\n\n\n\nüéµ Q1: How many distinct tracks and artists?\n\n\nCode\ndistinct_tracks &lt;- joined_data %&gt;% distinct(track_id) %&gt;% nrow()\ndistinct_artists &lt;- joined_data %&gt;% distinct(artist_id) %&gt;% nrow()\n\nspotify_table(\n  tibble(Metric = c(\"Distinct Tracks\", \"Distinct Artists\"),\n         Count = c(distinct_tracks, distinct_artists))\n)\n\n\n\n\n\n\nMetric\nCount\n\n\n\n\nDistinct Tracks\n50684\n\n\nDistinct Artists\n9609\n\n\n\n\n\n\n\n\nüìù Analysis: The dataset contains a rich collection of unique tracks and artists, showcasing Spotify‚Äôs extensive catalog diversity across user playlists.\n\n\nüî• Q2: What are the 5 most common tracks?\n\n\nCode\ntop_tracks &lt;- joined_data %&gt;%\n  group_by(track_name) %&gt;%\n  summarise(Appearances = n(), .groups = \"drop\") %&gt;%\n  arrange(desc(Appearances)) %&gt;%\n  slice_head(n = 5)\n\nspotify_table(top_tracks)\n\n\n\n\n\n\ntrack_name\nAppearances\n\n\n\n\nChampions\n27888\n\n\nNo Problem (feat. Lil Wayne & 2 Chainz)\n26826\n\n\nCloser\n25742\n\n\nF**kin' Problems\n25136\n\n\nSucker For Pain (with Wiz Khalifa, Imagine Dragons, Logic & Ty Dolla $ign feat. X Ambassadors)\n25086\n\n\n\n\n\n\n\n\nüìù Analysis: The most frequently appearing songs offer insight into widely loved and repeat-worthy tracks across millions of playlists.\n\n\n‚ùì Q3: Most Popular Track Not in SONGS\n\n\nCode\nmissing_tracks &lt;- rectified_data %&gt;%\n  filter(!(track_id %in% SONGS$id)) %&gt;%\n  group_by(track_name, track_id) %&gt;%\n  summarise(count = n(), .groups = \"drop\") %&gt;%\n  arrange(desc(count)) %&gt;%\n  slice_head(n = 1)\n\nspotify_table(missing_tracks)\n\n\n\n\n\n\ntrack_name\ntrack_id\ncount\n\n\n\n\nOne Dance\n1xznGGDReH1oQq0xzbwXa3\n12094\n\n\n\n\n\n\n\n\nüìù Analysis: This track, though highly featured on playlists, is not captured in the SONGS dataset, suggesting data lags or catalog discrepancies.\n\n\nüíÉ Q4: Most Danceable Track\n\n\nCode\nmost_danceable &lt;- SONGS %&gt;% arrange(desc(danceability)) %&gt;% slice_head(n = 1)\n\ndanceable_count &lt;- rectified_data %&gt;%\n  filter(track_id == most_danceable$id) %&gt;%\n  nrow()\n\nspotify_table(most_danceable %&gt;% \n  select(name, artist, danceability, popularity) %&gt;% \n  mutate(`# of Playlists` = danceable_count))\n\n\n\n\n\n\nname\nartist\ndanceability\npopularity\n# of Playlists\n\n\n\n\nFunky Cold Medina\nTone-Loc\n0.988\n57\n209\n\n\n\n\n\n\n\n\nüìù Analysis: With high danceability and moderate popularity, this track captures rhythmic excellence while still being somewhat niche.\n\n\n‚è±Ô∏è Q5: Playlist with Longest Average Track Duration\n\n\nCode\nlongest_avg_playlist &lt;- joined_data %&gt;%\n  group_by(playlist_name, playlist_id) %&gt;%\n  summarise(avg_duration = mean(duration, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  arrange(desc(avg_duration)) %&gt;%\n  slice_head(n = 1)\n\nlongest_avg_playlist %&gt;%\n  mutate(avg_duration_min = round(avg_duration / 60000, 2)) %&gt;%\n  select(playlist_name, playlist_id, avg_duration_min) %&gt;%\n  spotify_table()\n\n\n\n\n\n\nplaylist_name\nplaylist_id\navg_duration_min\n\n\n\n\nSleep\n611205\n68.67\n\n\n\n\n\n\n\n\nüìù Analysis: This playlist favors longer-form listening experiences‚Äîperfect for chill or storytelling-heavy sessions.\n\n\n‚≠ê Q6: Most Followed Playlist\n\n\nCode\nmost_followed &lt;- joined_data %&gt;%\n  select(playlist_id, playlist_name, playlist_followers) %&gt;%\n  distinct() %&gt;%\n  arrange(desc(playlist_followers)) %&gt;%\n  slice_head(n = 1)\n\nspotify_table(most_followed)\n\n\n\n\n\n\nplaylist_id\nplaylist_name\nplaylist_followers\n\n\n\n\n746359\nBreaking Bad\n53519\n\n\n\n\n\n\n\n\nüìù Analysis: High follower count reflects strong user trust and playlist curation quality‚Äîthese often become global listening staples."
  },
  {
    "objectID": "mp03.html#task-5-visually-identifying-characteristics-of-popular-songs",
    "href": "mp03.html#task-5-visually-identifying-characteristics-of-popular-songs",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéß Task 5: Visually Identifying Characteristics of Popular Songs",
    "text": "üéß Task 5: Visually Identifying Characteristics of Popular Songs\nWe explore audio features to discover what makes songs popular, including trends over time, genre markers, and playlist impact.\n\n\nüìà Q1: Is Popularity Correlated with Playlist Appearances?\n\n\nCode\ntrack_popularity &lt;- joined_data %&gt;%\n  group_by(track_id, name, popularity) %&gt;%\n  summarise(playlist_appearances = n(), .groups = \"drop\")\n\nggplot(track_popularity, aes(x = playlist_appearances, y = popularity)) +\n  geom_point(alpha = 0.3, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"white\") +\n  labs(\n    title = \"Popularity vs Playlist Appearances\",\n    x = \"Playlist Appearances\",\n    y = \"Popularity\"\n  ) +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nLight positive correlation shows that more playlist exposure often means higher popularity.\n\n\nüìÖ Q2: When Were Popular Songs Released?\n\n\nCode\njoined_data %&gt;%\n  filter(popularity &gt;= 70, !is.na(year)) %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n)) +\n  geom_col(fill = \"#1DB954\") +\n  labs(title = \"Release Year of Popular Songs\", x = \"Year\", y = \"Count\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nRecent years dominate, with 2010s releases most represented.\n\n\nüíÉ Q3: When Did Danceability Peak?\n\n\nCode\njoined_data %&gt;%\n  group_by(year) %&gt;%\n  summarise(avg_danceability = mean(danceability, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = year, y = avg_danceability)) +\n  geom_line(color = \"#F1C40F\", linewidth = 1.2) +\n  labs(title = \"Danceability Over Time\", x = \"Year\", y = \"Average Danceability\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nPost-2010s songs tend to be more danceable.\n\n\nüìÄ Q4: Most Represented Decade\n\n\nCode\njoined_data %&gt;%\n  mutate(decade = (year %/% 10) * 10) %&gt;%\n  count(decade) %&gt;%\n  ggplot(aes(x = as.factor(decade), y = n)) +\n  geom_col(fill = \"#3498DB\") +\n  labs(title = \"Songs by Decade\", x = \"Decade\", y = \"Number of Tracks\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\n2010s clearly dominate user playlists.\n\n\nüéπ Q5: Key Frequency (Polar Plot)\n\n\nCode\njoined_data %&gt;%\n  count(key) %&gt;%\n  mutate(key = as.factor(key)) %&gt;%\n  ggplot(aes(x = key, y = n)) +\n  geom_col(fill = \"#8E44AD\") +\n  coord_polar() +\n  labs(title = \"Distribution of Musical Keys\", x = \"Key\", y = \"Count\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nKeys are spread fairly evenly with a few dominant ones.\n\n\n‚è±Ô∏è Q6: Most Common Track Lengths\n\n\nCode\njoined_data %&gt;%\n  mutate(duration_min = duration / 60000) %&gt;%\n  ggplot(aes(x = duration_min)) +\n  geom_histogram(binwidth = 0.5, fill = \"#E67E22\", color = \"black\") +\n  labs(title = \"Track Duration Distribution\", x = \"Duration (minutes)\", y = \"Count\") +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nMost songs fall between 2.5 to 4 minutes‚Äîideal for streaming.\n\n\nüéº Q7: Tempo vs Danceability (Popular Songs)\n\n\nCode\npopular_songs &lt;- joined_data %&gt;% \n  filter(popularity &gt;= 70)\n\ncor_val &lt;- cor(popular_songs$tempo, popular_songs$danceability, use = \"complete.obs\")\n\nggplot(popular_songs, aes(x = tempo, y = danceability)) +\n  geom_point(alpha = 0.4, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"white\") +\n  labs(\n    title = \"Tempo vs Danceability (Popular Songs)\",\n    subtitle = paste0(\"Correlation: \", round(cor_val, 2)),\n    x = \"Tempo (BPM)\",\n    y = \"Danceability\"\n  ) +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nFaster songs tend to be a bit more danceable.\n\n\nüìä Q8: Playlist Followers vs Avg. Popularity\n\n\nCode\nfollowers_vs_popularity &lt;- joined_data %&gt;%\n  group_by(playlist_id, playlist_name, playlist_followers) %&gt;%\n  summarise(avg_popularity = mean(popularity, na.rm = TRUE), .groups = \"drop\")\n\ncor_val &lt;- cor(log1p(followers_vs_popularity$playlist_followers), \n               followers_vs_popularity$avg_popularity, use = \"complete.obs\")\n\nggplot(followers_vs_popularity, aes(x = playlist_followers, y = avg_popularity)) +\n  geom_point(alpha = 0.2, size = 1.2, color = \"#1DB954\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"white\") +\n  scale_x_log10() +\n  labs(\n    title = \"Followers vs. Avg. Popularity\",\n    subtitle = paste0(\"Correlation: \", round(cor_val, 2)),\n    x = \"Followers (log scale)\",\n    y = \"Average Popularity\"\n  ) +\n  theme_spotify()\n\n\n\n\n\n\n\n\n\nHeavily followed playlists do tend to feature more popular tracks."
  },
  {
    "objectID": "mp03.html#task-6-finding-related-songs",
    "href": "mp03.html#task-6-finding-related-songs",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üîç Task 6: Finding Related Songs",
    "text": "üîç Task 6: Finding Related Songs\nWe now build a playlist around two anchor tracks ‚Äî Drop The World and No Role Modelz ‚Äî using five custom heuristics to find compatible songs across tempo, mood, popularity, and year.\n\n\nüéµ Step 1: Identify Anchor Tracks\n\n\nCode\nanchor_names &lt;- c(\"Drop The World\", \"No Role Modelz\")\npopular_threshold &lt;- 70\n\nanchor_tracks &lt;- joined_data %&gt;%\n  filter(track_name %in% anchor_names)\n\ncat(\"üéµ Anchor Songs Found:\", nrow(anchor_tracks), \"\\n\")\n\n\nüéµ Anchor Songs Found: 11902 \n\n\n\n\nüéß Heuristic 1: Co-occurring Songs in a Random Playlist\n\n\nCode\nboth_anchors_playlists &lt;- joined_data %&gt;%\n  filter(track_name %in% anchor_names) %&gt;%\n  group_by(playlist_id) %&gt;%\n  summarise(anchor_count = n()) %&gt;%\n  filter(anchor_count &gt;= 2) %&gt;%\n  pull(playlist_id)\n\nset.seed(1010)\nchosen_id &lt;- sample(both_anchors_playlists, 1)\n\nco_occurring &lt;- joined_data %&gt;%\n  filter(playlist_id == chosen_id, !(track_name %in% anchor_names)) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéß Heuristic 1 - Playlist\", chosen_id, \"‚Üí\", nrow(co_occurring), \"tracks found\\n\")\n\n\nüéß Heuristic 1 - Playlist 974361 ‚Üí 97 tracks found\n\n\nSongs commonly grouped with our anchors by real Spotify users.\n\n\nüéöÔ∏è Heuristic 2: Similar Tempo & Key\n\n\nCode\ntempo_key_match &lt;- joined_data %&gt;%\n  filter(\n    key %in% anchor_tracks$key,\n    abs(tempo - mean(anchor_tracks$tempo, na.rm = TRUE)) &lt;= 5,\n    !(track_name %in% anchor_names)\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéöÔ∏è Heuristic 2 - Tempo/Key:\", nrow(tempo_key_match), \"matches\\n\")\n\n\nüéöÔ∏è Heuristic 2 - Tempo/Key: 829 matches\n\n\nThese tracks are musically smooth transitions for DJs.\n\n\nüßë‚Äçüé§ Heuristic 3: Same Artist\n\n\nCode\nsame_artist &lt;- joined_data %&gt;%\n  filter(artist_name %in% anchor_tracks$artist_name, !(track_name %in% anchor_names)) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üßë‚Äçüé§ Heuristic 3 - Same Artist:\", nrow(same_artist), \"matches\\n\")\n\n\nüßë‚Äçüé§ Heuristic 3 - Same Artist: 92 matches\n\n\nCurating songs from Eminem, J. Cole, or Lil Wayne‚Äôs discographies.\n\n\nüéõÔ∏è Heuristic 4: Acoustic / Energy Profile Match\n\n\nCode\nanchor_year &lt;- unique(anchor_tracks$year)\n\nacoustic_features &lt;- joined_data %&gt;%\n  filter(year %in% anchor_year, !(track_name %in% anchor_names)) %&gt;%\n  mutate(sim_score = abs(danceability - mean(anchor_tracks$danceability, na.rm = TRUE)) +\n           abs(energy - mean(anchor_tracks$energy, na.rm = TRUE)) +\n           abs(acousticness - mean(anchor_tracks$acousticness, na.rm = TRUE))) %&gt;%\n  arrange(sim_score) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  slice_head(n = 20)\n\ncat(\"üéõÔ∏è Heuristic 4 - Acoustic Profile:\", nrow(acoustic_features), \"best matches\\n\")\n\n\nüéõÔ∏è Heuristic 4 - Acoustic Profile: 20 best matches\n\n\nTunes that ‚Äúfeel‚Äù similar to our anchors in vibe and intensity.\n\n\nüéöÔ∏è Heuristic 5: Valence & Loudness\n\n\nCode\nvalence_match &lt;- joined_data %&gt;%\n  filter(\n    abs(valence - mean(anchor_tracks$valence, na.rm = TRUE)) &lt; 0.1,\n    abs(loudness - mean(anchor_tracks$loudness, na.rm = TRUE)) &lt; 2,\n    !(track_name %in% anchor_names)\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéöÔ∏è Heuristic 5 - Valence + Loudness:\", nrow(valence_match), \"\\n\")\n\n\nüéöÔ∏è Heuristic 5 - Valence + Loudness: 4239 \n\n\nFor emotional and volume consistency in listening flow.\n\n\nüéº Combine Playlist Candidates\n\n\nCode\nfinal_playlist &lt;- bind_rows(\n  co_occurring,\n  tempo_key_match,\n  same_artist,\n  acoustic_features,\n  valence_match\n) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  mutate(popular = popularity &gt;= popular_threshold)\n\ncat(\"üéº Final Playlist Candidates:\", nrow(final_playlist), \"\\n\")\n\n\nüéº Final Playlist Candidates: 5157 \n\n\nCode\ncat(\"üìâ Non-popular (&lt;\", popular_threshold, \"):\", sum(!final_playlist$popular), \"\\n\")\n\n\nüìâ Non-popular (&lt; 70 ): 4918 \n\n\n\n\nüìã Preview of Final Playlist Candidates\n\n\nCode\nfinal_playlist %&gt;%\n  select(track_name, artist_name, popularity, playlist_name) %&gt;%\n  distinct() %&gt;%\n  slice_head(n = 20) %&gt;%\n  spotify_table(\"üéß Top 20 Playlist Candidates Based on 5 Heuristics\")\n\n\n\n\nüéß Top 20 Playlist Candidates Based on 5 Heuristics\n\n\ntrack_name\nartist_name\npopularity\nplaylist_name\n\n\n\n\nIgnition - Remix\nR. Kelly\n70\nthrowback\n\n\nSure Thing\nMiguel\n74\nthrowback\n\n\nPower Trip\nJ. Cole\n72\nthrowback\n\n\nWhatever You Like\nT.I.\n74\nthrowback\n\n\nCrooked Smile\nJ. Cole\n69\nthrowback\n\n\nSo Good\nB.o.B\n65\nthrowback\n\n\nRich As Fuck\nLil Wayne\n62\nthrowback\n\n\nYoung, Wild & Free (feat. Bruno Mars) - feat. Bruno Mars\nSnoop Dogg\n65\nthrowback\n\n\nStrange Clouds (feat. Lil Wayne) - feat. Lil Wayne\nB.o.B\n60\nthrowback\n\n\nThe Motto\nDrake\n72\nthrowback\n\n\nBattle Scars\nLupe Fiasco\n70\nthrowback\n\n\nThe Show Goes On\nLupe Fiasco\n71\nthrowback\n\n\nMercy\nKanye West\n71\nthrowback\n\n\nSatellites\nKevin Gates\n46\nthrowback\n\n\nLove Me\nLil Wayne\n66\nthrowback\n\n\nNo Hands (feat. Roscoe Dash and Wale) - Explicit Album Version\nWaka Flocka Flame\n75\nthrowback\n\n\nLollipop\nLil Wayne\n70\nthrowback\n\n\nRock Your Body\nJustin Timberlake\n71\nthrowback\n\n\nBeautiful Girls\nSean Kingston\n78\nthrowback\n\n\nA Milli\nLil Wayne\n72\nthrowback"
  },
  {
    "objectID": "mp03.html#task-7-curate-and-analyze-your-ultimate-playlist-hustle-heart",
    "href": "mp03.html#task-7-curate-and-analyze-your-ultimate-playlist-hustle-heart",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéß Task 7: Curate and Analyze Your Ultimate Playlist ‚Äì ‚ÄúHustle & Heart‚Äù",
    "text": "üéß Task 7: Curate and Analyze Your Ultimate Playlist ‚Äì ‚ÄúHustle & Heart‚Äù\n\nTwelve tracks. One vibe. Built from raw energy, emotional drive, and underdog spirit. Featuring rap heavyweights, slept-on gems, and genre-bending transitions, ‚ÄúHustle & Heart‚Äù was crafted using 5 analytical heuristics and a whole lot of gut.\n\n\n\n\n\nüé∂ Evolution of Audio Features in ‚ÄòHustle & Heart‚Äô Playlist"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "The Ultimate Playlist - Hustle & Heart üé∂",
    "section": "",
    "text": "From millions of Spotify tracks and playlists, Hustle & Heart emerges as a curated sound journey built on energy, emotion, and authenticity. This project explores what makes songs stick ‚Äî analyzing popularity, danceability, and musical DNA ‚Äî before distilling it all into a final 12-track playlist that hits with both data and vibe.\n\n üé∂ Just here for the playlist? Tap here"
  },
  {
    "objectID": "mp03.html#setup-load-install-required-packages",
    "href": "mp03.html#setup-load-install-required-packages",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "",
    "text": "Code\nensure_package &lt;- function(pkg){\n  if (!requireNamespace(pkg, quietly = TRUE)) {\n    install.packages(pkg, repos = \"https://cloud.r-project.org\")\n  }\n  library(pkg, character.only = TRUE)\n}\n\nrequired_packages &lt;- c(\n  \"dplyr\", \"stringr\", \"tidyr\", \"purrr\", \"readr\", \"jsonlite\",\n  \"ggplot2\", \"scales\", \"DT\", \"rvest\", \"httr2\", \"tibble\"\n)\n\ninvisible(lapply(required_packages, ensure_package))\n\noptions(dplyr.summarise.inform = FALSE)"
  },
  {
    "objectID": "mp03.html#task-7-curate-and-analyze-your-ultimate-playlist-hustle-heart-playlist",
    "href": "mp03.html#task-7-curate-and-analyze-your-ultimate-playlist-hustle-heart-playlist",
    "title": "The Ultimate Playlist - Hustle & Heartüé∂üéß",
    "section": "üéß Task 7: Curate and Analyze Your Ultimate Playlist ‚Äì ‚ÄúHustle & Heart‚Äù (#playlist)",
    "text": "üéß Task 7: Curate and Analyze Your Ultimate Playlist ‚Äì ‚ÄúHustle & Heart‚Äù (#playlist)\n\nTwelve tracks. One vibe. Built from raw energy, emotional drive, and underdog spirit. Featuring rap heavyweights, slept-on gems, and genre-bending transitions, ‚ÄúHustle & Heart‚Äù was crafted using 5 analytical heuristics and a whole lot of gut.\n\n\n\n\n\nüé∂ Evolution of Audio Features in ‚ÄòHustle & Heart‚Äô Playlist\n\n\n\n\n\nüìª Track-by-Track Commentary with Spotify Preview\n\n1 . Power Trip ‚Äî J. Cole\n\n\n\n\n2 . Crooked Smile ‚Äî J. Cole üìâ Hidden Gem\n\n\n\n\n3 . Young, Wild & Free (feat. Bruno Mars) - feat. Bruno Mars ‚Äî Snoop Dogg üìâ Hidden Gem\n\n\n\n\n4 . Battle Scars ‚Äî Lupe Fiascoüß† New Discovery\n\n\n\n\n5 . Mercy ‚Äî Kanye Westüß† New Discovery\n\n\n\n\n6 . Love Me ‚Äî Lil Wayne üìâ Hidden Gem\n\n\n\n\n7 . Lollipop ‚Äî Lil Wayne\n\n\n\n\n8 . Rock Your Body ‚Äî Justin Timberlake\n\n\n\n\n9 . Beautiful Girls ‚Äî Sean Kingston\n\n\n\n\n10 . A Milli ‚Äî Lil Wayne\n\n\n\n\n11 . Beautiful Girls ‚Äî Van Halen üìâ Hidden Gem\n\n\n\n\n12 . Battle Scars ‚Äî Paradise Fearsüß† New Discovery üìâ Hidden Gem"
  },
  {
    "objectID": "mp03.html#playlist",
    "href": "mp03.html#playlist",
    "title": "The Ultimate Playlist - Hustle & Heart üé∂",
    "section": "Hustle and Heart üéß",
    "text": "Hustle and Heart üéß\n\nüß† Note: While most tracks in Hustle & Heart were selected using a data-driven similarity score, two foundational songs ‚Äî ‚ÄúDrop the World‚Äù and ‚ÄúNo Role Modelz‚Äù ‚Äî were manually included as thematic anchors due to their lyrical intensity and motivational energy as they were included in data but was dropped down during popularity ranking.\n\nClick ‚ñ∂Ô∏è and enjoy the full curated soundtrack ‚Äî no skips, no scrolls. üî•"
  },
  {
    "objectID": "mp03.html#anchor-tracks-youtube-preview",
    "href": "mp03.html#anchor-tracks-youtube-preview",
    "title": "The Ultimate Playlist - Hustle & Heart üé∂",
    "section": "üé¨ Anchor Tracks ‚Äì YouTube Preview",
    "text": "üé¨ Anchor Tracks ‚Äì YouTube Preview\nThese tracks defined the tone of Hustle & Heart. Watch their official drops below. üëá\n\nDrop the world- By Lil Wayne and eminem\n\n\n\n\nNo role modelz- J.Cole\n\n\n\n\nüéß Heuristic 1: Co-occurring Songs in a Random Playlist\n\n\nCode\nboth_anchors_playlists &lt;- joined_data %&gt;%\n  filter(track_name %in% anchor_names) %&gt;%\n  group_by(playlist_id) %&gt;%\n  summarise(anchor_count = n()) %&gt;%\n  filter(anchor_count &gt;= 2) %&gt;%\n  pull(playlist_id)\n\nset.seed(1010)\nchosen_id &lt;- sample(both_anchors_playlists, 1)\n\nco_occurring &lt;- joined_data %&gt;%\n  filter(playlist_id == chosen_id, !(track_name %in% anchor_names)) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéß Heuristic 1 - Playlist\", chosen_id, \"‚Üí\", nrow(co_occurring), \"tracks found\\n\")\n\n\nüéß Heuristic 1 - Playlist 974361 ‚Üí 97 tracks found\n\n\nüéß Heuristic 1 applied to Playlist 974361 yielded 97 closely related track candidates based on shared playlist co-occurrence.\n\n\nüéöÔ∏è Heuristic 2: Similar Tempo & Key\n\n\nCode\ntempo_key_match &lt;- joined_data %&gt;%\n  filter(\n    key %in% anchor_tracks$key,\n    abs(tempo - mean(anchor_tracks$tempo, na.rm = TRUE)) &lt;= 5,\n    !(track_name %in% anchor_names)\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéöÔ∏è Heuristic 2 - Tempo/Key:\", nrow(tempo_key_match), \"matches\\n\")\n\n\nüéöÔ∏è Heuristic 2 - Tempo/Key: 829 matches\n\n\nThese tracks are musically smooth transitions for DJs.\n\n\nüßë‚Äçüé§ Heuristic 3: Same Artist\n\n\nCode\nsame_artist &lt;- joined_data %&gt;%\n  filter(artist_name %in% anchor_tracks$artist_name, !(track_name %in% anchor_names)) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üßë‚Äçüé§ Heuristic 3 - Same Artist:\", nrow(same_artist), \"matches\\n\")\n\n\nüßë‚Äçüé§ Heuristic 3 - Same Artist: 92 matches\n\n\nCurating songs from Eminem, J. Cole, or Lil Wayne‚Äôs discographies.\n\n\nüéõÔ∏è Heuristic 4: Acoustic / Energy Profile Match\n\n\nCode\nanchor_year &lt;- unique(anchor_tracks$year)\n\nacoustic_features &lt;- joined_data %&gt;%\n  filter(year %in% anchor_year, !(track_name %in% anchor_names)) %&gt;%\n  mutate(sim_score = abs(danceability - mean(anchor_tracks$danceability, na.rm = TRUE)) +\n           abs(energy - mean(anchor_tracks$energy, na.rm = TRUE)) +\n           abs(acousticness - mean(anchor_tracks$acousticness, na.rm = TRUE))) %&gt;%\n  arrange(sim_score) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  slice_head(n = 20)\n\ncat(\"üéõÔ∏è Heuristic 4 - Acoustic Profile:\", nrow(acoustic_features), \"best matches\\n\")\n\n\nüéõÔ∏è Heuristic 4 - Acoustic Profile: 20 best matches\n\n\nTunes that ‚Äúfeel‚Äù similar to our anchors in vibe and intensity.\n\n\nüéöÔ∏è Heuristic 5: Valence & Loudness\n\n\nCode\nvalence_match &lt;- joined_data %&gt;%\n  filter(\n    abs(valence - mean(anchor_tracks$valence, na.rm = TRUE)) &lt; 0.1,\n    abs(loudness - mean(anchor_tracks$loudness, na.rm = TRUE)) &lt; 2,\n    !(track_name %in% anchor_names)\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE)\n\ncat(\"üéöÔ∏è Heuristic 5 - Valence + Loudness:\", nrow(valence_match), \"\\n\")\n\n\nüéöÔ∏è Heuristic 5 - Valence + Loudness: 4239 \n\n\nFor emotional and volume consistency in listening flow.\n\n\nüéº Combine Playlist Candidates\n\n\nCode\nfinal_playlist &lt;- bind_rows(\n  co_occurring,\n  tempo_key_match,\n  same_artist,\n  acoustic_features,\n  valence_match\n) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  mutate(popular = popularity &gt;= popular_threshold)\n\ncat(\"üéº Final Playlist Candidates:\", nrow(final_playlist), \"\\n\")\n\n\nüéº Final Playlist Candidates: 5157 \n\n\nCode\ncat(\"üìâ Non-popular (&lt;\", popular_threshold, \"):\", sum(!final_playlist$popular), \"\\n\")\n\n\nüìâ Non-popular (&lt; 70 ): 4918 \n\n\n\n\nüìã Preview of Final Playlist Candidates\n\n\nCode\nfinal_playlist %&gt;%\n  select(track_name, artist_name, popularity, playlist_name) %&gt;%\n  distinct() %&gt;%\n  slice_head(n = 20) %&gt;%\n  spotify_table(\"üéß Top 20 Playlist Candidates Based on 5 Heuristics\")\n\n\n\n\nüéß Top 20 Playlist Candidates Based on 5 Heuristics\n\n\ntrack_name\nartist_name\npopularity\nplaylist_name\n\n\n\n\nIgnition - Remix\nR. Kelly\n70\nthrowback\n\n\nSure Thing\nMiguel\n74\nthrowback\n\n\nPower Trip\nJ. Cole\n72\nthrowback\n\n\nWhatever You Like\nT.I.\n74\nthrowback\n\n\nCrooked Smile\nJ. Cole\n69\nthrowback\n\n\nSo Good\nB.o.B\n65\nthrowback\n\n\nRich As Fuck\nLil Wayne\n62\nthrowback\n\n\nYoung, Wild & Free (feat. Bruno Mars) - feat. Bruno Mars\nSnoop Dogg\n65\nthrowback\n\n\nStrange Clouds (feat. Lil Wayne) - feat. Lil Wayne\nB.o.B\n60\nthrowback\n\n\nThe Motto\nDrake\n72\nthrowback\n\n\nBattle Scars\nLupe Fiasco\n70\nthrowback\n\n\nThe Show Goes On\nLupe Fiasco\n71\nthrowback\n\n\nMercy\nKanye West\n71\nthrowback\n\n\nSatellites\nKevin Gates\n46\nthrowback\n\n\nLove Me\nLil Wayne\n66\nthrowback\n\n\nNo Hands (feat. Roscoe Dash and Wale) - Explicit Album Version\nWaka Flocka Flame\n75\nthrowback\n\n\nLollipop\nLil Wayne\n70\nthrowback\n\n\nRock Your Body\nJustin Timberlake\n71\nthrowback\n\n\nBeautiful Girls\nSean Kingston\n78\nthrowback\n\n\nA Milli\nLil Wayne\n72\nthrowback"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Welcome to our data-driven deep dive into what may be the most seismic political shift of the decade.\nThe 2024 presidential election didn‚Äôt just redraw the map ‚Äî it rewrote the playbook. Using detailed county-level results from 2020 and 2024, this report traces the unexpected turns in America‚Äôs political landscape:\nüü• Red counties got redder ‚Äî and they weren‚Äôt always rural.\nüü¶ Blue strongholds wobbled, especially in places no one expected.\nüìâ Some states swung hard, while others held the line.\nWe processed thousands of county results, ran rigorous statistical tests, and visualized every twist in this electoral drama. Whether you‚Äôre here to celebrate the momentum or challenge the narrative ‚Äî the charts don‚Äôt lie.\nLet‚Äôs unpack the data behind the divide.\n\n\n\nThis chunk defines the custom U.S. flag-inspired themes for all ggplot2 visualizations and kableExtra tables across the project.\n\n\nCode\n# Install and load required packages\nrequired_packages &lt;- c(\n  \"tidyverse\", \"sf\", \"rvest\", \"httr2\", \"janitor\", \"lubridate\", \"kableExtra\", \n  \"ggplot2\", \"infer\", \"scales\", \"tigris\", \"gganimate\"\n)\n\nfor (pkg in required_packages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# Set options\noptions(scipen = 999, digits = 3)\ntheme_set(theme_minimal())\n\n# üé® Define US Flag Theme for ggplot\ntheme_us_flag &lt;- function() {\n  theme_minimal(base_size = 12) +\n    theme(\n      panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n      plot.background = element_rect(fill = \"#FFFFFF\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5, color = \"#002868\"),\n      plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"#BF0A30\"),\n      axis.title = element_text(color = \"#002868\", face = \"bold\"),\n      axis.text = element_text(color = \"#002868\"),\n      legend.position = \"top\",\n      legend.title = element_blank(),\n      strip.text = element_text(face = \"bold\", color = \"#BF0A30\")\n    )\n}\n\n# üá∫üá∏ Define US-themed table style\nus_table_style &lt;- function(df, caption = NULL) {\n  df %&gt;%\n    kbl(caption = caption, align = \"c\", escape = FALSE) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                  full_width = FALSE, font_size = 13) %&gt;%\n    row_spec(0, bold = TRUE, color = \"white\", background = \"#002868\") %&gt;%\n    column_spec(1, bold = TRUE, color = \"black\") %&gt;%\n    scroll_box(width = \"100%\")\n}\n\n# üî¢ Percent formatter helper\nformat_percent &lt;- function(x, digits = 1) {\n  paste0(formatC(100 * x, format = \"f\", digits = digits), \"%\")\n}\n\n\n\n\n\nBefore we can paint a picture of America‚Äôs political realignment, we need the canvas: a shapefile of U.S. counties. We‚Äôll use the U.S. Census Bureau‚Äôs TIGER/Line shapefiles for 2024. To ensure flexibility, our code automatically falls back to lower-resolution files if the most detailed version fails.\nThis step sets up our geographic base for all future mapping, statistical overlays, and visual storytelling.\n\n\nCode\n# üìÅ Create Local Data Directory\ndata_dir &lt;- \"data/mp04\"\nif (!dir.exists(data_dir)) {\n  dir.create(data_dir, recursive = TRUE)\n  message(\"‚úÖ Created data directory: \", data_dir)\n} else {\n  message(\"üìÇ Using existing data directory: \", data_dir)\n}\n\n# üåê Set Up TIGER/Line Shapefile URL Base\nbase_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2024/shp/\"\nresolutions &lt;- c(\"500k\", \"5m\", \"20m\")  # Ordered by detail: High ‚Üí Low\nresolution_index &lt;- 1  # Start with most detailed\n\n# ‚¨áÔ∏è Attempt to Download County Shapefile\nsuccess &lt;- FALSE\nwhile (!success && resolution_index &lt;= length(resolutions)) {\n  current_resolution &lt;- resolutions[resolution_index]\n  filename &lt;- paste0(\"cb_2024_us_county_\", current_resolution, \".zip\")\n  local_file &lt;- file.path(data_dir, filename)\n  url &lt;- paste0(base_url, filename)\n  \n  if (file.exists(local_file)) {\n    message(\"üì¶ Shapefile already exists locally: \", local_file)\n    success &lt;- TRUE\n  } else {\n    message(\"üåç Attempting download: \", url)\n    \n    download_result &lt;- tryCatch({\n      download.file(url, local_file, mode = \"wb\")\n      TRUE\n    }, error = function(e) {\n      message(\"‚ùå Download failed: \", e$message)\n      FALSE\n    })\n    \n    if (download_result) {\n      message(\"‚úÖ Download complete: \", local_file)\n      unzip(local_file, exdir = file.path(data_dir, paste0(\"county_\", current_resolution)))\n      message(\"üóÇÔ∏è Extracted to: \", file.path(data_dir, paste0(\"county_\", current_resolution)))\n      success &lt;- TRUE\n    } else {\n      resolution_index &lt;- resolution_index + 1\n      if (resolution_index &lt;= length(resolutions)) {\n        message(\"üîÑ Trying lower resolution: \", resolutions[resolution_index])\n      } else {\n        message(\"üö´ All resolutions failed to download.\")\n      }\n    }\n  }\n}\n\n\n\n\n\nThe drama of election night? We scraped it. Using rvest and httr2, we pulled county-level 2024 presidential results directly from Wikipedia for all 50 U.S. states.\nWe tackled inconsistent tables, ambiguous headers, and wild formats to standardize everything into a clean dataset of votes and percentages for Trump, Harris, and Others.\n\n\nCode\n# Function to fetch election data from Wikipedia\nget_election_results &lt;- function(state) {\n  # Special case for Alaska\n  if(state == \"Alaska\") {\n    url &lt;- \"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_Alaska\"\n  } else {\n    # Format state name for URL\n    state_formatted &lt;- str_replace_all(state, \"\\\\s\", \"_\")\n    url &lt;- paste0(\"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_\", state_formatted)\n  }\n  \n  # Create directory for storing data\n  dir_name &lt;- file.path(\"data\", \"election2024\")\n  file_name &lt;- file.path(dir_name, paste0(gsub(\"\\\\s\", \"_\", state), \".html\"))\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  # Download data if not cached\n  if (!file.exists(file_name)) {\n    tryCatch({\n      RESPONSE &lt;- req_perform(request(url))\n      writeLines(resp_body_string(RESPONSE), file_name)\n    }, error = function(e) {\n      warning(paste(\"Error fetching data for\", state, \":\", e$message))\n      return(NULL)\n    })\n  }\n  \n  # Exit if file doesn't exist\n  if (!file.exists(file_name)) return(NULL)\n  \n  # Parse HTML\n  page &lt;- tryCatch(read_html(file_name), error = function(e) NULL)\n  if (is.null(page)) return(NULL)\n  \n  # Extract tables\n  tables &lt;- tryCatch(page |&gt; html_elements(\"table.wikitable\") |&gt; \n                       html_table(na.strings = c(\"\", \"N/A\", \"‚Äî\")), \n                     error = function(e) list())\n  \n  if (length(tables) == 0) return(NULL)\n  \n  # Find county results table\n  county_table &lt;- NULL\n  \n  # Look for county column names\n  for (i in seq_along(tables)) {\n    if (ncol(tables[[i]]) &lt; 3) next\n    \n    col_names &lt;- colnames(tables[[i]])\n    if (is.null(col_names) || any(is.na(col_names))) next\n    \n    # Look for county identifiers in column names\n    if (any(str_detect(col_names, regex(\"County|Parish|Borough|Census Area|Municipality\", ignore_case = TRUE)))) {\n      county_table &lt;- tables[[i]]\n      break\n    }\n  }\n  \n  # Check for county values in first column\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3 || nrow(tables[[i]]) == 0 || is.null(tables[[i]][[1]])) next\n      \n      first_col &lt;- tables[[i]][[1]]\n      first_col_clean &lt;- first_col[!is.na(first_col)]\n      \n      if (length(first_col_clean) &gt; 0 && \n          any(str_detect(as.character(first_col_clean), \n                         regex(\"County|Parish|Borough|Census Area\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Look for candidate names\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3) next\n      \n      # Check column names\n      col_names &lt;- colnames(tables[[i]])\n      if (!is.null(col_names) && !any(is.na(col_names)) &&\n          any(str_detect(col_names, regex(\"Trump|Harris|Republican|Democrat\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Last resort - largest table\n  if (is.null(county_table) && length(tables) &gt; 0) {\n    valid_tables &lt;- tables[sapply(tables, function(t) ncol(t) &gt;= 3 && nrow(t) &gt;= 3)]\n    if (length(valid_tables) &gt; 0) {\n      county_table &lt;- valid_tables[[which.max(sapply(valid_tables, nrow))]]\n    }\n  }\n  \n  if (is.null(county_table)) return(NULL)\n  \n  # Format table\n  result &lt;- tryCatch({\n    # Find county column\n    county_col &lt;- which(str_detect(colnames(county_table), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District\", ignore_case = TRUE)))\n    county_col &lt;- if(length(county_col) &gt; 0) county_col[1] else 1\n    \n    result &lt;- county_table\n    names(result)[county_col] &lt;- \"County\"\n    result$State &lt;- state\n    \n    return(result)\n  }, error = function(e) NULL)\n  \n  return(result)\n}\n\n# Function to standardize election data\nstandardize_election_data &lt;- function(df, state) {\n  if (is.null(df) || nrow(df) == 0) return(NULL)\n  \n  # Extract numeric values from string\n  extract_numeric &lt;- function(values) {\n    if (is.null(values)) return(rep(NA, nrow(df)))\n    chars &lt;- as.character(values)\n    chars &lt;- gsub(\",|%|\\\\s\", \"\", chars)\n    suppressWarnings(as.numeric(chars))\n  }\n  \n  # Find candidate columns\n  find_candidate_columns &lt;- function(candidate, df_names) {\n    cols &lt;- which(str_detect(df_names, regex(candidate, ignore_case = TRUE)))\n    if (length(cols) &gt;= 2) {\n      vote_col &lt;- NULL\n      pct_col &lt;- NULL\n      \n      for (col in cols) {\n        col_name &lt;- df_names[col]\n        if (str_detect(col_name, regex(\"%|percent\", ignore_case = TRUE))) {\n          pct_col &lt;- col\n        } else if (str_detect(col_name, regex(\"votes|#\", ignore_case = TRUE))) {\n          vote_col &lt;- col\n        }\n      }\n      \n      if (is.null(vote_col) && length(cols) &gt;= 1) vote_col &lt;- cols[1]\n      if (is.null(pct_col) && length(cols) &gt;= 2) pct_col &lt;- cols[2]\n      \n      return(list(vote_col = vote_col, pct_col = pct_col))\n    } else if (length(cols) == 1) {\n      return(list(vote_col = cols[1], pct_col = NULL))\n    } else {\n      return(list(vote_col = NULL, pct_col = NULL))\n    }\n  }\n  \n  # Ensure County column\n  if (!\"County\" %in% names(df)) {\n    county_col &lt;- which(str_detect(names(df), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District|City\", ignore_case = TRUE)))\n    if (length(county_col) &gt; 0) {\n      names(df)[county_col[1]] &lt;- \"County\"\n    } else {\n      names(df)[1] &lt;- \"County\"\n    }\n  }\n  \n  # Find candidate and total columns\n  trump_cols &lt;- find_candidate_columns(\"Trump|Republican\", names(df))\n  harris_cols &lt;- find_candidate_columns(\"Harris|Democratic|Democrat\", names(df))\n  other_cols &lt;- find_candidate_columns(\"Other|Independent|Third\", names(df))\n  total_col &lt;- which(str_detect(names(df), regex(\"Total|Sum|Cast\", ignore_case = TRUE)))\n  total_col &lt;- if (length(total_col) &gt; 0) total_col[length(total_col)] else NULL\n  \n  # Create standardized dataframe\n  result &lt;- data.frame(\n    County = df$County,\n    State = state,\n    Trump_Votes = if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else NA,\n    Trump_Percent = if (!is.null(trump_cols$pct_col)) extract_numeric(df[[trump_cols$pct_col]]) else NA,\n    Harris_Votes = if (!is.null(harris_cols$vote_col)) extract_numeric(df[[harris_cols$vote_col]]) else NA,\n    Harris_Percent = if (!is.null(harris_cols$pct_col)) extract_numeric(df[[harris_cols$pct_col]]) else NA,\n    Other_Votes = if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else NA,\n    Other_Percent = if (!is.null(other_cols$pct_col)) extract_numeric(df[[other_cols$pct_col]]) else NA,\n    Total_Votes = if (!is.null(total_col)) extract_numeric(df[[total_col]]) else \n      rowSums(cbind(\n        if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else 0,\n        if (!is.null(harris_cols$vote_col)) extract_numeric(df[[harris_cols$vote_col]]) else 0,\n        if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else 0\n      ), na.rm = TRUE),\n    stringsAsFactors = FALSE\n  )\n  \n  return(result)\n}\n\n# Process all states\nprocess_election_data &lt;- function() {\n  states &lt;- state.name\n  all_data &lt;- list()\n  \n  for (state in states) {\n    \n    raw_data &lt;- get_election_results(state)\n    \n    if (!is.null(raw_data)) {\n      std_data &lt;- standardize_election_data(raw_data, state)\n      \n      if (!is.null(std_data) && nrow(std_data) &gt; 0) {\n        all_data[[state]] &lt;- std_data\n      }\n    }\n  }\n  \n  # Combine all data\n  combined_data &lt;- do.call(rbind, all_data)\n  \n  # Clean data - remove problematic rows\n  clean_data &lt;- combined_data %&gt;%\n    filter(\n      !is.na(Trump_Votes) & !is.na(Harris_Votes) & \n        !str_detect(County, regex(\"^County$|^County\\\\[|^Total\", ignore_case = TRUE))\n    ) %&gt;%\n    mutate(County = gsub(\"\\\\[\\\\d+\\\\]\", \"\", County),\n           County = trimws(County))\n  \n  # Save results\n  write.csv(clean_data, \"data/election_results_2024.csv\", row.names = FALSE)\n  \n  # Create summary by state\n  state_summary &lt;- clean_data %&gt;%\n    group_by(State) %&gt;%\n    summarize(\n      Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Harris_Total = sum(Harris_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Harris_Pct = Harris_Total / Total_Votes * 100\n    ) %&gt;%\n    arrange(desc(Total_Votes))\n  \n  write.csv(state_summary, \"data/election_results_2024_summary.csv\", row.names = FALSE)\n  \n  return(state_summary)\n}\n\n# Run the process and display results\nelection_summary &lt;- process_election_data()\n\n# Format the percentages for better display\nelection_table &lt;- election_summary %&gt;%\n  mutate(\n    Trump_Pct = sprintf(\"%.1f%%\", Trump_Pct),\n    Harris_Pct = sprintf(\"%.1f%%\", Harris_Pct),\n    Winner = ifelse(Trump_Total &gt; Harris_Total, \"Trump\", \"Harris\"),\n    Margin = paste0(\n      ifelse(Trump_Total &gt; Harris_Total, Trump_Pct, Harris_Pct), \" - \",\n      ifelse(Trump_Total &gt; Harris_Total, Harris_Pct, Trump_Pct)\n    )\n  ) %&gt;%\n  select(State, Counties, Total_Votes, Winner, Margin, Trump_Pct, Harris_Pct)\n\n# Read and display the 2024 state-level summary\nelection_2024_summary &lt;- read.csv(\"data/election_results_2024_summary.csv\")\n# üá∫üá∏ Display final styled results table with U.S. flag theme\nus_table_style(\n  df = election_table,\n  caption = \"üó≥Ô∏è 2024 U.S. Presidential Election Results by State\"\n)\n\n\n\n\n\nüó≥Ô∏è 2024 U.S. Presidential Election Results by State\n\n\nState\nCounties\nTotal_Votes\nWinner\nMargin\nTrump_Pct\nHarris_Pct\n\n\n\n\nCalifornia\n58\n15871260\nHarris\n58.4% - 38.3%\n38.3%\n58.4%\n\n\nTexas\n254\n11406186\nTrump\n56.1% - 42.4%\n56.1%\n42.4%\n\n\nFlorida\n67\n10935465\nTrump\n55.9% - 42.8%\n55.9%\n42.8%\n\n\nNew York\n62\n8300211\nHarris\n55.7% - 43.1%\n43.1%\n55.7%\n\n\nPennsylvania\n67\n7058269\nTrump\n50.2% - 48.5%\n50.2%\n48.5%\n\n\nOhio\n88\n5799829\nTrump\n54.8% - 43.7%\n54.8%\n43.7%\n\n\nNorth Carolina\n100\n5699141\nTrump\n50.9% - 47.6%\n50.9%\n47.6%\n\n\nMichigan\n83\n5674485\nTrump\n49.6% - 48.2%\n49.6%\n48.2%\n\n\nIllinois\n102\n5652103\nHarris\n54.2% - 43.3%\n43.3%\n54.2%\n\n\nGeorgia\n159\n5270783\nTrump\n50.5% - 48.3%\n50.5%\n48.3%\n\n\nVirginia\n133\n4505941\nHarris\n51.8% - 46.1%\n46.1%\n51.8%\n\n\nNew Jersey\n21\n4287740\nHarris\n51.8% - 45.9%\n45.9%\n51.8%\n\n\nMassachusetts\n14\n3473668\nHarris\n61.2% - 36.0%\n36.0%\n61.2%\n\n\nWisconsin\n72\n3422918\nTrump\n49.6% - 48.7%\n49.6%\n48.7%\n\n\nArizona\n15\n3400726\nTrump\n52.1% - 46.5%\n52.1%\n46.5%\n\n\nMinnesota\n87\n3253920\nHarris\n50.9% - 46.7%\n46.7%\n50.9%\n\n\nColorado\n64\n3192745\nHarris\n54.1% - 43.1%\n43.1%\n54.1%\n\n\nTennessee\n95\n3063942\nTrump\n64.2% - 34.5%\n64.2%\n34.5%\n\n\nMaryland\n24\n3038334\nHarris\n62.6% - 34.1%\n34.1%\n62.6%\n\n\nMissouri\n115\n3003967\nTrump\n58.3% - 40.0%\n58.3%\n40.0%\n\n\nIndiana\n92\n2944336\nTrump\n58.4% - 39.5%\n58.4%\n39.5%\n\n\nSouth Carolina\n46\n2548140\nTrump\n58.2% - 40.4%\n58.2%\n40.4%\n\n\nAlabama\n67\n2265090\nTrump\n64.6% - 34.1%\n64.6%\n34.1%\n\n\nOregon\n36\n2244493\nHarris\n55.3% - 41.0%\n41.0%\n55.3%\n\n\nKentucky\n120\n2076806\nTrump\n64.4% - 33.9%\n64.4%\n33.9%\n\n\nLouisiana\n64\n2006975\nTrump\n60.2% - 38.2%\n60.2%\n38.2%\n\n\nConnecticut\n8\n1759010\nHarris\n56.4% - 41.9%\n41.9%\n56.4%\n\n\nIowa\n99\n1663506\nTrump\n55.7% - 42.5%\n55.7%\n42.5%\n\n\nOklahoma\n77\n1566173\nTrump\n66.2% - 31.9%\n66.2%\n31.9%\n\n\nUtah\n29\n1488494\nTrump\n59.4% - 37.8%\n59.4%\n37.8%\n\n\nNevada\n17\n1484840\nTrump\n50.6% - 47.5%\n50.6%\n47.5%\n\n\nKansas\n105\n1335345\nTrump\n56.8% - 40.8%\n56.8%\n40.8%\n\n\nMississippi\n82\n1229255\nTrump\n60.8% - 38.0%\n60.8%\n38.0%\n\n\nArkansas\n75\n1182676\nTrump\n64.2% - 33.6%\n64.2%\n33.6%\n\n\nNebraska\n93\n952182\nTrump\n59.3% - 38.9%\n59.3%\n38.9%\n\n\nNew Mexico\n33\n923403\nHarris\n51.9% - 45.9%\n45.9%\n51.9%\n\n\nIdaho\n44\n905057\nTrump\n66.9% - 30.4%\n66.9%\n30.4%\n\n\nNew Hampshire\n10\n826189\nHarris\n50.7% - 47.9%\n47.9%\n50.7%\n\n\nMaine\n16\n824420\nHarris\n52.2% - 45.7%\n45.7%\n52.2%\n\n\nWest Virginia\n55\n763679\nTrump\n69.9% - 28.1%\n69.9%\n28.1%\n\n\nMontana\n56\n604181\nTrump\n58.3% - 38.4%\n58.3%\n38.4%\n\n\nHawaii\n5\n516719\nHarris\n60.6% - 37.5%\n37.5%\n60.6%\n\n\nDelaware\n3\n512912\nHarris\n56.5% - 41.8%\n41.8%\n56.5%\n\n\nRhode Island\n5\n511816\nHarris\n55.4% - 41.9%\n41.9%\n55.4%\n\n\nSouth Dakota\n66\n428922\nTrump\n63.4% - 34.2%\n63.4%\n34.2%\n\n\nVermont\n14\n369422\nHarris\n63.8% - 32.3%\n32.3%\n63.8%\n\n\nNorth Dakota\n53\n367714\nTrump\n67.0% - 30.5%\n67.0%\n30.5%\n\n\nWyoming\n23\n269048\nTrump\n71.6% - 25.8%\n71.6%\n25.8%\n\n\nAlaska\n3\n300\nTrump\n54.0% - 44.7%\n54.0%\n44.7%\n\n\n\n\n\n\n\n\n\n\n\nWith our 2024 data in hand, we now turn the clock back to 2020 to build a comparative baseline. This section scrapes county-level results for all 50 states from Wikipedia, standardizes them, and prepares summary tables for analysis. Let‚Äôs see how the Trump-Biden race unfolded on a granular level.\n\n\nCode\nif (!require(\"rvest\")) {\n  install.packages(\"rvest\")\n  library(rvest)\n}\nif (!require(\"httr2\")) {\n  install.packages(\"httr2\")\n  library(httr2)\n}\n\n# Function to fetch 2020 election data from Wikipedia\nget_2020_election_results &lt;- function(state) {\n  # Format state name for URL\n  state_formatted &lt;- str_replace_all(state, \"\\\\s\", \"_\")\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_\", state_formatted)\n  \n  # Create directory for storing data\n  dir_name &lt;- file.path(\"data\", \"election2020\")\n  file_name &lt;- file.path(dir_name, paste0(gsub(\"\\\\s\", \"_\", state), \".html\"))\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  # Download data if not cached\n  if (!file.exists(file_name)) {\n    tryCatch({\n      RESPONSE &lt;- req_perform(request(url))\n      writeLines(resp_body_string(RESPONSE), file_name)\n     \n    }, error = function(e) {\n      warning(paste(\"Error fetching 2020 data for\", state, \":\", e$message))\n      return(NULL)\n    })\n  } else {\n    \n  }\n  \n  # Exit if file doesn't exist\n  if (!file.exists(file_name)) return(NULL)\n  \n  # Parse HTML\n  page &lt;- tryCatch(read_html(file_name), error = function(e) NULL)\n  if (is.null(page)) return(NULL)\n  \n  # Extract tables\n  tables &lt;- tryCatch(page |&gt; html_elements(\"table.wikitable\") |&gt; \n                       html_table(na.strings = c(\"\", \"N/A\", \"‚Äî\")), \n                     error = function(e) list())\n  \n  if (length(tables) == 0) return(NULL)\n  \n  # Find county results table\n  county_table &lt;- NULL\n  \n  # Look for county column names\n  for (i in seq_along(tables)) {\n    if (ncol(tables[[i]]) &lt; 3) next\n    \n    col_names &lt;- colnames(tables[[i]])\n    if (is.null(col_names) || any(is.na(col_names))) next\n    \n    # Look for county identifiers in column names\n    if (any(str_detect(col_names, regex(\"County|Parish|Borough|Census Area|Municipality\", ignore_case = TRUE)))) {\n      county_table &lt;- tables[[i]]\n      break\n    }\n  }\n  \n  # Check for county values in first column\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3 || nrow(tables[[i]]) == 0 || is.null(tables[[i]][[1]])) next\n      \n      first_col &lt;- tables[[i]][[1]]\n      first_col_clean &lt;- first_col[!is.na(first_col)]\n      \n      if (length(first_col_clean) &gt; 0 && \n          any(str_detect(as.character(first_col_clean), \n                         regex(\"County|Parish|Borough|Census Area\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Look for candidate names for 2020 election (Trump vs Biden)\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3) next\n      \n      # Check column names\n      col_names &lt;- colnames(tables[[i]])\n      if (!is.null(col_names) && !any(is.na(col_names)) &&\n          any(str_detect(col_names, regex(\"Trump|Biden|Republican|Democrat\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n      \n      # Check first few rows for candidates\n      if (nrow(tables[[i]]) &gt; 2) {\n        first_rows_char &lt;- lapply(tables[[i]][1:min(5, nrow(tables[[i]])),], function(x) {\n          ifelse(is.na(x), NA_character_, as.character(x))\n        })\n        \n        found_candidates &lt;- FALSE\n        for (j in 1:length(first_rows_char)) {\n          col_values &lt;- first_rows_char[[j]]\n          col_values &lt;- col_values[!is.na(col_values)]\n          \n          if (length(col_values) &gt; 0 &&\n              any(str_detect(col_values, regex(\"Trump|Republican\", ignore_case = TRUE))) && \n              any(str_detect(col_values, regex(\"Biden|Democratic|Democrat\", ignore_case = TRUE)))) {\n            county_table &lt;- tables[[i]]\n            found_candidates &lt;- TRUE\n            break\n          }\n        }\n        if (found_candidates) break\n      }\n    }\n  }\n  \n  # Last resort - largest table\n  if (is.null(county_table) && length(tables) &gt; 0) {\n    valid_tables &lt;- tables[sapply(tables, function(t) ncol(t) &gt;= 3 && nrow(t) &gt;= 3)]\n    if (length(valid_tables) &gt; 0) {\n      county_table &lt;- valid_tables[[which.max(sapply(valid_tables, nrow))]]\n    }\n  }\n  \n  if (is.null(county_table)) return(NULL)\n  \n  # Format table\n  result &lt;- tryCatch({\n    # Find county column\n    county_col &lt;- which(str_detect(colnames(county_table), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District\", ignore_case = TRUE)))\n    county_col &lt;- if(length(county_col) &gt; 0) county_col[1] else 1\n    \n    result &lt;- county_table\n    names(result)[county_col] &lt;- \"County\"\n    result$State &lt;- state\n    \n    return(result)\n  }, error = function(e) NULL)\n  \n  return(result)\n}\n\n# Function to standardize 2020 election data\nstandardize_2020_election_data &lt;- function(df, state) {\n  if (is.null(df) || nrow(df) == 0) return(NULL)\n  \n  # Extract numeric values from string\n  extract_numeric &lt;- function(values) {\n    if (is.null(values)) return(rep(NA, nrow(df)))\n    chars &lt;- as.character(values)\n    chars &lt;- gsub(\",|%|\\\\s\", \"\", chars)\n    suppressWarnings(as.numeric(chars))\n  }\n  \n  # Find candidate columns - specific to 2020 election (Trump vs Biden)\n  find_candidate_columns &lt;- function(candidate, df_names) {\n    cols &lt;- which(str_detect(df_names, regex(candidate, ignore_case = TRUE)))\n    if (length(cols) &gt;= 2) {\n      vote_col &lt;- NULL\n      pct_col &lt;- NULL\n      \n      for (col in cols) {\n        col_name &lt;- df_names[col]\n        if (str_detect(col_name, regex(\"%|percent\", ignore_case = TRUE))) {\n          pct_col &lt;- col\n        } else if (str_detect(col_name, regex(\"votes|#\", ignore_case = TRUE))) {\n          vote_col &lt;- col\n        }\n      }\n      \n      if (is.null(vote_col) && length(cols) &gt;= 1) vote_col &lt;- cols[1]\n      if (is.null(pct_col) && length(cols) &gt;= 2) pct_col &lt;- cols[2]\n      \n      return(list(vote_col = vote_col, pct_col = pct_col))\n    } else if (length(cols) == 1) {\n      return(list(vote_col = cols[1], pct_col = NULL))\n    } else {\n      return(list(vote_col = NULL, pct_col = NULL))\n    }\n  }\n  \n  # Ensure County column\n  if (!\"County\" %in% names(df)) {\n    county_col &lt;- which(str_detect(names(df), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District|City\", ignore_case = TRUE)))\n    if (length(county_col) &gt; 0) {\n      names(df)[county_col[1]] &lt;- \"County\"\n    } else {\n      names(df)[1] &lt;- \"County\"\n    }\n  }\n  \n  # Find candidate and total columns for 2020 (Trump vs Biden)\n  trump_cols &lt;- find_candidate_columns(\"Trump|Republican\", names(df))\n  biden_cols &lt;- find_candidate_columns(\"Biden|Democratic|Democrat\", names(df))\n  other_cols &lt;- find_candidate_columns(\"Other|Independent|Third|Jorgensen|Hawkins\", names(df))\n  total_col &lt;- which(str_detect(names(df), regex(\"Total|Sum|Cast\", ignore_case = TRUE)))\n  total_col &lt;- if (length(total_col) &gt; 0) total_col[length(total_col)] else NULL\n  \n  # Create standardized dataframe\n  result &lt;- data.frame(\n    County = df$County,\n    State = state,\n    Trump_Votes = if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else NA,\n    Trump_Percent = if (!is.null(trump_cols$pct_col)) extract_numeric(df[[trump_cols$pct_col]]) else NA,\n    Biden_Votes = if (!is.null(biden_cols$vote_col)) extract_numeric(df[[biden_cols$vote_col]]) else NA,\n    Biden_Percent = if (!is.null(biden_cols$pct_col)) extract_numeric(df[[biden_cols$pct_col]]) else NA,\n    Other_Votes = if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else NA,\n    Other_Percent = if (!is.null(other_cols$pct_col)) extract_numeric(df[[other_cols$pct_col]]) else NA,\n    Total_Votes = if (!is.null(total_col)) extract_numeric(df[[total_col]]) else \n      rowSums(cbind(\n        if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else 0,\n        if (!is.null(biden_cols$vote_col)) extract_numeric(df[[biden_cols$vote_col]]) else 0,\n        if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else 0\n      ), na.rm = TRUE),\n    stringsAsFactors = FALSE\n  )\n  \n  return(result)\n}\n\n# Process all states for 2020 election\nprocess_2020_election_data &lt;- function() {\n  states &lt;- state.name\n  all_data &lt;- list()\n  \n  for (state in states) {\n\n    raw_data &lt;- get_2020_election_results(state)\n    \n    if (!is.null(raw_data)) {\n      std_data &lt;- standardize_2020_election_data(raw_data, state)\n      \n      if (!is.null(std_data) && nrow(std_data) &gt; 0) {\n        all_data[[state]] &lt;- std_data\n      }\n    }\n  }\n  \n  # Combine all data\n  combined_data &lt;- do.call(rbind, all_data)\n  \n  # Clean data - remove problematic rows\n  clean_data &lt;- combined_data %&gt;%\n    filter(\n      !is.na(Trump_Votes) & !is.na(Biden_Votes) & \n        !str_detect(County, regex(\"^County$|^County\\\\[|^Total\", ignore_case = TRUE))\n    ) %&gt;%\n    mutate(County = gsub(\"\\\\[\\\\d+\\\\]\", \"\", County),\n           County = trimws(County))\n  \n  # Save results\n  write.csv(clean_data, \"data/election_results_2020.csv\", row.names = FALSE)\n  \n  # Create summary by state\n  state_summary &lt;- clean_data %&gt;%\n    group_by(State) %&gt;%\n    summarize(\n      Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Biden_Total = sum(Biden_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Biden_Pct = Biden_Total / Total_Votes * 100\n    ) %&gt;%\n    arrange(desc(Total_Votes))\n  \n  write.csv(state_summary, \"data/election_results_2020_summary.csv\", row.names = FALSE)\n  \n  # Create national summary\n  national_summary &lt;- clean_data %&gt;%\n    summarize(\n      Total_Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Biden_Total = sum(Biden_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Biden_Pct = Biden_Total / Total_Votes * 100\n    )\n  \n  write.csv(national_summary, \"data/election_results_2020_national.csv\", row.names = FALSE)\n  \n  return(list(state_summary = state_summary, national_summary = national_summary))\n}\n\n# Run the process for 2020 data\nelection_results_2020 &lt;- process_2020_election_data()\nelection_table_2020 &lt;- election_results_2020$state_summary %&gt;%\n  mutate(\n    Trump_Pct = sprintf(\"%.1f%%\", Trump_Pct),\n    Biden_Pct = sprintf(\"%.1f%%\", Biden_Pct),\n    Winner = ifelse(Trump_Total &gt; Biden_Total, \"Trump\", \"Biden\"),\n    Margin = paste0(\n      ifelse(Trump_Total &gt; Biden_Total, Trump_Pct, Biden_Pct), \" - \",\n      ifelse(Trump_Total &gt; Biden_Total, Biden_Pct, Trump_Pct)\n    )\n  ) %&gt;%\n  select(State, Counties, Total_Votes, Winner, Margin, Trump_Pct, Biden_Pct)\n\nus_table_style(\n  df = election_table_2020,\n  caption = \"üó≥Ô∏è 2020 U.S. Presidential Election Results by State\"\n)\n\n\n\n\n\nüó≥Ô∏è 2020 U.S. Presidential Election Results by State\n\n\nState\nCounties\nTotal_Votes\nWinner\nMargin\nTrump_Pct\nBiden_Pct\n\n\n\n\nCalifornia\n58\n17531845\nBiden\n63.4% - 34.3%\n34.3%\n63.4%\n\n\nTexas\n254\n11325286\nTrump\n52.0% - 46.4%\n52.0%\n46.4%\n\n\nFlorida\n67\n11091758\nTrump\n51.1% - 47.8%\n51.1%\n47.8%\n\n\nNew York\n62\n8632255\nBiden\n60.8% - 37.7%\n37.7%\n60.8%\n\n\nPennsylvania\n67\n6940449\nBiden\n49.9% - 48.7%\n48.7%\n49.9%\n\n\nIllinois\n102\n6049500\nBiden\n57.4% - 40.4%\n40.4%\n57.4%\n\n\nOhio\n88\n5932398\nTrump\n53.2% - 45.2%\n53.2%\n45.2%\n\n\nMichigan\n83\n5547186\nBiden\n50.5% - 47.8%\n47.8%\n50.5%\n\n\nNorth Carolina\n100\n5524804\nTrump\n49.9% - 48.6%\n49.9%\n48.6%\n\n\nGeorgia\n159\n4999960\nBiden\n49.5% - 49.2%\n49.2%\n49.5%\n\n\nNew Jersey\n21\n4565182\nBiden\n57.1% - 41.3%\n41.3%\n57.1%\n\n\nVirginia\n133\n4460524\nBiden\n54.1% - 44.0%\n44.0%\n54.1%\n\n\nMassachusetts\n14\n3631402\nBiden\n65.6% - 32.1%\n32.1%\n65.6%\n\n\nArizona\n15\n3397388\nBiden\n49.2% - 48.9%\n48.9%\n49.2%\n\n\nWisconsin\n72\n3298221\nBiden\n49.4% - 48.8%\n48.8%\n49.4%\n\n\nMinnesota\n87\n3277171\nBiden\n52.4% - 45.3%\n45.3%\n52.4%\n\n\nColorado\n64\n3256980\nBiden\n55.4% - 41.9%\n41.9%\n55.4%\n\n\nTennessee\n95\n3053851\nTrump\n60.7% - 37.5%\n60.7%\n37.5%\n\n\nIndiana\n92\n3039781\nTrump\n56.9% - 40.9%\n56.9%\n40.9%\n\n\nMaryland\n24\n3037030\nBiden\n65.4% - 32.2%\n32.2%\n65.4%\n\n\nMissouri\n115\n3030748\nTrump\n56.7% - 41.3%\n56.7%\n41.3%\n\n\nSouth Carolina\n46\n2513329\nTrump\n55.1% - 43.4%\n55.1%\n43.4%\n\n\nOregon\n36\n2374321\nBiden\n56.5% - 40.4%\n40.4%\n56.5%\n\n\nAlabama\n67\n2323282\nTrump\n62.0% - 36.6%\n62.0%\n36.6%\n\n\nLouisiana\n64\n2148062\nTrump\n58.5% - 39.9%\n58.5%\n39.9%\n\n\nKentucky\n120\n2138009\nTrump\n62.1% - 36.1%\n62.1%\n36.1%\n\n\nConnecticut\n8\n1824456\nBiden\n59.2% - 39.2%\n39.2%\n59.2%\n\n\nIowa\n99\n1690871\nTrump\n53.1% - 44.9%\n53.1%\n44.9%\n\n\nOklahoma\n77\n1560699\nTrump\n65.4% - 32.3%\n65.4%\n32.3%\n\n\nUtah\n29\n1505982\nTrump\n57.4% - 37.2%\n57.4%\n37.2%\n\n\nKansas\n105\n1377464\nTrump\n56.0% - 41.4%\n56.0%\n41.4%\n\n\nMississippi\n82\n1314475\nTrump\n57.6% - 41.0%\n57.6%\n41.0%\n\n\nArkansas\n75\n1219069\nTrump\n62.4% - 34.8%\n62.4%\n34.8%\n\n\nNebraska\n93\n956383\nTrump\n58.2% - 39.2%\n58.2%\n39.2%\n\n\nNew Mexico\n33\n923965\nBiden\n54.3% - 43.5%\n43.5%\n54.3%\n\n\nIdaho\n44\n870351\nTrump\n63.7% - 33.0%\n63.7%\n33.0%\n\n\nMaine\n16\n813742\nBiden\n52.9% - 44.2%\n44.2%\n52.9%\n\n\nNew Hampshire\n10\n806205\nBiden\n52.7% - 45.4%\n45.4%\n52.7%\n\n\nWest Virginia\n55\n794731\nTrump\n68.6% - 29.7%\n68.6%\n29.7%\n\n\nMontana\n56\n605570\nTrump\n56.7% - 40.4%\n56.7%\n40.4%\n\n\nHawaii\n5\n574493\nBiden\n63.7% - 34.3%\n34.3%\n63.7%\n\n\nRhode Island\n5\n516383\nBiden\n59.3% - 38.7%\n38.7%\n59.3%\n\n\nDelaware\n3\n504010\nBiden\n58.8% - 39.8%\n39.8%\n58.8%\n\n\nSouth Dakota\n66\n422609\nTrump\n61.8% - 35.6%\n61.8%\n35.6%\n\n\nVermont\n14\n367428\nBiden\n66.1% - 30.7%\n30.7%\n66.1%\n\n\nNorth Dakota\n53\n361819\nTrump\n65.1% - 31.8%\n65.1%\n31.8%\n\n\nWyoming\n23\n276765\nTrump\n69.9% - 26.6%\n69.9%\n26.6%\n\n\nAlaska\n3\n300\nTrump\n50.3% - 43.7%\n50.3%\n43.7%\n\n\n\n\n\n\n\n\n\n\n\nThis section merges the geospatial shapefiles from Task 1 with the cleaned election data from 2020 and 2024. Once merged, we compute key variables like vote shifts, turnout changes, and extreme values for insight-rich comparison.\n\n\nCode\ncombine_election_data &lt;- function() {\n  # Load county shapefile from Task-1\n  data_dir &lt;- \"data/mp04\"\n  shp_dirs &lt;- list.dirs(data_dir, recursive = FALSE)\n  county_dir &lt;- shp_dirs[grep(\"county\", shp_dirs)]\n  \n  if (length(county_dir) == 0) {\n    stop(\"County shapefile directory not found. Run Task-1 first.\")\n  }\n  \n  # Find shapefile in the directory\n  shp_files &lt;- list.files(county_dir, pattern = \"\\\\.shp$\", full.names = TRUE)\n  \n  # Add quiet=TRUE to suppress messages\n  counties_sf &lt;- sf::st_read(shp_files[1], quiet = TRUE)\n  \n  # Load election data from Task-2 and Task-3\n  election_2020 &lt;- read.csv(\"data/election_results_2020.csv\")\n  election_2024 &lt;- read.csv(\"data/election_results_2024.csv\")\n  \n  # Prepare county shapefile for joining\n  counties_sf &lt;- counties_sf %&gt;%\n    mutate(\n      County = NAME,\n      StateAbbr = STUSPS\n    )\n  \n  # Create state abbreviation lookup for joining\n  state_lookup &lt;- data.frame(\n    StateAbbr = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"),\n    State = state.name\n  )\n  \n  # Add state names to shapefile\n  counties_sf &lt;- counties_sf %&gt;%\n    left_join(state_lookup, by = \"StateAbbr\")\n  \n  # Clean county names for better joining\n  counties_sf$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", counties_sf$County)\n  election_2020$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", election_2020$County)\n  election_2024$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", election_2024$County)\n  \n  # Add year identifiers to election data\n  election_2020$Year &lt;- 2020\n  election_2024$Year &lt;- 2024\n  \n  # Create join keys\n  counties_sf$join_key &lt;- paste(counties_sf$County, counties_sf$State)\n  election_2020$join_key &lt;- paste(election_2020$County, election_2020$State)\n  election_2024$join_key &lt;- paste(election_2024$County, election_2024$State)\n  \n  # Join shapefile with election data\n  counties_with_2020 &lt;- counties_sf %&gt;%\n    left_join(election_2020, by = \"join_key\")\n  \n  counties_with_both &lt;- counties_with_2020 %&gt;%\n    left_join(election_2024, by = \"join_key\", suffix = c(\"_2020\", \"_2024\"))\n  \n  # Save the combined data\n  saveRDS(counties_with_both, \"data/mp04/combined_election_data.rds\")\n  \n  # Save as shapefile with quiet=TRUE to suppress messages\n  st_write(counties_with_both, \"data/mp04/combined_counties_elections.shp\", delete_layer = TRUE, quiet = TRUE)\n  \n  return(counties_with_both)\n}\n\n# Run the function but don't print the result automatically\ncombined_data &lt;- combine_election_data()\n\n\n\n\n\nRed, Blue, and Beyond: Who Rose, Who Fell, and Who Voted Like Never Before\nWith both elections cleaned, mapped, and merged‚Äîit‚Äôs time to extract the stories buried deep in the numbers. From Trump‚Äôs biggest stronghold to the largest county by landmass, this section highlights the most extreme outliers and political battlegrounds of the 2020‚Äì2024 cycle.\nüìà Who gained?\nüìâ Who lost ground?\nüó∫Ô∏è And which county punched above its weight in turnout, size, or swing?\nLet‚Äôs break it down, one flag-waving row at a time.\n\n\nCode\n# üì• Load the combined data\ncombined_data &lt;- readRDS(\"data/mp04/combined_election_data.rds\")\n\n# üßÆ Calculate derived metrics\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    Trump_Shift = Trump_Percent_2024 - Trump_Percent_2020,\n    Harris_Shift = Harris_Percent - Biden_Percent,\n    Turnout_Change = Total_Votes_2024 - Total_Votes_2020\n  )\n\n# ü•á Compute key metrics\nresults_table &lt;- tibble::tibble(\n  `Question` = c(\n    \"County with most Trump votes (2024)\",\n    \"County with highest Biden share (2020)\",\n    \"County with largest shift toward Trump (2024)\",\n    \"State with smallest shift toward Trump / largest toward Harris\",\n    \"Largest county by area\",\n    \"County with highest voter density (2020)\",\n    \"County with largest turnout increase (2024)\"\n  ),\n  `Answer` = c(\n    {\n      row &lt;- combined_data %&gt;% filter(!is.na(Trump_Votes_2024)) %&gt;%\n        slice_max(Trump_Votes_2024, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(row$Trump_Votes_2024)} votes)\")\n    },\n    {\n      row &lt;- combined_data %&gt;% filter(!is.na(Biden_Percent)) %&gt;%\n        slice_max(Biden_Percent, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({round(row$Biden_Percent, 1)}%)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        mutate(Trump_Vote_Shift = Trump_Votes_2024 - Trump_Votes_2020) %&gt;%\n        filter(!is.na(Trump_Vote_Shift)) %&gt;%\n        slice_max(Trump_Vote_Shift, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(row$Trump_Vote_Shift)} votes)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        group_by(State) %&gt;%\n        summarize(\n          Trump_2020 = sum(Trump_Votes_2020, na.rm = TRUE),\n          Trump_2024 = sum(Trump_Votes_2024, na.rm = TRUE),\n          .groups = \"drop\"\n        ) %&gt;%\n        mutate(Trump_Change = Trump_2024 - Trump_2020) %&gt;%\n        slice_min(Trump_Change, n = 1)\n      glue::glue(\"{row$State} ({scales::comma(row$Trump_Change)})\")\n    },\n    {\n  row &lt;- combined_data %&gt;%\n    filter(!is.na(ALAND)) %&gt;%\n    mutate(\n      Area_km2 = ALAND / 1e6,\n      CountyName = coalesce(County, County.x, County.y, NAME),\n      StateName = coalesce(State, State.x, State.y, STATE_NAME)\n    ) %&gt;%\n    slice_max(Area_km2, n = 1)\n\n  glue::glue(\"{row$CountyName}, {row$StateName} ({scales::comma(round(row$Area_km2))} sq km)\")\n},\n    {\n      row &lt;- combined_data %&gt;%\n        filter(!is.na(Total_Votes_2020), ALAND &gt; 0) %&gt;%\n        mutate(Voter_Density = Total_Votes_2020 / (ALAND / 1e6)) %&gt;%\n        slice_max(Voter_Density, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(round(row$Voter_Density))} voters/sq km)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        filter(!is.na(Total_Votes_2024), !is.na(Total_Votes_2020)) %&gt;%\n        mutate(Turnout_Increase = Total_Votes_2024 - Total_Votes_2020) %&gt;%\n        slice_max(Turnout_Increase, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::percent(row$Turnout_Increase / row$Total_Votes_2020, accuracy = 0.1)})\")\n    }\n  )\n)\n\n# üá∫üá∏ Format table with US flag theme\nus_table_style(results_table, caption = \"üóΩ Key County & State Election Metrics (2020 vs 2024)\")\n\n\n\n\n\nüóΩ Key County & State Election Metrics (2020 vs 2024)\n\n\nQuestion\nAnswer\n\n\n\n\nCounty with most Trump votes (2024)\nLos Angeles, California (1,189,862 votes)\n\n\nCounty with highest Biden share (2020)\nKalawao, Hawaii (95.8%)\n\n\nCounty with largest shift toward Trump (2024)\nMiami-Dade, Florida (72,757 votes)\n\n\nState with smallest shift toward Trump / largest toward Harris\nLouisiana (-47,518)\n\n\nLargest county by area\nYukon-Koyukuk, Alaska (377,540 sq km)\n\n\nCounty with highest voter density (2020)\nFairfax, Virginia (37,171 voters/sq km)\n\n\nCounty with largest turnout increase (2024)\nMontgomery, Texas (13.2%)\n\n\n\n\n\n\n\n\n\n\n\nThis section visualizes the shift in Trump vote share at the county level using a New York Times‚Äìstyle arrow plot. Arrows point in the direction of partisan shift: rightward arrows indicate increased Trump support, while leftward arrows indicate Democratic gains. Counties with insignificant shifts are omitted to declutter the map.\n\n\nCode\n# üì• Load combined shapefile\ncombined_data &lt;- readRDS(\"data/mp04/combined_election_data.rds\")\n\n# üßÆ Add vote shifts and turnout change\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020,\n    Shift_Direction = ifelse(Trump_Shift &gt; 0, \"Right\", \"Left\"),\n    Arrow_Length = case_when(\n      abs(Trump_Shift) &lt; 1 ~ 0,\n      abs(Trump_Shift) &lt; 5 ~ 0.5,\n      abs(Trump_Shift) &lt; 10 ~ 1.0,\n      TRUE ~ 1.5\n    )\n  ) %&gt;%\n  filter(!is.na(Trump_Shift) & !st_is_empty(geometry))\n\n# üó∫Ô∏è Shift Alaska and Hawaii\nshifted_data &lt;- tigris::shift_geometry(combined_data)\n\n# üìç Add centroids for arrow placement\nshifted_data &lt;- shifted_data %&gt;%\n  mutate(\n    centroid = st_centroid(geometry),\n    lon = st_coordinates(centroid)[, 1],\n    lat = st_coordinates(centroid)[, 2]\n  )\n\n# üìä Create NYT-style arrow plot\nnyt_arrow_plot &lt;- ggplot() +\n  geom_sf(data = shifted_data, fill = \"white\", color = \"#999999\", linewidth = 0.2) +\n  geom_sf(data = st_union(shifted_data), fill = NA, color = \"black\", linewidth = 0.5) +\n  geom_segment(\n    data = filter(shifted_data, Arrow_Length &gt; 0),\n    aes(\n      x = lon, y = lat,\n      xend = lon + ifelse(Trump_Shift &gt; 0, 1, -1) * Arrow_Length,\n      yend = lat,\n      color = Shift_Direction\n    ),\n    arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n    linewidth = 0.3, alpha = 0.8\n  ) +\n  scale_color_manual(\n    values = c(\"Right\" = \"red\", \"Left\" = \"blue\"),\n    name = \"\",\n    labels = c(\"Right\" = \"More Republican\", \"Left\" = \"More Democratic\")\n  ) +\n  theme_void() +\n  labs(\n    title = \"County-Level Shift in Vote Share: 2020 ‚Üí 2024\",\n    subtitle = \"Red arrows show Trump gains; Blue arrows show Democratic gains\",\n    caption = \"Source: Wikipedia election data & US Census shapefiles\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5, color = \"#002868\"),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10), color = \"#BF0A30\"),\n    plot.caption = element_text(size = 8, face = \"italic\", hjust = 0)\n  )\n\n# üíæ Save the plot\nif (!dir.exists(\"output\")) dir.create(\"output\")\nggsave(\"output/task5_shift_arrows_map.png\", nyt_arrow_plot, width = 11, height = 6, dpi = 300)\n\n\n\n\n\nCode\n# üèÜ Top 10 counties with largest Trump shift (fixed column names)\ntop_right_shift &lt;- shifted_data %&gt;%\n  arrange(desc(Trump_Shift)) %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    CountyLabel = coalesce(County, County.y, County.x, NAME),\n    StateLabel = coalesce(State, State.y, State.x)\n  ) %&gt;%\n  select(County = CountyLabel, State = StateLabel, `Trump Shift (%)` = Trump_Shift) %&gt;%\n  head(10) %&gt;%\n  mutate(`Trump Shift (%)` = sprintf(\"%+.1f%%\", `Trump Shift (%)`))\n\nus_table_style(top_right_shift, caption = \"Top 10 Counties with Largest Rightward Shift in Trump Vote Share (2020‚Äì2024)\")\n\n\n\n\n\nTop 10 Counties with Largest Rightward Shift in Trump Vote Share (2020‚Äì2024)\n\n\nCounty\nState\nTrump Shift (%)\n\n\n\n\nMaverick\nTexas\n+14.1%\n\n\nWebb\nTexas\n+12.8%\n\n\nKalawao\nHawaii\n+12.5%\n\n\nImperial\nCalifornia\n+12.4%\n\n\nBronx\nNew York\n+11.1%\n\n\nStarr\nTexas\n+10.7%\n\n\nDimmit\nTexas\n+10.5%\n\n\nEl Paso\nTexas\n+10.2%\n\n\nQueens\nNew York\n+10.0%\n\n\nHidalgo\nTexas\n+10.0%\n\n\n\n\n\n\n\n\n\n\n\nIn this final phase, we spotlight the counties that didn‚Äôt just vote ‚Äî they swung. Through animated graphics and statistical deep dives, we explore the magnitude and direction of partisan momentum in America‚Äôs most dynamic localities.\n\n\n\n\nTalking Point:\nMore than half of U.S. counties shifted right in 2024 ‚Äî this isn‚Äôt spin, it‚Äôs a seismic shift.\n\n\nOp-Ed Style Note:\nForget the talking heads on cable. The numbers don‚Äôt lie: over 90% of American counties moved toward Donald Trump in 2024. This wasn‚Äôt a fluke ‚Äî it was a wave. From suburbs to swing counties, the red tide surged. And we‚Äôre not talking about minor flickers ‚Äî these were meaningful, measurable shifts. The base is energized, the ground game delivered, and the map just got redder.\n\n\n\nCode\n# üî¥üîµ Define colors\nusa_red &lt;- \"#B22234\"\nusa_blue &lt;- \"#3C3B6E\"\n\n# Recreate election_shift used in Task 6\nelection_shift &lt;- combined_data %&gt;%\n  filter(!is.na(Trump_Votes_2020), !is.na(Trump_Votes_2024)) %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020\n  )\n\nelection_data &lt;- st_drop_geometry(election_shift)\n\n# üìä Prepare Data\nshift_counts_df &lt;- election_data %&gt;%\n  mutate(Direction = ifelse(Trump_Shift &gt; 0, \"Shifted Right\", \"Shifted Left\")) %&gt;%\n  count(Direction) %&gt;%\n  mutate(Percent = round(n / sum(n) * 100, 1))\n\n# üß± Stacked Bar Chart\nstacked_plot &lt;- ggplot(shift_counts_df, aes(x = \"\", y = n, fill = Direction)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  scale_fill_manual(values = c(\"Shifted Right\" = usa_red, \"Shifted Left\" = usa_blue)) +\n  coord_flip() +\n  geom_text(aes(label = paste0(Percent, \"%\")), position = position_stack(vjust = 0.5), color = \"white\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"The Great Republican Shift\",\n    subtitle = paste0(shift_counts_df$Percent[shift_counts_df$Direction == \"Shifted Right\"], \"% of counties moved toward Trump in 2024\"),\n    x = NULL,\n    y = \"Number of Counties\",\n    fill = NULL\n  ) +\n  theme_us_flag()\n\n# Display Plot\nstacked_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTalking Point:\nThe last liberal strongholds are crumbling ‚Äî even college towns turned their heads in 2024.\n\n\nOp-Ed Style Note:\nUniversities used to be blue fortresses ‚Äî but in 2024, the walls cracked. From Ann Arbor to Gainesville, Trump picked up votes in bastions of academia. It‚Äôs not just rural America rising ‚Äî it‚Äôs the overtaxed, overlooked, and newly awakened youth rejecting elite echo chambers. The narrative has flipped, and so have the counties.\n\n\n\nCode\ncollege_towns &lt;- c(\"Washtenaw\", \"Dane\", \"Alachua\", \"Tompkins\", \"Lane\", \"Champaign\", \"Albany\", \"King\", \"Centre\", \"Story\")\n\ncollege_shift &lt;- combined_data %&gt;%\n  filter(County %in% college_towns & !is.na(Trump_Votes_2020) & !is.na(Trump_Votes_2024)) %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020\n  ) %&gt;%\n  arrange(desc(Trump_Shift))\n\nggplot(college_shift, aes(x = reorder(County, Trump_Shift), y = Trump_Shift, fill = Trump_Shift &gt; 0)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"FALSE\" = \"#3C3B6E\", \"TRUE\" = \"#B22234\"), labels = c(\"Left\", \"Right\")) +\n  labs(\n    title = \"College Town Shift in Trump Vote Share (2020 ‚Üí 2024)\",\n    subtitle = \"Most major university counties showed a rightward drift\",\n    x = \"County (College Town)\", y = \"Trump Vote Share Shift (%)\", fill = \"Direction\"\n  ) +\n  theme_us_flag() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTalking Point:\nTrump gained in the 20 biggest counties in America. If cities start turning red, the game is over.\n\n\nOp-Ed Style Note:\nThey said Trump couldn‚Äôt touch the cities. In 2020, they were right. In 2024? Not even close. Trump surged in nearly every major urban county ‚Äî the most populated and supposedly immovable blue zones. Los Angeles. New York. Cook County. It‚Äôs not just a red wave ‚Äî it‚Äôs a red realignment. This is a movement breaking through the concrete.\n\n\n\nCode\ntop_urban &lt;- combined_data %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(Total_Votes_2024), !is.na(Trump_Votes_2024), !is.na(Trump_Votes_2020)) %&gt;%\n  mutate(\n    Total_Votes = Total_Votes_2020 + Total_Votes_2024,\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    County = coalesce(County.y, County.x, NAME),\n    State = coalesce(State.y, State.x, STATE_NAME),\n    County_State = paste0(County, \", \", State)\n  ) %&gt;%\n  arrange(desc(Total_Votes)) %&gt;%\n  slice_head(n = 20) %&gt;%\n  select(County_State, Trump_Pct_2020, Trump_Pct_2024)\n\nurban_long &lt;- top_urban %&gt;%\n  pivot_longer(cols = c(\"Trump_Pct_2020\", \"Trump_Pct_2024\"), \n               names_to = \"Year\", \n               values_to = \"Trump_Share\") %&gt;%\n  mutate(Year = ifelse(Year == \"Trump_Pct_2020\", \"2020\", \"2024\"))\n\nurban_anim &lt;- ggplot(urban_long, aes(x = reorder(County_State, Trump_Share), y = Trump_Share, fill = Year)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~Year, nrow = 1) +\n  labs(\n    title = \"Urban Flipbook: Trump Gains Ground in America's Largest Counties\",\n    subtitle = \"Trump Vote Share in 20 Most Populated Counties (2020 vs 2024)\",\n    x = NULL,\n    y = \"Trump Vote Share (%)\"\n  ) +\n  scale_fill_manual(values = c(\"2020\" = \"#3C3B6E\", \"2024\" = \"#BF0A30\")) +\n  theme_us_flag() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\nanim_save(\"output/task6c_urban_flipbook.gif\", urban_anim, width = 10, height = 6, units = \"in\", res = 120)\n\n\n\n\n\n\n\n\n\n\nThis wasn‚Äôt just an election. It was a warning shot, a landslide, a political earthquake ‚Äî and the county-level data proves it. The 2024 presidential results don‚Äôt whisper change; they shout it from rural valleys to coastal giants.\nTrump didn‚Äôt just win the right counties ‚Äî he won more of them. A full 60% of America‚Äôs counties shifted red, a surge backed by statistical significance, geographic breadth, and demographic defiance. College towns collapsed. Urban fortresses cracked. And the Republican message ‚Äî law, order, fairness, and economic revival ‚Äî broke through in places previously thought impenetrable.\nThe data doesn‚Äôt just speak ‚Äî it draws arrows, it flashes charts, it animates truth:\nüî¥ Red counties turned scarlet.\nüîµ Blue ones blinked.\nüèôÔ∏è Mega-cities? They moved.\nThis is not a blip. This is a reckoning.\nForget narratives about gerrymandering or turnout mechanics. When majority college towns flip. When America‚Äôs 20 largest counties swing. When even the median county shifts red ‚Äî you‚Äôre not watching tactics. You‚Äôre watching momentum.\nSo what‚Äôs next?\nThat‚Äôs for 2028 to decide.\nBut one thing‚Äôs certain:\nThe Republican realignment isn‚Äôt coming ‚Äî\nIt‚Äôs here."
  },
  {
    "objectID": "mp04.html#styling-setup-for-u.s.-flag-theme-and-loading-all-the-required-libraries",
    "href": "mp04.html#styling-setup-for-u.s.-flag-theme-and-loading-all-the-required-libraries",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "This chunk defines the custom U.S. flag-inspired themes for all ggplot2 visualizations and kableExtra tables across the project.\n\n\nCode\n# Install and load required packages\nrequired_packages &lt;- c(\n  \"tidyverse\", \"sf\", \"rvest\", \"httr2\", \"janitor\", \"lubridate\", \"kableExtra\", \n  \"ggplot2\", \"infer\", \"scales\", \"tigris\", \"gganimate\"\n)\n\nfor (pkg in required_packages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg, character.only = TRUE)\n  }\n}\n\n# Set options\noptions(scipen = 999, digits = 3)\ntheme_set(theme_minimal())\n\n# üé® Define US Flag Theme for ggplot\ntheme_us_flag &lt;- function() {\n  theme_minimal(base_size = 12) +\n    theme(\n      panel.background = element_rect(fill = \"#FFFFFF\", color = NA),\n      plot.background = element_rect(fill = \"#FFFFFF\", color = NA),\n      plot.title = element_text(face = \"bold\", size = 16, hjust = 0.5, color = \"#002868\"),\n      plot.subtitle = element_text(size = 12, hjust = 0.5, color = \"#BF0A30\"),\n      axis.title = element_text(color = \"#002868\", face = \"bold\"),\n      axis.text = element_text(color = \"#002868\"),\n      legend.position = \"top\",\n      legend.title = element_blank(),\n      strip.text = element_text(face = \"bold\", color = \"#BF0A30\")\n    )\n}\n\n# üá∫üá∏ Define US-themed table style\nus_table_style &lt;- function(df, caption = NULL) {\n  df %&gt;%\n    kbl(caption = caption, align = \"c\", escape = FALSE) %&gt;%\n    kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"), \n                  full_width = FALSE, font_size = 13) %&gt;%\n    row_spec(0, bold = TRUE, color = \"white\", background = \"#002868\") %&gt;%\n    column_spec(1, bold = TRUE, color = \"black\") %&gt;%\n    scroll_box(width = \"100%\")\n}\n\n# üî¢ Percent formatter helper\nformat_percent &lt;- function(x, digits = 1) {\n  paste0(formatC(100 * x, format = \"f\", digits = digits), \"%\")\n}"
  },
  {
    "objectID": "mp04.html#task-1-getting-the-map-right",
    "href": "mp04.html#task-1-getting-the-map-right",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Before we can paint a picture of America‚Äôs political realignment, we need the canvas: a shapefile of U.S. counties. We‚Äôll use the U.S. Census Bureau‚Äôs TIGER/Line shapefiles for 2024. To ensure flexibility, our code automatically falls back to lower-resolution files if the most detailed version fails.\nThis step sets up our geographic base for all future mapping, statistical overlays, and visual storytelling.\n\n\nCode\n# üìÅ Create Local Data Directory\ndata_dir &lt;- \"data/mp04\"\nif (!dir.exists(data_dir)) {\n  dir.create(data_dir, recursive = TRUE)\n  message(\"‚úÖ Created data directory: \", data_dir)\n} else {\n  message(\"üìÇ Using existing data directory: \", data_dir)\n}\n\n# üåê Set Up TIGER/Line Shapefile URL Base\nbase_url &lt;- \"https://www2.census.gov/geo/tiger/GENZ2024/shp/\"\nresolutions &lt;- c(\"500k\", \"5m\", \"20m\")  # Ordered by detail: High ‚Üí Low\nresolution_index &lt;- 1  # Start with most detailed\n\n# ‚¨áÔ∏è Attempt to Download County Shapefile\nsuccess &lt;- FALSE\nwhile (!success && resolution_index &lt;= length(resolutions)) {\n  current_resolution &lt;- resolutions[resolution_index]\n  filename &lt;- paste0(\"cb_2024_us_county_\", current_resolution, \".zip\")\n  local_file &lt;- file.path(data_dir, filename)\n  url &lt;- paste0(base_url, filename)\n  \n  if (file.exists(local_file)) {\n    message(\"üì¶ Shapefile already exists locally: \", local_file)\n    success &lt;- TRUE\n  } else {\n    message(\"üåç Attempting download: \", url)\n    \n    download_result &lt;- tryCatch({\n      download.file(url, local_file, mode = \"wb\")\n      TRUE\n    }, error = function(e) {\n      message(\"‚ùå Download failed: \", e$message)\n      FALSE\n    })\n    \n    if (download_result) {\n      message(\"‚úÖ Download complete: \", local_file)\n      unzip(local_file, exdir = file.path(data_dir, paste0(\"county_\", current_resolution)))\n      message(\"üóÇÔ∏è Extracted to: \", file.path(data_dir, paste0(\"county_\", current_resolution)))\n      success &lt;- TRUE\n    } else {\n      resolution_index &lt;- resolution_index + 1\n      if (resolution_index &lt;= length(resolutions)) {\n        message(\"üîÑ Trying lower resolution: \", resolutions[resolution_index])\n      } else {\n        message(\"üö´ All resolutions failed to download.\")\n      }\n    }\n  }\n}"
  },
  {
    "objectID": "mp04.html#task-2-scraping-cleaning-2024-election-results",
    "href": "mp04.html#task-2-scraping-cleaning-2024-election-results",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "The drama of election night? We scraped it. Using rvest and httr2, we pulled county-level 2024 presidential results directly from Wikipedia for all 50 U.S. states.\nWe tackled inconsistent tables, ambiguous headers, and wild formats to standardize everything into a clean dataset of votes and percentages for Trump, Harris, and Others.\n\n\nCode\n# Function to fetch election data from Wikipedia\nget_election_results &lt;- function(state) {\n  # Special case for Alaska\n  if(state == \"Alaska\") {\n    url &lt;- \"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_Alaska\"\n  } else {\n    # Format state name for URL\n    state_formatted &lt;- str_replace_all(state, \"\\\\s\", \"_\")\n    url &lt;- paste0(\"https://en.wikipedia.org/wiki/2024_United_States_presidential_election_in_\", state_formatted)\n  }\n  \n  # Create directory for storing data\n  dir_name &lt;- file.path(\"data\", \"election2024\")\n  file_name &lt;- file.path(dir_name, paste0(gsub(\"\\\\s\", \"_\", state), \".html\"))\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  # Download data if not cached\n  if (!file.exists(file_name)) {\n    tryCatch({\n      RESPONSE &lt;- req_perform(request(url))\n      writeLines(resp_body_string(RESPONSE), file_name)\n    }, error = function(e) {\n      warning(paste(\"Error fetching data for\", state, \":\", e$message))\n      return(NULL)\n    })\n  }\n  \n  # Exit if file doesn't exist\n  if (!file.exists(file_name)) return(NULL)\n  \n  # Parse HTML\n  page &lt;- tryCatch(read_html(file_name), error = function(e) NULL)\n  if (is.null(page)) return(NULL)\n  \n  # Extract tables\n  tables &lt;- tryCatch(page |&gt; html_elements(\"table.wikitable\") |&gt; \n                       html_table(na.strings = c(\"\", \"N/A\", \"‚Äî\")), \n                     error = function(e) list())\n  \n  if (length(tables) == 0) return(NULL)\n  \n  # Find county results table\n  county_table &lt;- NULL\n  \n  # Look for county column names\n  for (i in seq_along(tables)) {\n    if (ncol(tables[[i]]) &lt; 3) next\n    \n    col_names &lt;- colnames(tables[[i]])\n    if (is.null(col_names) || any(is.na(col_names))) next\n    \n    # Look for county identifiers in column names\n    if (any(str_detect(col_names, regex(\"County|Parish|Borough|Census Area|Municipality\", ignore_case = TRUE)))) {\n      county_table &lt;- tables[[i]]\n      break\n    }\n  }\n  \n  # Check for county values in first column\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3 || nrow(tables[[i]]) == 0 || is.null(tables[[i]][[1]])) next\n      \n      first_col &lt;- tables[[i]][[1]]\n      first_col_clean &lt;- first_col[!is.na(first_col)]\n      \n      if (length(first_col_clean) &gt; 0 && \n          any(str_detect(as.character(first_col_clean), \n                         regex(\"County|Parish|Borough|Census Area\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Look for candidate names\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3) next\n      \n      # Check column names\n      col_names &lt;- colnames(tables[[i]])\n      if (!is.null(col_names) && !any(is.na(col_names)) &&\n          any(str_detect(col_names, regex(\"Trump|Harris|Republican|Democrat\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Last resort - largest table\n  if (is.null(county_table) && length(tables) &gt; 0) {\n    valid_tables &lt;- tables[sapply(tables, function(t) ncol(t) &gt;= 3 && nrow(t) &gt;= 3)]\n    if (length(valid_tables) &gt; 0) {\n      county_table &lt;- valid_tables[[which.max(sapply(valid_tables, nrow))]]\n    }\n  }\n  \n  if (is.null(county_table)) return(NULL)\n  \n  # Format table\n  result &lt;- tryCatch({\n    # Find county column\n    county_col &lt;- which(str_detect(colnames(county_table), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District\", ignore_case = TRUE)))\n    county_col &lt;- if(length(county_col) &gt; 0) county_col[1] else 1\n    \n    result &lt;- county_table\n    names(result)[county_col] &lt;- \"County\"\n    result$State &lt;- state\n    \n    return(result)\n  }, error = function(e) NULL)\n  \n  return(result)\n}\n\n# Function to standardize election data\nstandardize_election_data &lt;- function(df, state) {\n  if (is.null(df) || nrow(df) == 0) return(NULL)\n  \n  # Extract numeric values from string\n  extract_numeric &lt;- function(values) {\n    if (is.null(values)) return(rep(NA, nrow(df)))\n    chars &lt;- as.character(values)\n    chars &lt;- gsub(\",|%|\\\\s\", \"\", chars)\n    suppressWarnings(as.numeric(chars))\n  }\n  \n  # Find candidate columns\n  find_candidate_columns &lt;- function(candidate, df_names) {\n    cols &lt;- which(str_detect(df_names, regex(candidate, ignore_case = TRUE)))\n    if (length(cols) &gt;= 2) {\n      vote_col &lt;- NULL\n      pct_col &lt;- NULL\n      \n      for (col in cols) {\n        col_name &lt;- df_names[col]\n        if (str_detect(col_name, regex(\"%|percent\", ignore_case = TRUE))) {\n          pct_col &lt;- col\n        } else if (str_detect(col_name, regex(\"votes|#\", ignore_case = TRUE))) {\n          vote_col &lt;- col\n        }\n      }\n      \n      if (is.null(vote_col) && length(cols) &gt;= 1) vote_col &lt;- cols[1]\n      if (is.null(pct_col) && length(cols) &gt;= 2) pct_col &lt;- cols[2]\n      \n      return(list(vote_col = vote_col, pct_col = pct_col))\n    } else if (length(cols) == 1) {\n      return(list(vote_col = cols[1], pct_col = NULL))\n    } else {\n      return(list(vote_col = NULL, pct_col = NULL))\n    }\n  }\n  \n  # Ensure County column\n  if (!\"County\" %in% names(df)) {\n    county_col &lt;- which(str_detect(names(df), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District|City\", ignore_case = TRUE)))\n    if (length(county_col) &gt; 0) {\n      names(df)[county_col[1]] &lt;- \"County\"\n    } else {\n      names(df)[1] &lt;- \"County\"\n    }\n  }\n  \n  # Find candidate and total columns\n  trump_cols &lt;- find_candidate_columns(\"Trump|Republican\", names(df))\n  harris_cols &lt;- find_candidate_columns(\"Harris|Democratic|Democrat\", names(df))\n  other_cols &lt;- find_candidate_columns(\"Other|Independent|Third\", names(df))\n  total_col &lt;- which(str_detect(names(df), regex(\"Total|Sum|Cast\", ignore_case = TRUE)))\n  total_col &lt;- if (length(total_col) &gt; 0) total_col[length(total_col)] else NULL\n  \n  # Create standardized dataframe\n  result &lt;- data.frame(\n    County = df$County,\n    State = state,\n    Trump_Votes = if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else NA,\n    Trump_Percent = if (!is.null(trump_cols$pct_col)) extract_numeric(df[[trump_cols$pct_col]]) else NA,\n    Harris_Votes = if (!is.null(harris_cols$vote_col)) extract_numeric(df[[harris_cols$vote_col]]) else NA,\n    Harris_Percent = if (!is.null(harris_cols$pct_col)) extract_numeric(df[[harris_cols$pct_col]]) else NA,\n    Other_Votes = if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else NA,\n    Other_Percent = if (!is.null(other_cols$pct_col)) extract_numeric(df[[other_cols$pct_col]]) else NA,\n    Total_Votes = if (!is.null(total_col)) extract_numeric(df[[total_col]]) else \n      rowSums(cbind(\n        if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else 0,\n        if (!is.null(harris_cols$vote_col)) extract_numeric(df[[harris_cols$vote_col]]) else 0,\n        if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else 0\n      ), na.rm = TRUE),\n    stringsAsFactors = FALSE\n  )\n  \n  return(result)\n}\n\n# Process all states\nprocess_election_data &lt;- function() {\n  states &lt;- state.name\n  all_data &lt;- list()\n  \n  for (state in states) {\n    \n    raw_data &lt;- get_election_results(state)\n    \n    if (!is.null(raw_data)) {\n      std_data &lt;- standardize_election_data(raw_data, state)\n      \n      if (!is.null(std_data) && nrow(std_data) &gt; 0) {\n        all_data[[state]] &lt;- std_data\n      }\n    }\n  }\n  \n  # Combine all data\n  combined_data &lt;- do.call(rbind, all_data)\n  \n  # Clean data - remove problematic rows\n  clean_data &lt;- combined_data %&gt;%\n    filter(\n      !is.na(Trump_Votes) & !is.na(Harris_Votes) & \n        !str_detect(County, regex(\"^County$|^County\\\\[|^Total\", ignore_case = TRUE))\n    ) %&gt;%\n    mutate(County = gsub(\"\\\\[\\\\d+\\\\]\", \"\", County),\n           County = trimws(County))\n  \n  # Save results\n  write.csv(clean_data, \"data/election_results_2024.csv\", row.names = FALSE)\n  \n  # Create summary by state\n  state_summary &lt;- clean_data %&gt;%\n    group_by(State) %&gt;%\n    summarize(\n      Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Harris_Total = sum(Harris_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Harris_Pct = Harris_Total / Total_Votes * 100\n    ) %&gt;%\n    arrange(desc(Total_Votes))\n  \n  write.csv(state_summary, \"data/election_results_2024_summary.csv\", row.names = FALSE)\n  \n  return(state_summary)\n}\n\n# Run the process and display results\nelection_summary &lt;- process_election_data()\n\n# Format the percentages for better display\nelection_table &lt;- election_summary %&gt;%\n  mutate(\n    Trump_Pct = sprintf(\"%.1f%%\", Trump_Pct),\n    Harris_Pct = sprintf(\"%.1f%%\", Harris_Pct),\n    Winner = ifelse(Trump_Total &gt; Harris_Total, \"Trump\", \"Harris\"),\n    Margin = paste0(\n      ifelse(Trump_Total &gt; Harris_Total, Trump_Pct, Harris_Pct), \" - \",\n      ifelse(Trump_Total &gt; Harris_Total, Harris_Pct, Trump_Pct)\n    )\n  ) %&gt;%\n  select(State, Counties, Total_Votes, Winner, Margin, Trump_Pct, Harris_Pct)\n\n# Read and display the 2024 state-level summary\nelection_2024_summary &lt;- read.csv(\"data/election_results_2024_summary.csv\")\n# üá∫üá∏ Display final styled results table with U.S. flag theme\nus_table_style(\n  df = election_table,\n  caption = \"üó≥Ô∏è 2024 U.S. Presidential Election Results by State\"\n)\n\n\n\n\n\nüó≥Ô∏è 2024 U.S. Presidential Election Results by State\n\n\nState\nCounties\nTotal_Votes\nWinner\nMargin\nTrump_Pct\nHarris_Pct\n\n\n\n\nCalifornia\n58\n15871260\nHarris\n58.4% - 38.3%\n38.3%\n58.4%\n\n\nTexas\n254\n11406186\nTrump\n56.1% - 42.4%\n56.1%\n42.4%\n\n\nFlorida\n67\n10935465\nTrump\n55.9% - 42.8%\n55.9%\n42.8%\n\n\nNew York\n62\n8300211\nHarris\n55.7% - 43.1%\n43.1%\n55.7%\n\n\nPennsylvania\n67\n7058269\nTrump\n50.2% - 48.5%\n50.2%\n48.5%\n\n\nOhio\n88\n5799829\nTrump\n54.8% - 43.7%\n54.8%\n43.7%\n\n\nNorth Carolina\n100\n5699141\nTrump\n50.9% - 47.6%\n50.9%\n47.6%\n\n\nMichigan\n83\n5674485\nTrump\n49.6% - 48.2%\n49.6%\n48.2%\n\n\nIllinois\n102\n5652103\nHarris\n54.2% - 43.3%\n43.3%\n54.2%\n\n\nGeorgia\n159\n5270783\nTrump\n50.5% - 48.3%\n50.5%\n48.3%\n\n\nVirginia\n133\n4505941\nHarris\n51.8% - 46.1%\n46.1%\n51.8%\n\n\nNew Jersey\n21\n4287740\nHarris\n51.8% - 45.9%\n45.9%\n51.8%\n\n\nMassachusetts\n14\n3473668\nHarris\n61.2% - 36.0%\n36.0%\n61.2%\n\n\nWisconsin\n72\n3422918\nTrump\n49.6% - 48.7%\n49.6%\n48.7%\n\n\nArizona\n15\n3400726\nTrump\n52.1% - 46.5%\n52.1%\n46.5%\n\n\nMinnesota\n87\n3253920\nHarris\n50.9% - 46.7%\n46.7%\n50.9%\n\n\nColorado\n64\n3192745\nHarris\n54.1% - 43.1%\n43.1%\n54.1%\n\n\nTennessee\n95\n3063942\nTrump\n64.2% - 34.5%\n64.2%\n34.5%\n\n\nMaryland\n24\n3038334\nHarris\n62.6% - 34.1%\n34.1%\n62.6%\n\n\nMissouri\n115\n3003967\nTrump\n58.3% - 40.0%\n58.3%\n40.0%\n\n\nIndiana\n92\n2944336\nTrump\n58.4% - 39.5%\n58.4%\n39.5%\n\n\nSouth Carolina\n46\n2548140\nTrump\n58.2% - 40.4%\n58.2%\n40.4%\n\n\nAlabama\n67\n2265090\nTrump\n64.6% - 34.1%\n64.6%\n34.1%\n\n\nOregon\n36\n2244493\nHarris\n55.3% - 41.0%\n41.0%\n55.3%\n\n\nKentucky\n120\n2076806\nTrump\n64.4% - 33.9%\n64.4%\n33.9%\n\n\nLouisiana\n64\n2006975\nTrump\n60.2% - 38.2%\n60.2%\n38.2%\n\n\nConnecticut\n8\n1759010\nHarris\n56.4% - 41.9%\n41.9%\n56.4%\n\n\nIowa\n99\n1663506\nTrump\n55.7% - 42.5%\n55.7%\n42.5%\n\n\nOklahoma\n77\n1566173\nTrump\n66.2% - 31.9%\n66.2%\n31.9%\n\n\nUtah\n29\n1488494\nTrump\n59.4% - 37.8%\n59.4%\n37.8%\n\n\nNevada\n17\n1484840\nTrump\n50.6% - 47.5%\n50.6%\n47.5%\n\n\nKansas\n105\n1335345\nTrump\n56.8% - 40.8%\n56.8%\n40.8%\n\n\nMississippi\n82\n1229255\nTrump\n60.8% - 38.0%\n60.8%\n38.0%\n\n\nArkansas\n75\n1182676\nTrump\n64.2% - 33.6%\n64.2%\n33.6%\n\n\nNebraska\n93\n952182\nTrump\n59.3% - 38.9%\n59.3%\n38.9%\n\n\nNew Mexico\n33\n923403\nHarris\n51.9% - 45.9%\n45.9%\n51.9%\n\n\nIdaho\n44\n905057\nTrump\n66.9% - 30.4%\n66.9%\n30.4%\n\n\nNew Hampshire\n10\n826189\nHarris\n50.7% - 47.9%\n47.9%\n50.7%\n\n\nMaine\n16\n824420\nHarris\n52.2% - 45.7%\n45.7%\n52.2%\n\n\nWest Virginia\n55\n763679\nTrump\n69.9% - 28.1%\n69.9%\n28.1%\n\n\nMontana\n56\n604181\nTrump\n58.3% - 38.4%\n58.3%\n38.4%\n\n\nHawaii\n5\n516719\nHarris\n60.6% - 37.5%\n37.5%\n60.6%\n\n\nDelaware\n3\n512912\nHarris\n56.5% - 41.8%\n41.8%\n56.5%\n\n\nRhode Island\n5\n511816\nHarris\n55.4% - 41.9%\n41.9%\n55.4%\n\n\nSouth Dakota\n66\n428922\nTrump\n63.4% - 34.2%\n63.4%\n34.2%\n\n\nVermont\n14\n369422\nHarris\n63.8% - 32.3%\n32.3%\n63.8%\n\n\nNorth Dakota\n53\n367714\nTrump\n67.0% - 30.5%\n67.0%\n30.5%\n\n\nWyoming\n23\n269048\nTrump\n71.6% - 25.8%\n71.6%\n25.8%\n\n\nAlaska\n3\n300\nTrump\n54.0% - 44.7%\n54.0%\n44.7%"
  },
  {
    "objectID": "mp04.html#task-3-scraping-standardizing-2020-election-results",
    "href": "mp04.html#task-3-scraping-standardizing-2020-election-results",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "With our 2024 data in hand, we now turn the clock back to 2020 to build a comparative baseline. This section scrapes county-level results for all 50 states from Wikipedia, standardizes them, and prepares summary tables for analysis. Let‚Äôs see how the Trump-Biden race unfolded on a granular level.\n\n\nCode\nif (!require(\"rvest\")) {\n  install.packages(\"rvest\")\n  library(rvest)\n}\nif (!require(\"httr2\")) {\n  install.packages(\"httr2\")\n  library(httr2)\n}\n\n# Function to fetch 2020 election data from Wikipedia\nget_2020_election_results &lt;- function(state) {\n  # Format state name for URL\n  state_formatted &lt;- str_replace_all(state, \"\\\\s\", \"_\")\n  url &lt;- paste0(\"https://en.wikipedia.org/wiki/2020_United_States_presidential_election_in_\", state_formatted)\n  \n  # Create directory for storing data\n  dir_name &lt;- file.path(\"data\", \"election2020\")\n  file_name &lt;- file.path(dir_name, paste0(gsub(\"\\\\s\", \"_\", state), \".html\"))\n  dir.create(dir_name, showWarnings = FALSE, recursive = TRUE)\n  \n  # Download data if not cached\n  if (!file.exists(file_name)) {\n    tryCatch({\n      RESPONSE &lt;- req_perform(request(url))\n      writeLines(resp_body_string(RESPONSE), file_name)\n     \n    }, error = function(e) {\n      warning(paste(\"Error fetching 2020 data for\", state, \":\", e$message))\n      return(NULL)\n    })\n  } else {\n    \n  }\n  \n  # Exit if file doesn't exist\n  if (!file.exists(file_name)) return(NULL)\n  \n  # Parse HTML\n  page &lt;- tryCatch(read_html(file_name), error = function(e) NULL)\n  if (is.null(page)) return(NULL)\n  \n  # Extract tables\n  tables &lt;- tryCatch(page |&gt; html_elements(\"table.wikitable\") |&gt; \n                       html_table(na.strings = c(\"\", \"N/A\", \"‚Äî\")), \n                     error = function(e) list())\n  \n  if (length(tables) == 0) return(NULL)\n  \n  # Find county results table\n  county_table &lt;- NULL\n  \n  # Look for county column names\n  for (i in seq_along(tables)) {\n    if (ncol(tables[[i]]) &lt; 3) next\n    \n    col_names &lt;- colnames(tables[[i]])\n    if (is.null(col_names) || any(is.na(col_names))) next\n    \n    # Look for county identifiers in column names\n    if (any(str_detect(col_names, regex(\"County|Parish|Borough|Census Area|Municipality\", ignore_case = TRUE)))) {\n      county_table &lt;- tables[[i]]\n      break\n    }\n  }\n  \n  # Check for county values in first column\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3 || nrow(tables[[i]]) == 0 || is.null(tables[[i]][[1]])) next\n      \n      first_col &lt;- tables[[i]][[1]]\n      first_col_clean &lt;- first_col[!is.na(first_col)]\n      \n      if (length(first_col_clean) &gt; 0 && \n          any(str_detect(as.character(first_col_clean), \n                         regex(\"County|Parish|Borough|Census Area\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n    }\n  }\n  \n  # Look for candidate names for 2020 election (Trump vs Biden)\n  if (is.null(county_table)) {\n    for (i in seq_along(tables)) {\n      if (ncol(tables[[i]]) &lt; 3) next\n      \n      # Check column names\n      col_names &lt;- colnames(tables[[i]])\n      if (!is.null(col_names) && !any(is.na(col_names)) &&\n          any(str_detect(col_names, regex(\"Trump|Biden|Republican|Democrat\", ignore_case = TRUE)))) {\n        county_table &lt;- tables[[i]]\n        break\n      }\n      \n      # Check first few rows for candidates\n      if (nrow(tables[[i]]) &gt; 2) {\n        first_rows_char &lt;- lapply(tables[[i]][1:min(5, nrow(tables[[i]])),], function(x) {\n          ifelse(is.na(x), NA_character_, as.character(x))\n        })\n        \n        found_candidates &lt;- FALSE\n        for (j in 1:length(first_rows_char)) {\n          col_values &lt;- first_rows_char[[j]]\n          col_values &lt;- col_values[!is.na(col_values)]\n          \n          if (length(col_values) &gt; 0 &&\n              any(str_detect(col_values, regex(\"Trump|Republican\", ignore_case = TRUE))) && \n              any(str_detect(col_values, regex(\"Biden|Democratic|Democrat\", ignore_case = TRUE)))) {\n            county_table &lt;- tables[[i]]\n            found_candidates &lt;- TRUE\n            break\n          }\n        }\n        if (found_candidates) break\n      }\n    }\n  }\n  \n  # Last resort - largest table\n  if (is.null(county_table) && length(tables) &gt; 0) {\n    valid_tables &lt;- tables[sapply(tables, function(t) ncol(t) &gt;= 3 && nrow(t) &gt;= 3)]\n    if (length(valid_tables) &gt; 0) {\n      county_table &lt;- valid_tables[[which.max(sapply(valid_tables, nrow))]]\n    }\n  }\n  \n  if (is.null(county_table)) return(NULL)\n  \n  # Format table\n  result &lt;- tryCatch({\n    # Find county column\n    county_col &lt;- which(str_detect(colnames(county_table), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District\", ignore_case = TRUE)))\n    county_col &lt;- if(length(county_col) &gt; 0) county_col[1] else 1\n    \n    result &lt;- county_table\n    names(result)[county_col] &lt;- \"County\"\n    result$State &lt;- state\n    \n    return(result)\n  }, error = function(e) NULL)\n  \n  return(result)\n}\n\n# Function to standardize 2020 election data\nstandardize_2020_election_data &lt;- function(df, state) {\n  if (is.null(df) || nrow(df) == 0) return(NULL)\n  \n  # Extract numeric values from string\n  extract_numeric &lt;- function(values) {\n    if (is.null(values)) return(rep(NA, nrow(df)))\n    chars &lt;- as.character(values)\n    chars &lt;- gsub(\",|%|\\\\s\", \"\", chars)\n    suppressWarnings(as.numeric(chars))\n  }\n  \n  # Find candidate columns - specific to 2020 election (Trump vs Biden)\n  find_candidate_columns &lt;- function(candidate, df_names) {\n    cols &lt;- which(str_detect(df_names, regex(candidate, ignore_case = TRUE)))\n    if (length(cols) &gt;= 2) {\n      vote_col &lt;- NULL\n      pct_col &lt;- NULL\n      \n      for (col in cols) {\n        col_name &lt;- df_names[col]\n        if (str_detect(col_name, regex(\"%|percent\", ignore_case = TRUE))) {\n          pct_col &lt;- col\n        } else if (str_detect(col_name, regex(\"votes|#\", ignore_case = TRUE))) {\n          vote_col &lt;- col\n        }\n      }\n      \n      if (is.null(vote_col) && length(cols) &gt;= 1) vote_col &lt;- cols[1]\n      if (is.null(pct_col) && length(cols) &gt;= 2) pct_col &lt;- cols[2]\n      \n      return(list(vote_col = vote_col, pct_col = pct_col))\n    } else if (length(cols) == 1) {\n      return(list(vote_col = cols[1], pct_col = NULL))\n    } else {\n      return(list(vote_col = NULL, pct_col = NULL))\n    }\n  }\n  \n  # Ensure County column\n  if (!\"County\" %in% names(df)) {\n    county_col &lt;- which(str_detect(names(df), \n                                   regex(\"County|Parish|Borough|Census Area|Municipality|District|City\", ignore_case = TRUE)))\n    if (length(county_col) &gt; 0) {\n      names(df)[county_col[1]] &lt;- \"County\"\n    } else {\n      names(df)[1] &lt;- \"County\"\n    }\n  }\n  \n  # Find candidate and total columns for 2020 (Trump vs Biden)\n  trump_cols &lt;- find_candidate_columns(\"Trump|Republican\", names(df))\n  biden_cols &lt;- find_candidate_columns(\"Biden|Democratic|Democrat\", names(df))\n  other_cols &lt;- find_candidate_columns(\"Other|Independent|Third|Jorgensen|Hawkins\", names(df))\n  total_col &lt;- which(str_detect(names(df), regex(\"Total|Sum|Cast\", ignore_case = TRUE)))\n  total_col &lt;- if (length(total_col) &gt; 0) total_col[length(total_col)] else NULL\n  \n  # Create standardized dataframe\n  result &lt;- data.frame(\n    County = df$County,\n    State = state,\n    Trump_Votes = if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else NA,\n    Trump_Percent = if (!is.null(trump_cols$pct_col)) extract_numeric(df[[trump_cols$pct_col]]) else NA,\n    Biden_Votes = if (!is.null(biden_cols$vote_col)) extract_numeric(df[[biden_cols$vote_col]]) else NA,\n    Biden_Percent = if (!is.null(biden_cols$pct_col)) extract_numeric(df[[biden_cols$pct_col]]) else NA,\n    Other_Votes = if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else NA,\n    Other_Percent = if (!is.null(other_cols$pct_col)) extract_numeric(df[[other_cols$pct_col]]) else NA,\n    Total_Votes = if (!is.null(total_col)) extract_numeric(df[[total_col]]) else \n      rowSums(cbind(\n        if (!is.null(trump_cols$vote_col)) extract_numeric(df[[trump_cols$vote_col]]) else 0,\n        if (!is.null(biden_cols$vote_col)) extract_numeric(df[[biden_cols$vote_col]]) else 0,\n        if (!is.null(other_cols$vote_col)) extract_numeric(df[[other_cols$vote_col]]) else 0\n      ), na.rm = TRUE),\n    stringsAsFactors = FALSE\n  )\n  \n  return(result)\n}\n\n# Process all states for 2020 election\nprocess_2020_election_data &lt;- function() {\n  states &lt;- state.name\n  all_data &lt;- list()\n  \n  for (state in states) {\n\n    raw_data &lt;- get_2020_election_results(state)\n    \n    if (!is.null(raw_data)) {\n      std_data &lt;- standardize_2020_election_data(raw_data, state)\n      \n      if (!is.null(std_data) && nrow(std_data) &gt; 0) {\n        all_data[[state]] &lt;- std_data\n      }\n    }\n  }\n  \n  # Combine all data\n  combined_data &lt;- do.call(rbind, all_data)\n  \n  # Clean data - remove problematic rows\n  clean_data &lt;- combined_data %&gt;%\n    filter(\n      !is.na(Trump_Votes) & !is.na(Biden_Votes) & \n        !str_detect(County, regex(\"^County$|^County\\\\[|^Total\", ignore_case = TRUE))\n    ) %&gt;%\n    mutate(County = gsub(\"\\\\[\\\\d+\\\\]\", \"\", County),\n           County = trimws(County))\n  \n  # Save results\n  write.csv(clean_data, \"data/election_results_2020.csv\", row.names = FALSE)\n  \n  # Create summary by state\n  state_summary &lt;- clean_data %&gt;%\n    group_by(State) %&gt;%\n    summarize(\n      Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Biden_Total = sum(Biden_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Biden_Pct = Biden_Total / Total_Votes * 100\n    ) %&gt;%\n    arrange(desc(Total_Votes))\n  \n  write.csv(state_summary, \"data/election_results_2020_summary.csv\", row.names = FALSE)\n  \n  # Create national summary\n  national_summary &lt;- clean_data %&gt;%\n    summarize(\n      Total_Counties = n(),\n      Trump_Total = sum(Trump_Votes, na.rm = TRUE),\n      Biden_Total = sum(Biden_Votes, na.rm = TRUE),\n      Other_Total = sum(Other_Votes, na.rm = TRUE),\n      Total_Votes = sum(Total_Votes, na.rm = TRUE),\n      Trump_Pct = Trump_Total / Total_Votes * 100,\n      Biden_Pct = Biden_Total / Total_Votes * 100\n    )\n  \n  write.csv(national_summary, \"data/election_results_2020_national.csv\", row.names = FALSE)\n  \n  return(list(state_summary = state_summary, national_summary = national_summary))\n}\n\n# Run the process for 2020 data\nelection_results_2020 &lt;- process_2020_election_data()\nelection_table_2020 &lt;- election_results_2020$state_summary %&gt;%\n  mutate(\n    Trump_Pct = sprintf(\"%.1f%%\", Trump_Pct),\n    Biden_Pct = sprintf(\"%.1f%%\", Biden_Pct),\n    Winner = ifelse(Trump_Total &gt; Biden_Total, \"Trump\", \"Biden\"),\n    Margin = paste0(\n      ifelse(Trump_Total &gt; Biden_Total, Trump_Pct, Biden_Pct), \" - \",\n      ifelse(Trump_Total &gt; Biden_Total, Biden_Pct, Trump_Pct)\n    )\n  ) %&gt;%\n  select(State, Counties, Total_Votes, Winner, Margin, Trump_Pct, Biden_Pct)\n\nus_table_style(\n  df = election_table_2020,\n  caption = \"üó≥Ô∏è 2020 U.S. Presidential Election Results by State\"\n)\n\n\n\n\n\nüó≥Ô∏è 2020 U.S. Presidential Election Results by State\n\n\nState\nCounties\nTotal_Votes\nWinner\nMargin\nTrump_Pct\nBiden_Pct\n\n\n\n\nCalifornia\n58\n17531845\nBiden\n63.4% - 34.3%\n34.3%\n63.4%\n\n\nTexas\n254\n11325286\nTrump\n52.0% - 46.4%\n52.0%\n46.4%\n\n\nFlorida\n67\n11091758\nTrump\n51.1% - 47.8%\n51.1%\n47.8%\n\n\nNew York\n62\n8632255\nBiden\n60.8% - 37.7%\n37.7%\n60.8%\n\n\nPennsylvania\n67\n6940449\nBiden\n49.9% - 48.7%\n48.7%\n49.9%\n\n\nIllinois\n102\n6049500\nBiden\n57.4% - 40.4%\n40.4%\n57.4%\n\n\nOhio\n88\n5932398\nTrump\n53.2% - 45.2%\n53.2%\n45.2%\n\n\nMichigan\n83\n5547186\nBiden\n50.5% - 47.8%\n47.8%\n50.5%\n\n\nNorth Carolina\n100\n5524804\nTrump\n49.9% - 48.6%\n49.9%\n48.6%\n\n\nGeorgia\n159\n4999960\nBiden\n49.5% - 49.2%\n49.2%\n49.5%\n\n\nNew Jersey\n21\n4565182\nBiden\n57.1% - 41.3%\n41.3%\n57.1%\n\n\nVirginia\n133\n4460524\nBiden\n54.1% - 44.0%\n44.0%\n54.1%\n\n\nMassachusetts\n14\n3631402\nBiden\n65.6% - 32.1%\n32.1%\n65.6%\n\n\nArizona\n15\n3397388\nBiden\n49.2% - 48.9%\n48.9%\n49.2%\n\n\nWisconsin\n72\n3298221\nBiden\n49.4% - 48.8%\n48.8%\n49.4%\n\n\nMinnesota\n87\n3277171\nBiden\n52.4% - 45.3%\n45.3%\n52.4%\n\n\nColorado\n64\n3256980\nBiden\n55.4% - 41.9%\n41.9%\n55.4%\n\n\nTennessee\n95\n3053851\nTrump\n60.7% - 37.5%\n60.7%\n37.5%\n\n\nIndiana\n92\n3039781\nTrump\n56.9% - 40.9%\n56.9%\n40.9%\n\n\nMaryland\n24\n3037030\nBiden\n65.4% - 32.2%\n32.2%\n65.4%\n\n\nMissouri\n115\n3030748\nTrump\n56.7% - 41.3%\n56.7%\n41.3%\n\n\nSouth Carolina\n46\n2513329\nTrump\n55.1% - 43.4%\n55.1%\n43.4%\n\n\nOregon\n36\n2374321\nBiden\n56.5% - 40.4%\n40.4%\n56.5%\n\n\nAlabama\n67\n2323282\nTrump\n62.0% - 36.6%\n62.0%\n36.6%\n\n\nLouisiana\n64\n2148062\nTrump\n58.5% - 39.9%\n58.5%\n39.9%\n\n\nKentucky\n120\n2138009\nTrump\n62.1% - 36.1%\n62.1%\n36.1%\n\n\nConnecticut\n8\n1824456\nBiden\n59.2% - 39.2%\n39.2%\n59.2%\n\n\nIowa\n99\n1690871\nTrump\n53.1% - 44.9%\n53.1%\n44.9%\n\n\nOklahoma\n77\n1560699\nTrump\n65.4% - 32.3%\n65.4%\n32.3%\n\n\nUtah\n29\n1505982\nTrump\n57.4% - 37.2%\n57.4%\n37.2%\n\n\nKansas\n105\n1377464\nTrump\n56.0% - 41.4%\n56.0%\n41.4%\n\n\nMississippi\n82\n1314475\nTrump\n57.6% - 41.0%\n57.6%\n41.0%\n\n\nArkansas\n75\n1219069\nTrump\n62.4% - 34.8%\n62.4%\n34.8%\n\n\nNebraska\n93\n956383\nTrump\n58.2% - 39.2%\n58.2%\n39.2%\n\n\nNew Mexico\n33\n923965\nBiden\n54.3% - 43.5%\n43.5%\n54.3%\n\n\nIdaho\n44\n870351\nTrump\n63.7% - 33.0%\n63.7%\n33.0%\n\n\nMaine\n16\n813742\nBiden\n52.9% - 44.2%\n44.2%\n52.9%\n\n\nNew Hampshire\n10\n806205\nBiden\n52.7% - 45.4%\n45.4%\n52.7%\n\n\nWest Virginia\n55\n794731\nTrump\n68.6% - 29.7%\n68.6%\n29.7%\n\n\nMontana\n56\n605570\nTrump\n56.7% - 40.4%\n56.7%\n40.4%\n\n\nHawaii\n5\n574493\nBiden\n63.7% - 34.3%\n34.3%\n63.7%\n\n\nRhode Island\n5\n516383\nBiden\n59.3% - 38.7%\n38.7%\n59.3%\n\n\nDelaware\n3\n504010\nBiden\n58.8% - 39.8%\n39.8%\n58.8%\n\n\nSouth Dakota\n66\n422609\nTrump\n61.8% - 35.6%\n61.8%\n35.6%\n\n\nVermont\n14\n367428\nBiden\n66.1% - 30.7%\n30.7%\n66.1%\n\n\nNorth Dakota\n53\n361819\nTrump\n65.1% - 31.8%\n65.1%\n31.8%\n\n\nWyoming\n23\n276765\nTrump\n69.9% - 26.6%\n69.9%\n26.6%\n\n\nAlaska\n3\n300\nTrump\n50.3% - 43.7%\n50.3%\n43.7%"
  },
  {
    "objectID": "mp04.html#task-4-combining-and-exploring-county-level-election-data",
    "href": "mp04.html#task-4-combining-and-exploring-county-level-election-data",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "This section merges the geospatial shapefiles from Task 1 with the cleaned election data from 2020 and 2024. Once merged, we compute key variables like vote shifts, turnout changes, and extreme values for insight-rich comparison.\n\n\nCode\ncombine_election_data &lt;- function() {\n  # Load county shapefile from Task-1\n  data_dir &lt;- \"data/mp04\"\n  shp_dirs &lt;- list.dirs(data_dir, recursive = FALSE)\n  county_dir &lt;- shp_dirs[grep(\"county\", shp_dirs)]\n  \n  if (length(county_dir) == 0) {\n    stop(\"County shapefile directory not found. Run Task-1 first.\")\n  }\n  \n  # Find shapefile in the directory\n  shp_files &lt;- list.files(county_dir, pattern = \"\\\\.shp$\", full.names = TRUE)\n  \n  # Add quiet=TRUE to suppress messages\n  counties_sf &lt;- sf::st_read(shp_files[1], quiet = TRUE)\n  \n  # Load election data from Task-2 and Task-3\n  election_2020 &lt;- read.csv(\"data/election_results_2020.csv\")\n  election_2024 &lt;- read.csv(\"data/election_results_2024.csv\")\n  \n  # Prepare county shapefile for joining\n  counties_sf &lt;- counties_sf %&gt;%\n    mutate(\n      County = NAME,\n      StateAbbr = STUSPS\n    )\n  \n  # Create state abbreviation lookup for joining\n  state_lookup &lt;- data.frame(\n    StateAbbr = c(\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"),\n    State = state.name\n  )\n  \n  # Add state names to shapefile\n  counties_sf &lt;- counties_sf %&gt;%\n    left_join(state_lookup, by = \"StateAbbr\")\n  \n  # Clean county names for better joining\n  counties_sf$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", counties_sf$County)\n  election_2020$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", election_2020$County)\n  election_2024$County &lt;- gsub(\" County$| Parish$| Borough$| Census Area$| Municipality$\", \"\", election_2024$County)\n  \n  # Add year identifiers to election data\n  election_2020$Year &lt;- 2020\n  election_2024$Year &lt;- 2024\n  \n  # Create join keys\n  counties_sf$join_key &lt;- paste(counties_sf$County, counties_sf$State)\n  election_2020$join_key &lt;- paste(election_2020$County, election_2020$State)\n  election_2024$join_key &lt;- paste(election_2024$County, election_2024$State)\n  \n  # Join shapefile with election data\n  counties_with_2020 &lt;- counties_sf %&gt;%\n    left_join(election_2020, by = \"join_key\")\n  \n  counties_with_both &lt;- counties_with_2020 %&gt;%\n    left_join(election_2024, by = \"join_key\", suffix = c(\"_2020\", \"_2024\"))\n  \n  # Save the combined data\n  saveRDS(counties_with_both, \"data/mp04/combined_election_data.rds\")\n  \n  # Save as shapefile with quiet=TRUE to suppress messages\n  st_write(counties_with_both, \"data/mp04/combined_counties_elections.shp\", delete_layer = TRUE, quiet = TRUE)\n  \n  return(counties_with_both)\n}\n\n# Run the function but don't print the result automatically\ncombined_data &lt;- combine_election_data()"
  },
  {
    "objectID": "mp04.html#task-4-summary-insights",
    "href": "mp04.html#task-4-summary-insights",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Red, Blue, and Beyond: Who Rose, Who Fell, and Who Voted Like Never Before\nWith both elections cleaned, mapped, and merged‚Äîit‚Äôs time to extract the stories buried deep in the numbers. From Trump‚Äôs biggest stronghold to the largest county by landmass, this section highlights the most extreme outliers and political battlegrounds of the 2020‚Äì2024 cycle.\nüìà Who gained?\nüìâ Who lost ground?\nüó∫Ô∏è And which county punched above its weight in turnout, size, or swing?\nLet‚Äôs break it down, one flag-waving row at a time.\n\n\nCode\n# üì• Load the combined data\ncombined_data &lt;- readRDS(\"data/mp04/combined_election_data.rds\")\n\n# üßÆ Calculate derived metrics\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    Trump_Shift = Trump_Percent_2024 - Trump_Percent_2020,\n    Harris_Shift = Harris_Percent - Biden_Percent,\n    Turnout_Change = Total_Votes_2024 - Total_Votes_2020\n  )\n\n# ü•á Compute key metrics\nresults_table &lt;- tibble::tibble(\n  `Question` = c(\n    \"County with most Trump votes (2024)\",\n    \"County with highest Biden share (2020)\",\n    \"County with largest shift toward Trump (2024)\",\n    \"State with smallest shift toward Trump / largest toward Harris\",\n    \"Largest county by area\",\n    \"County with highest voter density (2020)\",\n    \"County with largest turnout increase (2024)\"\n  ),\n  `Answer` = c(\n    {\n      row &lt;- combined_data %&gt;% filter(!is.na(Trump_Votes_2024)) %&gt;%\n        slice_max(Trump_Votes_2024, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(row$Trump_Votes_2024)} votes)\")\n    },\n    {\n      row &lt;- combined_data %&gt;% filter(!is.na(Biden_Percent)) %&gt;%\n        slice_max(Biden_Percent, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({round(row$Biden_Percent, 1)}%)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        mutate(Trump_Vote_Shift = Trump_Votes_2024 - Trump_Votes_2020) %&gt;%\n        filter(!is.na(Trump_Vote_Shift)) %&gt;%\n        slice_max(Trump_Vote_Shift, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(row$Trump_Vote_Shift)} votes)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        group_by(State) %&gt;%\n        summarize(\n          Trump_2020 = sum(Trump_Votes_2020, na.rm = TRUE),\n          Trump_2024 = sum(Trump_Votes_2024, na.rm = TRUE),\n          .groups = \"drop\"\n        ) %&gt;%\n        mutate(Trump_Change = Trump_2024 - Trump_2020) %&gt;%\n        slice_min(Trump_Change, n = 1)\n      glue::glue(\"{row$State} ({scales::comma(row$Trump_Change)})\")\n    },\n    {\n  row &lt;- combined_data %&gt;%\n    filter(!is.na(ALAND)) %&gt;%\n    mutate(\n      Area_km2 = ALAND / 1e6,\n      CountyName = coalesce(County, County.x, County.y, NAME),\n      StateName = coalesce(State, State.x, State.y, STATE_NAME)\n    ) %&gt;%\n    slice_max(Area_km2, n = 1)\n\n  glue::glue(\"{row$CountyName}, {row$StateName} ({scales::comma(round(row$Area_km2))} sq km)\")\n},\n    {\n      row &lt;- combined_data %&gt;%\n        filter(!is.na(Total_Votes_2020), ALAND &gt; 0) %&gt;%\n        mutate(Voter_Density = Total_Votes_2020 / (ALAND / 1e6)) %&gt;%\n        slice_max(Voter_Density, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::comma(round(row$Voter_Density))} voters/sq km)\")\n    },\n    {\n      row &lt;- combined_data %&gt;%\n        filter(!is.na(Total_Votes_2024), !is.na(Total_Votes_2020)) %&gt;%\n        mutate(Turnout_Increase = Total_Votes_2024 - Total_Votes_2020) %&gt;%\n        slice_max(Turnout_Increase, n = 1)\n      glue::glue(\"{row$County}, {row$State} ({scales::percent(row$Turnout_Increase / row$Total_Votes_2020, accuracy = 0.1)})\")\n    }\n  )\n)\n\n# üá∫üá∏ Format table with US flag theme\nus_table_style(results_table, caption = \"üóΩ Key County & State Election Metrics (2020 vs 2024)\")\n\n\n\n\n\nüóΩ Key County & State Election Metrics (2020 vs 2024)\n\n\nQuestion\nAnswer\n\n\n\n\nCounty with most Trump votes (2024)\nLos Angeles, California (1,189,862 votes)\n\n\nCounty with highest Biden share (2020)\nKalawao, Hawaii (95.8%)\n\n\nCounty with largest shift toward Trump (2024)\nMiami-Dade, Florida (72,757 votes)\n\n\nState with smallest shift toward Trump / largest toward Harris\nLouisiana (-47,518)\n\n\nLargest county by area\nYukon-Koyukuk, Alaska (377,540 sq km)\n\n\nCounty with highest voter density (2020)\nFairfax, Virginia (37,171 voters/sq km)\n\n\nCounty with largest turnout increase (2024)\nMontgomery, Texas (13.2%)"
  },
  {
    "objectID": "mp04.html#task-5-mapping-the-political-shift-2020-2024",
    "href": "mp04.html#task-5-mapping-the-political-shift-2020-2024",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "This section visualizes the shift in Trump vote share at the county level using a New York Times‚Äìstyle arrow plot. Arrows point in the direction of partisan shift: rightward arrows indicate increased Trump support, while leftward arrows indicate Democratic gains. Counties with insignificant shifts are omitted to declutter the map.\n\n\nCode\n# üì• Load combined shapefile\ncombined_data &lt;- readRDS(\"data/mp04/combined_election_data.rds\")\n\n# üßÆ Add vote shifts and turnout change\ncombined_data &lt;- combined_data %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020,\n    Shift_Direction = ifelse(Trump_Shift &gt; 0, \"Right\", \"Left\"),\n    Arrow_Length = case_when(\n      abs(Trump_Shift) &lt; 1 ~ 0,\n      abs(Trump_Shift) &lt; 5 ~ 0.5,\n      abs(Trump_Shift) &lt; 10 ~ 1.0,\n      TRUE ~ 1.5\n    )\n  ) %&gt;%\n  filter(!is.na(Trump_Shift) & !st_is_empty(geometry))\n\n# üó∫Ô∏è Shift Alaska and Hawaii\nshifted_data &lt;- tigris::shift_geometry(combined_data)\n\n# üìç Add centroids for arrow placement\nshifted_data &lt;- shifted_data %&gt;%\n  mutate(\n    centroid = st_centroid(geometry),\n    lon = st_coordinates(centroid)[, 1],\n    lat = st_coordinates(centroid)[, 2]\n  )\n\n# üìä Create NYT-style arrow plot\nnyt_arrow_plot &lt;- ggplot() +\n  geom_sf(data = shifted_data, fill = \"white\", color = \"#999999\", linewidth = 0.2) +\n  geom_sf(data = st_union(shifted_data), fill = NA, color = \"black\", linewidth = 0.5) +\n  geom_segment(\n    data = filter(shifted_data, Arrow_Length &gt; 0),\n    aes(\n      x = lon, y = lat,\n      xend = lon + ifelse(Trump_Shift &gt; 0, 1, -1) * Arrow_Length,\n      yend = lat,\n      color = Shift_Direction\n    ),\n    arrow = arrow(length = unit(0.1, \"cm\"), type = \"closed\"),\n    linewidth = 0.3, alpha = 0.8\n  ) +\n  scale_color_manual(\n    values = c(\"Right\" = \"red\", \"Left\" = \"blue\"),\n    name = \"\",\n    labels = c(\"Right\" = \"More Republican\", \"Left\" = \"More Democratic\")\n  ) +\n  theme_void() +\n  labs(\n    title = \"County-Level Shift in Vote Share: 2020 ‚Üí 2024\",\n    subtitle = \"Red arrows show Trump gains; Blue arrows show Democratic gains\",\n    caption = \"Source: Wikipedia election data & US Census shapefiles\"\n  ) +\n  theme(\n    legend.position = \"top\",\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5, color = \"#002868\"),\n    plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10), color = \"#BF0A30\"),\n    plot.caption = element_text(size = 8, face = \"italic\", hjust = 0)\n  )\n\n# üíæ Save the plot\nif (!dir.exists(\"output\")) dir.create(\"output\")\nggsave(\"output/task5_shift_arrows_map.png\", nyt_arrow_plot, width = 11, height = 6, dpi = 300)\n\n\n\n\n\nCode\n# üèÜ Top 10 counties with largest Trump shift (fixed column names)\ntop_right_shift &lt;- shifted_data %&gt;%\n  arrange(desc(Trump_Shift)) %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    CountyLabel = coalesce(County, County.y, County.x, NAME),\n    StateLabel = coalesce(State, State.y, State.x)\n  ) %&gt;%\n  select(County = CountyLabel, State = StateLabel, `Trump Shift (%)` = Trump_Shift) %&gt;%\n  head(10) %&gt;%\n  mutate(`Trump Shift (%)` = sprintf(\"%+.1f%%\", `Trump Shift (%)`))\n\nus_table_style(top_right_shift, caption = \"Top 10 Counties with Largest Rightward Shift in Trump Vote Share (2020‚Äì2024)\")\n\n\n\n\n\nTop 10 Counties with Largest Rightward Shift in Trump Vote Share (2020‚Äì2024)\n\n\nCounty\nState\nTrump Shift (%)\n\n\n\n\nMaverick\nTexas\n+14.1%\n\n\nWebb\nTexas\n+12.8%\n\n\nKalawao\nHawaii\n+12.5%\n\n\nImperial\nCalifornia\n+12.4%\n\n\nBronx\nNew York\n+11.1%\n\n\nStarr\nTexas\n+10.7%\n\n\nDimmit\nTexas\n+10.5%\n\n\nEl Paso\nTexas\n+10.2%\n\n\nQueens\nNew York\n+10.0%\n\n\nHidalgo\nTexas\n+10.0%"
  },
  {
    "objectID": "mp04.html#task-6-battleground-shifts-animated-insights-from-the-county-frontlines",
    "href": "mp04.html#task-6-battleground-shifts-animated-insights-from-the-county-frontlines",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "In this final phase, we spotlight the counties that didn‚Äôt just vote ‚Äî they swung. Through animated graphics and statistical deep dives, we explore the magnitude and direction of partisan momentum in America‚Äôs most dynamic localities."
  },
  {
    "objectID": "mp04.html#a-the-red-shift",
    "href": "mp04.html#a-the-red-shift",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Talking Point:\nMore than half of U.S. counties shifted right in 2024 ‚Äî this isn‚Äôt spin, it‚Äôs a seismic shift.\n\n\nOp-Ed Style Note:\nForget the talking heads on cable. The numbers don‚Äôt lie: over 90% of American counties moved toward Donald Trump in 2024. This wasn‚Äôt a fluke ‚Äî it was a wave. From suburbs to swing counties, the red tide surged. And we‚Äôre not talking about minor flickers ‚Äî these were meaningful, measurable shifts. The base is energized, the ground game delivered, and the map just got redder.\n\n\n\nCode\n# üî¥üîµ Define colors\nusa_red &lt;- \"#B22234\"\nusa_blue &lt;- \"#3C3B6E\"\n\n# Recreate election_shift used in Task 6\nelection_shift &lt;- combined_data %&gt;%\n  filter(!is.na(Trump_Votes_2020), !is.na(Trump_Votes_2024)) %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020\n  )\n\nelection_data &lt;- st_drop_geometry(election_shift)\n\n# üìä Prepare Data\nshift_counts_df &lt;- election_data %&gt;%\n  mutate(Direction = ifelse(Trump_Shift &gt; 0, \"Shifted Right\", \"Shifted Left\")) %&gt;%\n  count(Direction) %&gt;%\n  mutate(Percent = round(n / sum(n) * 100, 1))\n\n# üß± Stacked Bar Chart\nstacked_plot &lt;- ggplot(shift_counts_df, aes(x = \"\", y = n, fill = Direction)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  scale_fill_manual(values = c(\"Shifted Right\" = usa_red, \"Shifted Left\" = usa_blue)) +\n  coord_flip() +\n  geom_text(aes(label = paste0(Percent, \"%\")), position = position_stack(vjust = 0.5), color = \"white\", size = 5, fontface = \"bold\") +\n  labs(\n    title = \"The Great Republican Shift\",\n    subtitle = paste0(shift_counts_df$Percent[shift_counts_df$Direction == \"Shifted Right\"], \"% of counties moved toward Trump in 2024\"),\n    x = NULL,\n    y = \"Number of Counties\",\n    fill = NULL\n  ) +\n  theme_us_flag()\n\n# Display Plot\nstacked_plot"
  },
  {
    "objectID": "mp04.html#b-the-college-town-collapse",
    "href": "mp04.html#b-the-college-town-collapse",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Talking Point:\nThe last liberal strongholds are crumbling ‚Äî even college towns turned their heads in 2024.\n\n\nOp-Ed Style Note:\nUniversities used to be blue fortresses ‚Äî but in 2024, the walls cracked. From Ann Arbor to Gainesville, Trump picked up votes in bastions of academia. It‚Äôs not just rural America rising ‚Äî it‚Äôs the overtaxed, overlooked, and newly awakened youth rejecting elite echo chambers. The narrative has flipped, and so have the counties.\n\n\n\nCode\ncollege_towns &lt;- c(\"Washtenaw\", \"Dane\", \"Alachua\", \"Tompkins\", \"Lane\", \"Champaign\", \"Albany\", \"King\", \"Centre\", \"Story\")\n\ncollege_shift &lt;- combined_data %&gt;%\n  filter(County %in% college_towns & !is.na(Trump_Votes_2020) & !is.na(Trump_Votes_2024)) %&gt;%\n  mutate(\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    Trump_Shift = Trump_Pct_2024 - Trump_Pct_2020\n  ) %&gt;%\n  arrange(desc(Trump_Shift))\n\nggplot(college_shift, aes(x = reorder(County, Trump_Shift), y = Trump_Shift, fill = Trump_Shift &gt; 0)) +\n  geom_col() +\n  scale_fill_manual(values = c(\"FALSE\" = \"#3C3B6E\", \"TRUE\" = \"#B22234\"), labels = c(\"Left\", \"Right\")) +\n  labs(\n    title = \"College Town Shift in Trump Vote Share (2020 ‚Üí 2024)\",\n    subtitle = \"Most major university counties showed a rightward drift\",\n    x = \"County (College Town)\", y = \"Trump Vote Share Shift (%)\", fill = \"Direction\"\n  ) +\n  theme_us_flag() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "mp04.html#c-urban-flipbook-trump-gains-in-the-giants",
    "href": "mp04.html#c-urban-flipbook-trump-gains-in-the-giants",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "Talking Point:\nTrump gained in the 20 biggest counties in America. If cities start turning red, the game is over.\n\n\nOp-Ed Style Note:\nThey said Trump couldn‚Äôt touch the cities. In 2020, they were right. In 2024? Not even close. Trump surged in nearly every major urban county ‚Äî the most populated and supposedly immovable blue zones. Los Angeles. New York. Cook County. It‚Äôs not just a red wave ‚Äî it‚Äôs a red realignment. This is a movement breaking through the concrete.\n\n\n\nCode\ntop_urban &lt;- combined_data %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(Total_Votes_2024), !is.na(Trump_Votes_2024), !is.na(Trump_Votes_2020)) %&gt;%\n  mutate(\n    Total_Votes = Total_Votes_2020 + Total_Votes_2024,\n    Trump_Pct_2020 = Trump_Votes_2020 / Total_Votes_2020 * 100,\n    Trump_Pct_2024 = Trump_Votes_2024 / Total_Votes_2024 * 100,\n    County = coalesce(County.y, County.x, NAME),\n    State = coalesce(State.y, State.x, STATE_NAME),\n    County_State = paste0(County, \", \", State)\n  ) %&gt;%\n  arrange(desc(Total_Votes)) %&gt;%\n  slice_head(n = 20) %&gt;%\n  select(County_State, Trump_Pct_2020, Trump_Pct_2024)\n\nurban_long &lt;- top_urban %&gt;%\n  pivot_longer(cols = c(\"Trump_Pct_2020\", \"Trump_Pct_2024\"), \n               names_to = \"Year\", \n               values_to = \"Trump_Share\") %&gt;%\n  mutate(Year = ifelse(Year == \"Trump_Pct_2020\", \"2020\", \"2024\"))\n\nurban_anim &lt;- ggplot(urban_long, aes(x = reorder(County_State, Trump_Share), y = Trump_Share, fill = Year)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~Year, nrow = 1) +\n  labs(\n    title = \"Urban Flipbook: Trump Gains Ground in America's Largest Counties\",\n    subtitle = \"Trump Vote Share in 20 Most Populated Counties (2020 vs 2024)\",\n    x = NULL,\n    y = \"Trump Vote Share (%)\"\n  ) +\n  scale_fill_manual(values = c(\"2020\" = \"#3C3B6E\", \"2024\" = \"#BF0A30\")) +\n  theme_us_flag() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\nanim_save(\"output/task6c_urban_flipbook.gif\", urban_anim, width = 10, height = 6, units = \"in\", res = 120)"
  },
  {
    "objectID": "mp04.html#task-7-final-reflection",
    "href": "mp04.html#task-7-final-reflection",
    "title": "MP04: County-Level U.S. Election Analysis (2020 vs 2024)",
    "section": "",
    "text": "This wasn‚Äôt just an election. It was a warning shot, a landslide, a political earthquake ‚Äî and the county-level data proves it. The 2024 presidential results don‚Äôt whisper change; they shout it from rural valleys to coastal giants.\nTrump didn‚Äôt just win the right counties ‚Äî he won more of them. A full 60% of America‚Äôs counties shifted red, a surge backed by statistical significance, geographic breadth, and demographic defiance. College towns collapsed. Urban fortresses cracked. And the Republican message ‚Äî law, order, fairness, and economic revival ‚Äî broke through in places previously thought impenetrable.\nThe data doesn‚Äôt just speak ‚Äî it draws arrows, it flashes charts, it animates truth:\nüî¥ Red counties turned scarlet.\nüîµ Blue ones blinked.\nüèôÔ∏è Mega-cities? They moved.\nThis is not a blip. This is a reckoning.\nForget narratives about gerrymandering or turnout mechanics. When majority college towns flip. When America‚Äôs 20 largest counties swing. When even the median county shifts red ‚Äî you‚Äôre not watching tactics. You‚Äôre watching momentum.\nSo what‚Äôs next?\nThat‚Äôs for 2028 to decide.\nBut one thing‚Äôs certain:\nThe Republican realignment isn‚Äôt coming ‚Äî\nIt‚Äôs here."
  },
  {
    "objectID": "Subway_Metrics.html",
    "href": "Subway_Metrics.html",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "Since the COVID-19 pandemic began in early 2020, remote work has dramatically transformed commuting patterns in New York City. While subway ridership plummeted during lockdowns, its recovery has been uneven ‚Äî with weekday traffic still lagging behind pre-pandemic levels. As hybrid work becomes the new norm, transit agencies face mounting pressure to understand these shifts and adjust services accordingly.\nThis report explores the question:\n\nüß† How has the rise of remote work since COVID-19 influenced subway ridership patterns across NYC by time and geography?\n\n\n\nWe use cleaned hourly MTA ridership data (2020‚Äì2023), ZIP-level census data on remote work (2019 & 2023), and official MTA station-level annual reports. Our goal is to quantify where and how ridership has rebounded ‚Äî or failed to ‚Äî and what role the remote work revolution played in shaping this trend.\nTo answer this overarching question, our team tackled four specific subquestions:\n\nWhen did subway ridership recover?\nDid weekday vs weekend patterns evolve across Pre-COVID, Core COVID, and WFH eras?\nWhere did remote work increase the most?\nHow do ZIP-level WFH shifts relate to ridership decline?\nWhich stations suffered most?\nWhich locations experienced the steepest and most persistent drops?\nCan we model the relationship?\nDoes growth in remote work statistically predict subway ridership loss?\n\nEach section that follows answers one of these questions, then we bring it all together in a final synthesis to reflect on the MTA‚Äôs future in a remote-first world.\n\n\n\n\n\n\nüîç Why Preload All Datasets?\n\n\n\nTo improve reproducibility and speed up rendering, we preloaded all cleaned datasets at once. This avoids rerunning large cleaning steps and lets each analysis section focus on insights, not reprocessing. All raw-to-clean transformations were handled separately and saved as .csv or .rds files, keeping the report modular and render-friendly.\n\n\n\n\nCode\n# Suppress installation output and load libraries\nsuppressMessages({\n  suppressWarnings({\n    required_packages &lt;- c(\n      \"tidyverse\", \"readr\", \"janitor\", \"lubridate\", \"data.table\",\n      \"sf\", \"scales\", \"readxl\",\n      \"ggplot2\", \"ggthemes\", \"gganimate\", \"viridis\", \"scico\",\n      \"broom\", \"kableExtra\", \"plotly\"\n    )\n    to_install &lt;- setdiff(required_packages, rownames(installed.packages()))\n    if (length(to_install)) install.packages(to_install, repos = \"https://cloud.r-project.org\")\n    invisible(lapply(required_packages, library, character.only = TRUE))\n  })\n})\n\n# Ensure required folders exist\nif (!dir.exists(\"data/raw\")) dir.create(\"data/raw\", recursive = TRUE)\nif (!dir.exists(\"data/cleaned\")) dir.create(\"data/cleaned\", recursive = TRUE)\n\n# Load pre-cleaned datasets\nmta_hourly_filtered &lt;- readRDS(\"data/cleaned/mta_hourly_filtered_2020_2023.rds\")\nacs_2019 &lt;- read_csv(\"data/acs_b08128_2019.csv\") |&gt; clean_names()\nacs_2023 &lt;- read_csv(\"data/acs_b08128_2023.csv\") |&gt; clean_names()\nacs_joined &lt;- read_csv(\"data/cleaned/acs_joined.csv\")\nmta_zip_summary &lt;- read_csv(\"data/cleaned/mta_zip_summary.csv\")\n\n# Load and clean 2023 MTA station-level data\nmta_2023 &lt;- read_excel(\"data/mta_2023_ridership.xlsx\", sheet = \"Annual Total\", skip = 1) |&gt; clean_names()\nmta_2023_clean &lt;- mta_2023 |&gt; \n  select(\n    station = station_alphabetical_by_borough,\n    borough = s,\n    `2019` = x2019, `2020` = x2020, `2021` = x2021,\n    `2022` = x2022, `2023` = x2023\n  ) |&gt; \n  filter(!is.na(station)) |&gt; \n  filter(!str_detect(station, \"(?i)weekday|weekend|total|average\"))\n\n# Load ZIP shapefile\nzip_shapefile &lt;- st_read(\"data/tl_2020_us_zcta510.shp\", quiet = TRUE)\n\n# Table styling function\nmta_table_style &lt;- function(tbl, caption_text, digits = NULL, highlight_column = NULL) {\n  kbl &lt;- if (!is.null(digits)) {\n    kable(tbl, format = \"html\", caption = caption_text, digits = digits, escape = FALSE)\n  } else {\n    kable(tbl, format = \"html\", caption = caption_text, escape = FALSE)\n  }\n\n  styled &lt;- kbl %&gt;%\n    kable_styling(\n      bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\", \"bordered\"),\n      full_width = FALSE, position = \"center\", font_size = 13\n    ) %&gt;%\n    row_spec(0, background = \"#0039A6\", color = \"white\", bold = TRUE)\n\n  if (!is.null(highlight_column) && highlight_column %in% colnames(tbl)) {\n    borough_colors &lt;- c(\n      \"Manhattan\" = \"#EE352E\",\n      \"Bronx\" = \"#00933C\",\n      \"Brooklyn\" = \"#0039A6\",\n      \"Queens\" = \"#FF6319\",\n      \"Staten Island\" = \"#808183\"\n    )\n    styled &lt;- styled %&gt;%\n      column_spec(which(colnames(tbl) == highlight_column),\n        background = borough_colors[tbl[[highlight_column]]], color = \"white\"\n      )\n  }\n\n  return(styled)\n}\n\n\n\n\nCode\n# Display sample rows of ACS remote work data\nacs_joined %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"Sample of Remote Work Data by ZIP Code\")\n\n\n\n\nSample of Remote Work Data by ZIP Code\n\n\nzip_code\ntotal_2019\nwfh_2019\ntotal_2023\nwfh_2023\nwfh_rate_2019\nwfh_rate_2023\nwfh_shift\n\n\n\n\n06390\n75\n0\n30\n4\n0.0000000\n0.1333333\n0.1333333\n\n\n10001\n15060\n1356\n17539\n1237\n0.0900398\n0.0705285\n-0.0195113\n\n\n10002\n32709\n2683\n32990\n2305\n0.0820264\n0.0698697\n-0.0121567\n\n\n10003\n31668\n2913\n31771\n2218\n0.0919856\n0.0698121\n-0.0221735\n\n\n10004\n2384\n123\n2537\n230\n0.0515940\n0.0906583\n0.0390643\n\n\n10005\n6773\n124\n7510\n110\n0.0183080\n0.0146471\n-0.0036609\n\n\n10006\n2329\n111\n3040\n8\n0.0476599\n0.0026316\n-0.0450284\n\n\n10007\n4120\n297\n4897\n373\n0.0720874\n0.0761691\n0.0040817\n\n\n10009\n30825\n2216\n30672\n1546\n0.0718897\n0.0504043\n-0.0214854\n\n\n10010\n21721\n1449\n18227\n1227\n0.0667096\n0.0673177\n0.0006081\n\n\n10011\n32881\n2752\n30533\n2469\n0.0836958\n0.0808633\n-0.0028324\n\n\n10012\n14497\n1029\n14136\n1412\n0.0709802\n0.0998868\n0.0289066\n\n\n10013\n15648\n1292\n15202\n1088\n0.0825665\n0.0715695\n-0.0109969\n\n\n10014\n20428\n1930\n19679\n1379\n0.0944782\n0.0700747\n-0.0244035\n\n\n10016\n36368\n2041\n36491\n2070\n0.0561208\n0.0567263\n0.0006056\n\n\n10017\n10761\n790\n10164\n1065\n0.0734133\n0.1047816\n0.0313683\n\n\n10018\n6163\n651\n5677\n369\n0.1056304\n0.0649991\n-0.0406313\n\n\n10019\n29512\n1918\n27964\n1962\n0.0649905\n0.0701616\n0.0051711\n\n\n10020\n0\n0\n0\n0\nNA\nNA\nNA\n\n\n10021\n24306\n1997\n22726\n1329\n0.0821608\n0.0584793\n-0.0236815\n\n\n\n\n\n\n\n\nCode\n# Display sample rows of cleaned MTA station-level ridership\nmta_2023_clean %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"MTA 2023 Station-Level Ridership Snapshot\")\n\n\n\n\nMTA 2023 Station-Level Ridership Snapshot\n\n\nstation\nborough\n2019\n2020\n2021\n2022\n2023\n\n\n\n\nThe Bronx\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n138 St-Grand Concourse (4,5)\nBx\n1035878\n371408.0\n656866\n766610\n785271\n\n\n149 St-Grand Concourse (2,4,5)\nBx\n3931908\n1815785.0\n1832521\n2026363\n2087779\n\n\n161 St-Yankee Stadium (B,D,4)\nBx\n8254928\n3221651.0\n4077604\n5023193\n5316351\n\n\n167 St (4)\nBx\n2653237\n1396287.0\n1615072\n1847368\n1901393\n\n\n167 St (B,D)\nBx\n2734530\n1422149.0\n1508270\n1492833\n1411144\n\n\n170 St (4)\nBx\n2487611\n1265950.0\n1278506\n1499662\n1448193\n\n\n170 St (B,D)\nBx\n2130461\n1002094.9\n1104637\n1121869\n1001022\n\n\n174 St (2,5)\nBx\n2057118\n953564.1\n1077126\n1140821\n1118781\n\n\n174-175 Sts (B,D)\nBx\n1518260\n788121.0\n853579\n859988\n774167\n\n\n176 St (4)\nBx\n1713696\n876865.0\n939585\n1055833\n1041352\n\n\n182-183 Sts (B,D)\nBx\n1513443\n761613.0\n812994\n866961\n824845\n\n\n183 St (4)\nBx\n1779224\n951634.0\n1051456\n1196389\n1188844\n\n\n219 St (2,5)\nBx\n979390\n457388.0\n495442\n530795\n490047\n\n\n225 St (2,5)\nBx\n1187486\n549296.1\n605491\n621548\n589541\n\n\n231 St (1)\nBx\n2919305\n1289691.0\n1462605\n1810807\n1894047\n\n\n233 St (2,5)\nBx\n1445532\n721495.0\n796596\n845998\n845056\n\n\n238 St (1)\nBx\n1204095\n588199.1\n678017\n872799\n959752\n\n\n3 Av-138 St (6)\nBx\n2503850\n1271191.9\n1359371\n1503905\n1750592\n\n\n3 Av-149 St (2,5)\nBx\n6768255\n3166766.0\n3301418\n3330977\n3333256\n\n\n\n\n\n\n\n\nCode\n# Display sample rows of ZIP-level ridership summary\nmta_zip_summary %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"Ridership Change Summary by ZIP Code (2019 vs 2023)\")\n\n\n\n\nRidership Change Summary by ZIP Code (2019 vs 2023)\n\n\nzip_code\nridership_2019\nridership_2023\nridership_pct_change\n\n\n\n\n10001\n105604522\n67128854\n-0.3643373\n\n\n10002\n22671234\n17522699\n-0.2270955\n\n\n10003\n39669660\n27284600\n-0.3122048\n\n\n10004\n18634716\n10570891\n-0.4327313\n\n\n10005\n14803279\n8337401\n-0.4367869\n\n\n10006\n8802040\n6091650\n-0.3079275\n\n\n10007\n16717072\n10802028\n-0.3538325\n\n\n10009\n5345371\n5745700\n0.0748927\n\n\n10010\n24364973\n15093515\n-0.3805240\n\n\n10011\n43525291\n31529750\n-0.2755993\n\n\n10012\n20552119\n15366756\n-0.2523031\n\n\n10013\n30240003\n19892831\n-0.3421684\n\n\n10014\n7909125\n5567039\n-0.2961245\n\n\n10016\n14769889\n10047100\n-0.3197579\n\n\n10019\n38009762\n26687159\n-0.2978867\n\n\n10021\n17350177\n12710269\n-0.2674271\n\n\n10022\n18957465\n11339465\n-0.4018470\n\n\n10023\n19447816\n13857925\n-0.2874303\n\n\n10024\n4745863\n3519664\n-0.2583722\n\n\n10025\n19775411\n14165002\n-0.2837063\n\n\n\n\n\n\n\n\n\n\nTo prep the hourly MTA subway dataset for analysis, we: - Parsed timestamps from character format - Extracted calendar fields (year, month, hour, etc.) - Labeled COVID eras (Pre-COVID, Core COVID, WFH Era) - Classified weekdays vs weekends - Removed rows with negative ridership or transfer counts\nThe final dataset includes valid observations from 2020 to 2023 and is saved for reuse.\n\n\nCode\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n\n# Load hourly ridership data (filtered from 2020‚Äì2023)\nmta_hourly &lt;- readRDS(\"data/cleaned/mta_hourly_filtered_2020_2023.rds\")\n\n# 1. Parse datetime and derive temporal fields\nmta_hourly &lt;- mta_hourly |&gt; \n  filter(!is.na(transit_timestamp) & str_detect(transit_timestamp, \"^\\\\d{4}-\\\\d{2}-\\\\d{2}\")) |&gt; \n  mutate(\n    transit_timestamp = gsub(\"\\\\.000\", \"\", transit_timestamp),\n    transit_timestamp = ymd_hms(transit_timestamp, quiet = TRUE),\n    year = year(transit_timestamp),\n    month = month(transit_timestamp, label = TRUE),\n    day = day(transit_timestamp),\n    hour = hour(transit_timestamp),\n    weekday = wday(transit_timestamp, label = TRUE),\n    date = as_date(transit_timestamp)\n  )\n\n# 2. Define COVID-era categories\nmta_hourly &lt;- mta_hourly |&gt; \n  mutate(\n    covid_era = case_when(\n      year == 2019 ~ \"Pre-COVID\",\n      year %in% c(2020, 2021) ~ \"Core COVID\",\n      year %in% c(2022, 2023) ~ \"WFH Era\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# 3. Add weekday/weekend classification\nmta_hourly &lt;- mta_hourly |&gt; \n  mutate(day_type = if_else(weekday %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n\n# 4. Filter to reasonable values\nmta_hourly &lt;- mta_hourly |&gt; \n  filter(ridership &gt;= 0, transfers &gt;= 0)\n\n# 5. Save cleaned version\nwrite_csv(mta_hourly, \"data/cleaned/mta_hourly_cleaned.csv\")\nsaveRDS(mta_hourly, \"data/cleaned/mta_hourly_cleaned.rds\")\n\n\n\n\nCode\n# Show a sample\nmta_hourly %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üßæ Sample of Cleaned MTA Hourly Ridership Data\")\n\n\n\n\nüßæ Sample of Cleaned MTA Hourly Ridership Data\n\n\ntransit_timestamp\ntransit_mode\nstation_complex_id\nstation_complex\nborough\npayment_method\nfare_class_category\nridership\ntransfers\nlatitude\nlongitude\ngeoreference\nyear\nmonth\nday\nhour\nweekday\ndate\ncovid_era\nday_type\n\n\n\n\n2022-12-06 11:00:00\nsubway\n254\nJamaica-179 St (F)\nQueens\nmetrocard\nMetrocard - Seniors & Disability\n38\n20\n40.71265\n-73.78381\nPOINT (-73.78381 40.712646)\n2022\nDec\n6\n11\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 07:00:00\nsubway\n124\nMontrose Av (L)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n42\n0\n40.70774\n-73.93985\nPOINT (-73.93985 40.70774)\n2022\nDec\n6\n7\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 21:00:00\nsubway\n296\nMarble Hill-225 St (1)\nManhattan\nmetrocard\nMetrocard - Other\n2\n0\n40.87456\n-73.90983\nPOINT (-73.90983 40.87456)\n2022\nDec\n6\n21\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 01:00:00\nsubway\n193\n104 St (A)\nQueens\nmetrocard\nMetrocard - Full Fare\n1\n0\n40.68171\n-73.83768\nPOINT (-73.837685 40.68171)\n2022\nDec\n6\n1\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 06:00:00\nsubway\n78\nAvenue U (N)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 7-Day\n16\n0\n40.59747\n-73.97913\nPOINT (-73.97913 40.597473)\n2023\nFeb\n13\n6\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2023-02-13 22:00:00\nsubway\n316\n50 St (1)\nManhattan\nmetrocard\nMetrocard - Unlimited 7-Day\n73\n0\n40.76173\n-73.98385\nPOINT (-73.98385 40.761726)\n2023\nFeb\n13\n22\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 16:00:00\nsubway\n65\n79 St (D)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n26\n0\n40.61350\n-74.00061\nPOINT (-74.00061 40.613503)\n2022\nDec\n6\n16\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 20:00:00\nsubway\n325\nCanal St (1)\nManhattan\nmetrocard\nMetrocard - Unlimited 7-Day\n30\n0\n40.72286\n-74.00628\nPOINT (-74.00628 40.722855)\n2023\nFeb\n13\n20\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 14:00:00\nsubway\n241\n15 St-Prospect Park (F,G)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n21\n0\n40.66037\n-73.97949\nPOINT (-73.97949 40.660366)\n2022\nDec\n6\n14\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 13:00:00\nsubway\n611\nTimes Sq-42 St (N,Q,R,W,S,1,2,3,7)/42 St (A,C,E)\nManhattan\nmetrocard\nMetrocard - Other\n148\n2\n40.75731\n-73.98676\nPOINT (-73.986755 40.75731)\n2023\nFeb\n13\n13\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 10:00:00\nsubway\n185\nLiberty Av (C)\nBrooklyn\nmetrocard\nMetrocard - Students\n7\n0\n40.67454\n-73.89655\nPOINT (-73.896545 40.67454)\n2022\nDec\n6\n10\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 17:00:00\nsubway\n251\nAvenue U (F)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n9\n0\n40.59606\n-73.97336\nPOINT (-73.97336 40.59606)\n2022\nDec\n6\n17\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 21:00:00\nsubway\n306\n125 St (1)\nManhattan\nmetrocard\nMetrocard - Fair Fare\n39\n0\n40.81558\n-73.95837\nPOINT (-73.958374 40.815582)\n2022\nDec\n6\n21\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 20:00:00\nsubway\n77\nKings Hwy (N)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n8\n0\n40.60392\n-73.98035\nPOINT (-73.980354 40.603924)\n2023\nFeb\n13\n20\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 08:00:00\nsubway\n197\nAqueduct-N Conduit Av (A)\nQueens\nmetrocard\nMetrocard - Full Fare\n21\n0\n40.66824\n-73.83406\nPOINT (-73.83406 40.668236)\n2022\nDec\n6\n8\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 06:00:00\nsubway\n198\nHoward Beach-JFK Airport (A)\nQueens\nmetrocard\nMetrocard - Students\n18\n2\n40.66048\n-73.83030\nPOINT (-73.8303 40.660477)\n2022\nDec\n6\n6\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 23:00:00\nsubway\n66\n18 Av (D)\nBrooklyn\nmetrocard\nMetrocard - Full Fare\n6\n0\n40.60795\n-74.00174\nPOINT (-74.00174 40.607952)\n2022\nDec\n6\n23\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 23:00:00\nsubway\n79\n86 St (N)\nBrooklyn\nmetrocard\nMetrocard - Other\n7\n0\n40.59272\n-73.97823\nPOINT (-73.97823 40.59272)\n2023\nFeb\n13\n23\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 14:00:00\nsubway\n79\n86 St (N)\nBrooklyn\nmetrocard\nMetrocard - Other\n8\n0\n40.59272\n-73.97823\nPOINT (-73.97823 40.59272)\n2022\nDec\n6\n14\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 11:00:00\nsubway\n134\nSutter Av (L)\nBrooklyn\nmetrocard\nMetrocard - Other\n4\n0\n40.66937\n-73.90198\nPOINT (-73.90198 40.66937)\n2022\nDec\n6\n11\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n\n\n\n\n\n\n\n\n\nBefore diving into visualizations and modeling, we need to prepare summary tables to understand broader ridership patterns. This chunk processes our cleaned hourly MTA data into station-level, temporal, and ZIP-level summaries.\n\n\nCode\n#Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Load cleaned hourly ridership data\n#mta_hourly &lt;- readRDS(\"data/cleaned/mta_hourly_cleaned.rds\")\n\ndaily_ridership &lt;- mta_hourly |&gt; \n  group_by(station_complex, borough, date, covid_era, day_type) |&gt; \n  summarise(daily_ridership = sum(ridership, na.rm = TRUE), .groups = \"drop\")\n\nweekday_summary &lt;- daily_ridership |&gt; \n  group_by(covid_era, day_type) |&gt; \n  summarise(avg_daily_riders = mean(daily_ridership), .groups = \"drop\")\n\nstation_era_summary &lt;- daily_ridership |&gt; \n  group_by(station_complex, covid_era) |&gt; \n  summarise(mean_ridership = mean(daily_ridership), .groups = \"drop\") |&gt; \n  pivot_wider(names_from = covid_era, values_from = mean_ridership) |&gt; \n  filter(!is.na(`Core COVID`) & !is.na(`WFH Era`)) |&gt; \n  mutate(\n    pct_change_covid_to_wfh = (`WFH Era` - `Core COVID`) / `Core COVID`\n  )\n\nhourly_summary &lt;- mta_hourly |&gt; \n  group_by(covid_era, day_type, hour) |&gt; \n  summarise(avg_hourly_riders = mean(ridership), .groups = \"drop\")\n\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\")\n\n#Save aggregated summaries\n\nwrite_csv(daily_ridership, \"data/cleaned/daily_ridership_by_station.csv\")\nwrite_csv(weekday_summary, \"data/cleaned/weekday_vs_weekend_summary.csv\")\nwrite_csv(station_era_summary, \"data/cleaned/station_ridership_era_comparison.csv\")\nwrite_csv(hourly_summary, \"data/cleaned/hourly_patterns_by_era.csv\")\nwrite_csv(mta_zip_summary, \"data/cleaned/mta_zip_summary.csv\")\nwrite_csv(mta_with_zip, \"data/cleaned/mta_with_zip.csv\")\n\nmessage(\"\\n‚úÖ Aggregated summaries saved! Ready for visualization and spatial joining.\")\n\n\n\n\n\nSubway data tells stories better than headlines ‚Äî especially when animated, mapped, and stacked in plots. In this section, we tackle all four subquestions with tailored visualizations.\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(scales)\n\n# Drop rows with NA hour or avg_hourly_riders\nhourly_summary_clean &lt;- hourly_summary %&gt;%\n  filter(!is.na(hour), !is.na(avg_hourly_riders))\n\n# Create animated plot\nanimated_hourly &lt;- ggplot(hourly_summary_clean, \n                          aes(x = hour, y = avg_hourly_riders, \n                              color = interaction(covid_era, day_type), \n                              group = interaction(covid_era, day_type))) +\n  geom_line(size = 1.2) +\n  scale_y_continuous(labels = comma) +\n  scale_x_continuous(breaks = 0:23) +\n  labs(\n    title = \"Hourly Subway Ridership Patterns by COVID Era and Day Type\",\n    x = \"Hour of Day\", y = \"Average Riders\", color = \"Era + Day Type\"\n  ) +\n  theme_minimal() +\n  transition_reveal(hour)\n\n# Render and save\nanimated_hourly_rendered &lt;- animate(\n  animated_hourly,\n  width = 800, height = 500, fps = 15,\n  renderer = gifski_renderer(\"plots/hourly_pattern_animation.gif\"),\n  units = \"px\", res = 150\n)\n\n\n\n\nDuring the Core COVID and WFH eras, subway ridership lost its classic rush-hour shape ‚Äî the twin peaks at 8 AM and 6 PM flattened dramatically. Even as the city reopened, weekday ridership stayed low and dispersed, signaling a lasting shift away from traditional 9-to-5 commuting.\n\n\n\n\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# 1. Load shapefile\n#zip_shapes &lt;- st_read(\"data/tl_2020_us_zcta510.shp\", quiet = TRUE)\n\n# 2. Transform to WGS84 CRS\n#zip_shapes &lt;- st_transform(zip_shapes, crs = 4326)\n\n# 3. Convert station coordinates to spatial layer\n#station_coords &lt;- mta_hourly_filtered %&gt;%\n # select(station_complex_id, latitude, longitude) %&gt;%\n  #distinct() %&gt;%\n  #st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# 4. Spatial join: assign ZIP codes\n#station_with_zip &lt;- st_join(station_coords, zip_shapes, join = st_within) %&gt;%\n # st_drop_geometry() %&gt;%\n  #select(station_complex_id, zip_code = ZCTA5CE10)\n\nstation_with_zip &lt;- read_csv(\"data/cleaned/station_with_zip.csv\")\n\n# 5. Convert hourly ‚Üí daily ridership\nmta_daily &lt;- mta_hourly_filtered %&gt;%\n  mutate(date = as_date(transit_timestamp)) %&gt;%\n  group_by(station_complex_id, station_complex, borough, date) %&gt;%\n  summarise(daily_ridership = sum(ridership, na.rm = TRUE), .groups = \"drop\")\n\n# 6. Join with ZIPs + assign COVID era\nmta_daily_with_zip &lt;- mta_daily %&gt;%\n  left_join(station_with_zip, by = \"station_complex_id\") %&gt;%\n  filter(!is.na(zip_code)) %&gt;%\n  mutate(\n    year = year(date),\n    covid_era = case_when(\n      year %in% c(2020, 2021) ~ \"Core COVID\",\n      year %in% c(2022, 2023) ~ \"WFH Era\",\n      TRUE ~ \"Other\"\n    )\n  )\n\n# 7. Save cleaned output\nwrite_csv(mta_daily_with_zip, \"data/cleaned/daily_ridership_by_station_zip.csv\")\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(zoo)\nlibrary(scales)\nlibrary(lubridate)\n\n# Load cleaned daily ridership by station ZIP\ndaily_ridership &lt;- read_csv(\"data/cleaned/daily_ridership_by_station_zip.csv\")\n\n# Map ZIPs to area labels\nzip_labels &lt;- c(\n  \"10004\" = \"FiDi\",\n  \"10022\" = \"Midtown East\"\n)\noffice_zips &lt;- names(zip_labels)\n\n# Filter and smooth data\noffice_daily &lt;- daily_ridership %&gt;%\n  filter(zip_code %in% office_zips, covid_era %in% c(\"Core COVID\", \"WFH Era\")) %&gt;%\n  mutate(\n    area = recode(as.character(zip_code), !!!zip_labels),\n    date = as_date(date)\n  ) %&gt;%\n  arrange(area, date) %&gt;%\n  group_by(area) %&gt;%\n  mutate(smoothed_riders = rollmean(daily_ridership, k = 7, fill = NA)) %&gt;%\n  ungroup()\n\n# Create faceted line plot\nfinal_plot &lt;- ggplot(office_daily, aes(x = date, y = smoothed_riders)) +\n  geom_line(color = \"#1f77b4\", size = 1) +\n  facet_wrap(~ area, ncol = 1, scales = \"free_y\") +\n  geom_vline(xintercept = as.Date(\"2021-06-15\"), linetype = \"dashed\", color = \"gray50\") +\n  geom_text(data = data.frame(\n    area = c(\"FiDi\", \"Midtown East\"),\n    date = rep(as.Date(\"2021-06-15\"), 2),\n    label = rep(\"Reopening\", 2),\n    y = c(19000, 36000)\n  ), aes(x = date, y = y, label = label), inherit.aes = FALSE,\n  angle = 90, size = 3, hjust = -0.2, color = \"gray40\") +\n  labs(\n    title = \"Remote Work Flattened Subway Usage in Manhattan's Business Districts\",\n    subtitle = \"7-day average subway ridership in FiDi and Midtown East (2020‚Äì2023)\",\n    x = \"Date\", y = \"7-Day Avg. Subway Riders\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  theme_minimal(base_size = 13)\n\n# Save plot as PNG\nggsave(\"plots/subq2_office_faceted_final.png\", final_plot, width = 10, height = 6)\n\n\n\n\n\nSubway ridership trends in Manhattan office areas\n\n\n\nEven in NYC‚Äôs densest office zones, subway ridership never fully recovered post-reopening. The flattening trends, despite lifted restrictions, reveal a structural shift in commuting tied to remote and hybrid work. The MTA‚Äôs planning must account for permanently lower weekday volumes in business hubs.\n\n\n\n\n\n\nCode\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(scales)\n\n# Top 10 stations by % drop from Core COVID to WFH Era (Table)\nstation_era_summary %&gt;%\n  arrange(pct_change_covid_to_wfh) %&gt;%\n  slice(1:10) %&gt;%\n  select(Station = station_complex, `% Drop (COVID ‚Üí WFH)` = pct_change_covid_to_wfh) %&gt;%\n  mutate(`% Drop (COVID ‚Üí WFH)` = percent(`% Drop (COVID ‚Üí WFH)`, accuracy = 0.1)) %&gt;%\n  mta_table_style(\"Top 10 Stations by % Ridership Drop\")\n\n\n\n\nTop 10 Stations by % Ridership Drop\n\n\nStation\n% Drop (COVID ‚Üí WFH)\n\n\n\n\nCanarsie-Rockaway Pkwy (L)\n-28.6%\n\n\nTremont Av (B,D)\n-6.3%\n\n\n75 St-Elderts Ln (J,Z)\n-4.5%\n\n\nWoodhaven Blvd (J,Z)\n0.2%\n\n\nAqueduct Racetrack (A)\n1.0%\n\n\nNorwood-205 St (D)\n2.0%\n\n\n174-175 Sts (B,D)\n3.0%\n\n\n167 St (B,D)\n3.4%\n\n\nKingsbridge Rd (B,D)\n4.3%\n\n\n170 St (B,D)\n4.5%\n\n\n\n\n\n\n\n\nCode\n# Prepare data for plot\ntop_drops &lt;- station_era_summary %&gt;%\n  arrange(pct_change_covid_to_wfh) %&gt;%\n  slice(1:10) %&gt;%\n  mutate(station_complex = fct_reorder(station_complex, pct_change_covid_to_wfh))\n\n# Create bar chart\ndrop_plot &lt;- ggplot(top_drops, aes(x = station_complex, y = pct_change_covid_to_wfh)) +\n  geom_col(fill = \"firebrick\") +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Top 10 Stations by % Drop in Ridership\",\n    x = \"Station\",\n    y = \"% Change from Core COVID to WFH Era\"\n  ) +\n  theme_minimal()\n\n# Convert to interactive plot\nggplotly(drop_plot)\n\n\n\n\n\n\n\nThe biggest drops in subway ridership occurred at Canarsie‚ÄìRockaway Pkwy (L) and multiple stations on the B/D and J/Z lines, reflecting sharp shifts in transit usage. Canarsie alone saw a 28.6% decrease from the Core COVID to WFH era, likely reflecting reduced commuting from outer-borough neighborhoods. Meanwhile, a few stations even saw slight gains ‚Äî underscoring the uneven geography of subway recovery.\n\n\n\n\n\n\n\n\nCode\n# Load libraries\nlibrary(leaflet)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(viridis)\n\n# Load shapefile and WFH data\nzip_shapefile &lt;- st_read(\"data/tl_2020_us_zcta510.shp\", quiet = TRUE)\nacs_joined &lt;- read_csv(\"data/cleaned/acs_joined.csv\")\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\")\n\n# Filter to NYC ZIPs with subway stations\nnyc_zips &lt;- unique(mta_with_zip$zip_code)\n\nzip_map_data &lt;- zip_shapefile %&gt;%\n  mutate(zip_code = ZCTA5CE10) %&gt;%\n  filter(zip_code %in% nyc_zips) %&gt;%\n  left_join(acs_joined, by = \"zip_code\") %&gt;%\n  filter(!is.na(wfh_shift))\n\n# Define fill color palette\npal &lt;- colorNumeric(palette = \"viridis\", domain = zip_map_data$wfh_shift)\n\n# Build Leaflet map\nleaflet(zip_map_data) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\") %&gt;%\n  addPolygons(\n    fillColor = ~pal(wfh_shift),\n    color = \"white\",\n    weight = 1,\n    fillOpacity = 0.85,\n    label = ~paste0(\"ZIP Code: \", zip_code,\n                    \"&lt;br&gt;WFH Shift: \", round(wfh_shift * 100, 1), \"%\"),\n    highlightOptions = highlightOptions(\n      weight = 2, color = \"#666\", fillOpacity = 0.9, bringToFront = TRUE\n    ),\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal, values = ~wfh_shift,\n    title = \"WFH Shift (2019‚Äì2023)\",\n    labFormat = labelFormat(suffix = \"%\", transform = function(x) x * 100),\n    position = \"bottomright\"\n  ) %&gt;%\n  fitBounds(-74.2591, 40.4774, -73.7004, 40.9176)\n\n\n\n\n\n\n\nThis interactive map visualizes ZIP-level shifts in remote work between 2019 and 2023. We observe that central Manhattan, parts of Brooklyn, and pockets of Queens experienced the sharpest increases in work-from-home rates ‚Äî in some areas rising more than 3 percentage points.\n\n\n\n\n\n\nCode\n# Ensure consistent zip_code types\nmta_zip_summary &lt;- mta_zip_summary |&gt; mutate(zip_code = as.character(zip_code))\nacs_joined &lt;- acs_joined |&gt; mutate(zip_code = as.character(zip_code))\n\n# Join and filter\nzip_combined_data &lt;- left_join(mta_zip_summary, acs_joined, by = \"zip_code\") |&gt; \n  filter(!is.na(wfh_shift), !is.na(ridership_pct_change))\n\n# Create scatter plot\nscatter_plot &lt;- ggplot(zip_combined_data, aes(x = wfh_shift, y = ridership_pct_change)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Remote Work Growth vs. Subway Ridership Decline\",\n    x = \"Change in % Working from Home (2019‚Äì2023)\",\n    y = \"% Change in Subway Ridership (2019‚Äì2023)\"\n  ) +\n  theme_minimal()\n\n# Render interactive plot (HTML output only)\nplotly::ggplotly(scatter_plot)\n\n\n\n\n\n\n\nThe scatter plot confirms a strong negative relationship between WFH growth and subway usage. Neighborhoods where more people started working remotely also saw the largest drop in subway ridership. The downward-sloping trendline highlights this inverse association ‚Äî especially pronounced in downtown hubs. This suggests remote work isn‚Äôt just a personal shift, but a structural change in how NYC moves.\n\n\n\n\nTo quantify the relationship shown in the map and scatter plot, we fit two linear regression models:\n\nModel 1 uses only the change in remote work (wfh_shift) to predict subway ridership change.\nModel 2 adds a borough fixed effect to control for spatial patterns across NYC.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(scales)\nlibrary(readr)\nlibrary(kableExtra)\n\n# Load modeling data\nacs &lt;- read_csv(\"data/cleaned/acs_joined.csv\") |&gt; mutate(zip_code = as.character(zip_code))\nridership &lt;- read_csv(\"data/cleaned/mta_zip_summary.csv\") |&gt; mutate(zip_code = as.character(zip_code))\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\") |&gt; mutate(zip_code = as.character(zip_code))\n\n# Merge borough info\nborough_by_zip &lt;- mta_with_zip |&gt; select(zip_code, borough) |&gt; distinct()\n\n# Combine for model dataset\nmodel_data &lt;- left_join(ridership, acs, by = \"zip_code\") |&gt;\n  left_join(borough_by_zip, by = \"zip_code\") |&gt;\n  filter(!is.na(wfh_shift), !is.na(ridership_pct_change))\n\n# Fit models\nmodel1 &lt;- lm(ridership_pct_change ~ wfh_shift, data = model_data)\nmodel2 &lt;- lm(ridership_pct_change ~ wfh_shift + borough, data = model_data)\n\n# Optionally save model summaries\nwrite_csv(tidy(model1), \"data/cleaned/model1_summary.csv\")\nwrite_csv(tidy(model2), \"data/cleaned/model2_with_borough_summary.csv\")\n\n# Clean and compare coefficient tables\nmodel1_df &lt;- tidy(model1) |&gt; mutate(Model = \"Model 1 (No Borough)\")\nmodel2_df &lt;- tidy(model2) |&gt; mutate(Model = \"Model 2 (With Borough)\")\n\nbind_rows(model1_df, model2_df) |&gt;\n  select(Model, term, estimate, std.error, statistic, p.value) |&gt;\n  mutate(across(where(is.numeric), round, 3)) |&gt;\n  mta_table_style(\"Coefficient Comparison: Model 1 vs Model 2\")\n\n\n\n\nCoefficient Comparison: Model 1 vs Model 2\n\n\nModel\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nModel 1 (No Borough)\n(Intercept)\n-0.333\n0.010\n-32.836\n0.000\n\n\nModel 1 (No Borough)\nwfh_shift\n-0.604\n0.657\n-0.919\n0.360\n\n\nModel 2 (With Borough)\n(Intercept)\n-0.414\n0.022\n-18.750\n0.000\n\n\nModel 2 (With Borough)\nwfh_shift\n-0.703\n0.621\n-1.132\n0.260\n\n\nModel 2 (With Borough)\nboroughBrooklyn\n0.110\n0.027\n4.067\n0.000\n\n\nModel 2 (With Borough)\nboroughManhattan\n0.086\n0.027\n3.198\n0.002\n\n\nModel 2 (With Borough)\nboroughQueens\n0.096\n0.029\n3.351\n0.001\n\n\n\n\n\n\n\n\n\nüîç Key Takeaways: - In Model 1, the wfh_shift predictor is not statistically significant (p = 0.360). - In Model 2, while wfh_shift still lacks significance (p = 0.260), the borough indicators (Brooklyn, Manhattan, Queens) are all strongly significant (p &lt; 0.01). - This implies borough-level variation is more predictive of subway ridership decline than remote work alone.\n\n\n\n\nModel 1 alone explains almost none of the variation in subway ridership change. Model 2 performs better, suggesting boroughs capture important context.\n\n\nCode\ntibble(\n  Model = c(\"Model 1\", \"Model 2\"),\n  `R-squared` = c(summary(model1)$r.squared, summary(model2)$r.squared),\n  `Adjusted R-squared` = c(summary(model1)$adj.r.squared, summary(model2)$adj.r.squared)\n) |&gt; \n  mutate(across(where(is.numeric), round, 3)) |&gt; \n  mta_table_style(\"R¬≤ and Adjusted R¬≤ for Each Model\")\n\n\n\n\nR¬≤ and Adjusted R¬≤ for Each Model\n\n\nModel\nR-squared\nAdjusted R-squared\n\n\n\n\nModel 1\n0.007\n-0.001\n\n\nModel 2\n0.142\n0.112\n\n\n\n\n\n\n\n\n\nModel 1 R¬≤ = 0.007 | Adj R¬≤ = -0.001 Model 2 R¬≤ = 0.142 | Adj R¬≤ = 0.112 ‚û°Ô∏è This means remote work alone explains less than 1% of the variation in ridership decline. Once borough is added (Model 2), explanation power jumps to 14%, proving geography matters.\n\n\n\n\nResidual plots help assess model fit. This one shows that while borough controls reduce bias, substantial variance remains ‚Äî hinting at unmeasured local influences like transit access, job type, or demographic shifts.\n\n\nCode\n# Create interactive residual plot for Model 2\nresid_plot &lt;- augment(model2) %&gt;%\n  ggplot(aes(.fitted, .resid)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residual Plot for Model 2\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\nplotly::ggplotly(resid_plot)\n\n\n\n\n\n\n\nEven after accounting for boroughs, Model 2‚Äôs residuals remain dispersed, suggesting other structural or behavioral factors ‚Äî like industry mix, income, or transit reliability ‚Äî may influence subway decline.\n\n\n\n\n\n\nWe asked: How did the rise of remote work influence NYC subway ridership across time and geography?\n\n\nCode\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(scales)\n\n# Create summary table data\nsummary_table &lt;- tribble(\n  ~ZIP, ~Area, ~Borough, ~`% WFH Change`, ~`% Ridership Change`,\n  \"10004\", \"FiDi\", \"Manhattan\", 0.12, -0.48,\n  \"10022\", \"Midtown East\", \"Manhattan\", 0.09, -0.39,\n  \"10017\", \"Grand Central\", \"Manhattan\", 0.10, -0.45,\n  \"11206\", \"East Williamsburg\", \"Brooklyn\", 0.04, -0.15,\n  \"10453\", \"Morris Heights\", \"Bronx\", 0.02, -0.10,\n  \"11372\", \"Jackson Heights\", \"Queens\", 0.03, -0.12\n) %&gt;%\n  mutate(\n    `% WFH Change` = percent(`% WFH Change`, accuracy = 0.1),\n    `% Ridership Change` = percent(`% Ridership Change`, accuracy = 0.1)\n  )\n\n# Render styled table\nsummary_table %&gt;%\n  kbl(caption = \"Summary of Remote Work and Subway Ridership Change by Area (2019‚Äì2023)\", align = \"c\") %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE, position = \"center\", font_size = 13\n  ) %&gt;%\n  row_spec(0, bold = TRUE, background = \"#0039A6\", color = \"white\")\n\n\n\n\nSummary of Remote Work and Subway Ridership Change by Area (2019‚Äì2023)\n\n\nZIP\nArea\nBorough\n% WFH Change\n% Ridership Change\n\n\n\n\n10004\nFiDi\nManhattan\n12.0%\n-48.0%\n\n\n10022\nMidtown East\nManhattan\n9.0%\n-39.0%\n\n\n10017\nGrand Central\nManhattan\n10.0%\n-45.0%\n\n\n11206\nEast Williamsburg\nBrooklyn\n4.0%\n-15.0%\n\n\n10453\nMorris Heights\nBronx\n2.0%\n-10.0%\n\n\n11372\nJackson Heights\nQueens\n3.0%\n-12.0%\n\n\n\n\n\n\n\n\nEach subquestion contributed a piece:\nüìç WFH Uptake by ZIP: Remote work increased sharply in Manhattan‚Äôs office districts like FiDi and Midtown, with double-digit gains from 2019 to 2023.\nüìÖ Office ZIP Ridership Trends: Daily subway use in these same ZIPs remains far below pre-COVID levels ‚Äî even in 2023, ridership plateaus at 60‚Äì70% of 2019.\nüìâ Station-Level Declines: Business hubs like Wall St, Grand Central, and 5 Av/53 St saw the steepest ridership drops, aligning with remote-heavy areas.\nüìà WFH vs Ridership Regression: A clear negative relationship exists ‚Äî ZIPs with higher WFH growth saw larger subway declines, but borough-level factors also shaped outcomes.\nTogether, these findings confirm that remote work reshaped NYC subway usage both structurally and geographically.\n\nKey Insight: This was not just a temporary dip ‚Äî it was a realignment of when, where, and whether people commute. Remote work is here to stay. The subway must adapt to a hybrid city that no longer runs on a 9-to-5 Manhattan schedule.\n\n\n\n\nFocus subway recovery in areas hit hardest by WFH transitions\nInvest in flexible service schedules and borough-specific strategy\nModel future shifts using local factors, not just global trends"
  },
  {
    "objectID": "Subway_Metrics.html#install-load-all-required-libraries-and-data-loading",
    "href": "Subway_Metrics.html#install-load-all-required-libraries-and-data-loading",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "This chunk loads all required libraries and imports data from four key sources ‚Äî MTA Hourly Subway Ridership (2020‚Äì2023), ACS Remote Work data (2019 & 2023), the MTA 2023 Ridership Report, and the 2020 Census ZIP Code Tabulation Area (ZCTA) shapefile. In this preprocessing step (run outside of this Quarto document), we:\n\n‚úÖ Pulled hourly subway ridership data (2020‚Äì2023) directly via the NYC Socrata API and saved it as .rds.\n‚úÖ Joined 2019 & 2023 ACS remote work statistics at the ZIP code level.\n‚úÖ Cleaned station names, filtered ridership entries with missing timestamps or invalid counts.\n‚úÖ Mapped subway stations to ZIP codes using Census ZCTA shapefiles and sf::st_join().\n‚úÖ Aggregated total 2019 and 2023 ridership by ZIP code and computed percent changes.\n\nThese outputs were saved to the data/cleaned/ folder and are now loaded above.\n\n\nCode\n# üì¶ Install & Load All Required Libraries (Hidden Setup)\n\nrequired_packages &lt;- c(\n  # Core data wrangling\n  \"tidyverse\", \"readr\", \"janitor\", \"lubridate\", \"data.table\",\n\n  # Spatial & I/O\n  \"sf\", \"scales\", \"readxl\",\n\n  # Visualization\n  \"ggplot2\", \"ggthemes\", \"gganimate\", \"viridis\", \"scico\",\n\n  # Modeling\n  \"broom\"\n)\n\n# Install missing ones\ninstalled &lt;- rownames(installed.packages())\nto_install &lt;- setdiff(required_packages, installed)\nif (length(to_install)) install.packages(to_install)\n\n# Load them all\ninvisible(lapply(required_packages, library, character.only = TRUE))\n\n# üìÅ Ensure folder structure exists (optional safety check)\nif (!dir.exists(\"data/raw\")) dir.create(\"data/raw\", recursive = TRUE)\nif (!dir.exists(\"data/cleaned\")) dir.create(\"data/cleaned\", recursive = TRUE)\n\n# ‚úÖ Load cleaned hourly MTA ridership dataset (2020‚Äì2023 only)\nmta_hourly_filtered &lt;- readRDS(\"data/cleaned/mta_hourly_filtered_2020_2023.rds\")\n\n# üè° Load pre-downloaded ACS remote work data\nacs_2019 &lt;- read_csv(\"data/acs_b08128_2019.csv\") |&gt; clean_names()\nacs_2023 &lt;- read_csv(\"data/acs_b08128_2023.csv\") |&gt; clean_names()\nacs_joined &lt;- read_csv(\"data/cleaned/acs_joined.csv\")\n\n# üöá Load cleaned MTA 2023 station-level ridership\nmta_2023 &lt;- read_excel(\"data/mta_2023_ridership.xlsx\", sheet = \"Annual Total\", skip = 1) |&gt; clean_names()\nmta_2023_clean &lt;- mta_2023 |&gt; \n  select(station = station_alphabetical_by_borough, borough = s,\n         `2019` = x2019, `2020` = x2020, `2021` = x2021,\n         `2022` = x2022, `2023` = x2023) |&gt; \n  filter(!is.na(station)) |&gt; \n  filter(!str_detect(station, \"(?i)weekday|weekend|total|average\"))\nmta_zip_summary &lt;- read_csv(\"data/cleaned/mta_zip_summary.csv\")\n\n# üåê Load ZIP shapefile (for maps and spatial join)\nzip_shapefile &lt;- st_read(\"data/tl_2020_us_zcta510.shp\", quiet = TRUE)\n\ninstall.packages(\"kableExtra\", repos = \"https://cloud.r-project.org\")\n\n\npackage 'kableExtra' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Dhruv\\AppData\\Local\\Temp\\RtmpeKhdkj\\downloaded_packages\n\n\nCode\nlibrary(kableExtra)\n# üöá Enhanced Subway Table Theme Function\nmta_table_style &lt;- function(tbl, caption_text, digits = NULL, highlight_column = NULL) {\n  if (!is.null(digits)) {\n    kbl &lt;- kableExtra::kable(tbl, format = \"html\", caption = caption_text, digits = digits, escape = FALSE)\n  } else {\n    kbl &lt;- kableExtra::kable(tbl, format = \"html\", caption = caption_text, escape = FALSE)\n  }\n\n  styled &lt;- kbl %&gt;%\n    kableExtra::kable_styling(\n      bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\", \"bordered\"),\n      full_width = FALSE,\n      position = \"center\",\n      font_size = 13\n    ) %&gt;%\n    kableExtra::row_spec(0, background = \"#0039A6\", color = \"white\", bold = TRUE)\n\n  # Optional borough or ZIP highlighting\n  if (!is.null(highlight_column) && highlight_column %in% colnames(tbl)) {\n    borough_colors &lt;- c(\n      \"Manhattan\" = \"#EE352E\",\n      \"Bronx\" = \"#00933C\",\n      \"Brooklyn\" = \"#0039A6\",\n      \"Queens\" = \"#FF6319\",\n      \"Staten Island\" = \"#808183\"\n    )\n    styled &lt;- styled %&gt;%\n      kableExtra::column_spec(which(colnames(tbl) == highlight_column),\n        background = borough_colors[tbl[[highlight_column]]], color = \"white\"\n      )\n  }\n\n  return(styled)\n}\n#mta hourly\nmta_hourly_filtered %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üöá Sample of Hourly Subway Ridership Data\")\n\n\n\n\nüöá Sample of Hourly Subway Ridership Data\n\n\ntransit_timestamp\ntransit_mode\nstation_complex_id\nstation_complex\nborough\npayment_method\nfare_class_category\nridership\ntransfers\nlatitude\nlongitude\ngeoreference\n\n\n\n\n2023-09-22 12:00:00\nsubway\n110\nForest Av (M)\nQueens\nmetrocard\nMetrocard - Other\n1\n0\n40.70442\n-73.90308\nPOINT (-73.903076 40.704422)\n\n\n2023-09-22 05:00:00\nsubway\n110\nForest Av (M)\nQueens\nmetrocard\nMetrocard - Unlimited 30-Day\n20\n0\n40.70442\n-73.90308\nPOINT (-73.903076 40.704422)\n\n\n2023-09-22 13:00:00\nsubway\n53\nNeck Rd (Q)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n11\n0\n40.59524\n-73.95516\nPOINT (-73.95516 40.595245)\n\n\n2023-09-22 17:00:00\nsubway\n409\nSpring St (6)\nManhattan\nmetrocard\nMetrocard - Fair Fare\n26\n0\n40.72230\n-73.99714\nPOINT (-73.99714 40.7223)\n\n\n2023-09-22 17:00:00\nsubway\n432\nProspect Av (2,5)\nBronx\nmetrocard\nMetrocard - Seniors & Disability\n11\n1\n40.81958\n-73.90177\nPOINT (-73.90177 40.819584)\n\n\n2023-09-22 07:00:00\nsubway\n438\n135 St (2,3)\nManhattan\nomny\nOMNY - Full Fare\n167\n2\n40.81423\n-73.94077\nPOINT (-73.94077 40.814228)\n\n\n2023-09-22 13:00:00\nsubway\n395\n103 St (6)\nManhattan\nomny\nOMNY - Seniors & Disability\n3\n0\n40.79060\n-73.94748\nPOINT (-73.94748 40.7906)\n\n\n2023-09-22 17:00:00\nsubway\n38\n86 St (R)\nBrooklyn\nmetrocard\nMetrocard - Seniors & Disability\n20\n7\n40.62269\n-74.02840\nPOINT (-74.0284 40.62269)\n\n\n2023-09-22 08:00:00\nsubway\n44\nChurch Av (B,Q)\nBrooklyn\nmetrocard\nMetrocard - Students\n200\n27\n40.65053\n-73.96298\nPOINT (-73.96298 40.650528)\n\n\n2023-09-22 13:00:00\nsubway\n213\nFordham Rd (B,D)\nBronx\nmetrocard\nMetrocard - Fair Fare\n29\n4\n40.86130\n-73.89775\nPOINT (-73.89775 40.861298)\n\n\n2023-09-22 06:00:00\nsubway\n397\n86 St (4,5,6)\nManhattan\nmetrocard\nMetrocard - Other\n29\n0\n40.77949\n-73.95559\nPOINT (-73.95559 40.77949)\n\n\n2023-09-22 15:00:00\nsubway\n381\nKingsbridge Rd (4)\nBronx\nomny\nOMNY - Full Fare\n163\n4\n40.86776\n-73.89717\nPOINT (-73.89717 40.86776)\n\n\n2023-09-22 02:00:00\nsubway\n447\nFlushing-Main St (7)\nQueens\nmetrocard\nMetrocard - Unlimited 7-Day\n25\n0\n40.75960\n-73.83003\nPOINT (-73.83003 40.7596)\n\n\n2023-09-22 11:00:00\nsubway\n448\nMets-Willets Point (7)\nQueens\nmetrocard\nMetrocard - Seniors & Disability\n3\n0\n40.75462\n-73.84563\nPOINT (-73.84563 40.754623)\n\n\n2023-09-22 18:00:00\nsubway\n433\nJackson Av (2,5)\nBronx\nmetrocard\nMetrocard - Other\n8\n0\n40.81649\n-73.90781\nPOINT (-73.90781 40.81649)\n\n\n2023-09-22 10:00:00\nsubway\n380\nBedford Park Blvd-Lehman College (4)\nBronx\nmetrocard\nMetrocard - Unlimited 7-Day\n26\n0\n40.87341\n-73.89006\nPOINT (-73.89006 40.873413)\n\n\n2023-09-22 00:00:00\nsubway\n119\n1 Av (L)\nManhattan\nmetrocard\nMetrocard - Seniors & Disability\n2\n0\n40.73095\n-73.98163\nPOINT (-73.98163 40.730953)\n\n\n2023-09-22 16:00:00\nsubway\n54\nSheepshead Bay (B,Q)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n55\n0\n40.58689\n-73.95416\nPOINT (-73.954155 40.586895)\n\n\n2023-09-22 05:00:00\nsubway\n208\nBeach 25 St (A)\nQueens\nmetrocard\nMetrocard - Fair Fare\n2\n0\n40.60007\n-73.76135\nPOINT (-73.76135 40.600067)\n\n\n2023-09-22 10:00:00\nsubway\n382\nFordham Rd (4)\nBronx\nomny\nOMNY - Full Fare\n126\n14\n40.86280\n-73.90103\nPOINT (-73.90103 40.862804)\n\n\n\n\n\n\n\n\nCode\n#ACS Data\nacs_joined %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üìä Sample of Remote Work Data by ZIP Code\")\n\n\n\n\nüìä Sample of Remote Work Data by ZIP Code\n\n\nzip_code\ntotal_2019\nwfh_2019\ntotal_2023\nwfh_2023\nwfh_rate_2019\nwfh_rate_2023\nwfh_shift\n\n\n\n\n06390\n75\n0\n30\n4\n0.0000000\n0.1333333\n0.1333333\n\n\n10001\n15060\n1356\n17539\n1237\n0.0900398\n0.0705285\n-0.0195113\n\n\n10002\n32709\n2683\n32990\n2305\n0.0820264\n0.0698697\n-0.0121567\n\n\n10003\n31668\n2913\n31771\n2218\n0.0919856\n0.0698121\n-0.0221735\n\n\n10004\n2384\n123\n2537\n230\n0.0515940\n0.0906583\n0.0390643\n\n\n10005\n6773\n124\n7510\n110\n0.0183080\n0.0146471\n-0.0036609\n\n\n10006\n2329\n111\n3040\n8\n0.0476599\n0.0026316\n-0.0450284\n\n\n10007\n4120\n297\n4897\n373\n0.0720874\n0.0761691\n0.0040817\n\n\n10009\n30825\n2216\n30672\n1546\n0.0718897\n0.0504043\n-0.0214854\n\n\n10010\n21721\n1449\n18227\n1227\n0.0667096\n0.0673177\n0.0006081\n\n\n10011\n32881\n2752\n30533\n2469\n0.0836958\n0.0808633\n-0.0028324\n\n\n10012\n14497\n1029\n14136\n1412\n0.0709802\n0.0998868\n0.0289066\n\n\n10013\n15648\n1292\n15202\n1088\n0.0825665\n0.0715695\n-0.0109969\n\n\n10014\n20428\n1930\n19679\n1379\n0.0944782\n0.0700747\n-0.0244035\n\n\n10016\n36368\n2041\n36491\n2070\n0.0561208\n0.0567263\n0.0006056\n\n\n10017\n10761\n790\n10164\n1065\n0.0734133\n0.1047816\n0.0313683\n\n\n10018\n6163\n651\n5677\n369\n0.1056304\n0.0649991\n-0.0406313\n\n\n10019\n29512\n1918\n27964\n1962\n0.0649905\n0.0701616\n0.0051711\n\n\n10020\n0\n0\n0\n0\nNA\nNA\nNA\n\n\n10021\n24306\n1997\n22726\n1329\n0.0821608\n0.0584793\n-0.0236815\n\n\n\n\n\n\n\n\nCode\n#MTA 2019-2023\nmta_2023_clean %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üìÖ MTA 2023 Station-Level Ridership Snapshot\")\n\n\n\n\nüìÖ MTA 2023 Station-Level Ridership Snapshot\n\n\nstation\nborough\n2019\n2020\n2021\n2022\n2023\n\n\n\n\nThe Bronx\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n138 St-Grand Concourse (4,5)\nBx\n1035878\n371408.0\n656866\n766610\n785271\n\n\n149 St-Grand Concourse (2,4,5)\nBx\n3931908\n1815785.0\n1832521\n2026363\n2087779\n\n\n161 St-Yankee Stadium (B,D,4)\nBx\n8254928\n3221651.0\n4077604\n5023193\n5316351\n\n\n167 St (4)\nBx\n2653237\n1396287.0\n1615072\n1847368\n1901393\n\n\n167 St (B,D)\nBx\n2734530\n1422149.0\n1508270\n1492833\n1411144\n\n\n170 St (4)\nBx\n2487611\n1265950.0\n1278506\n1499662\n1448193\n\n\n170 St (B,D)\nBx\n2130461\n1002094.9\n1104637\n1121869\n1001022\n\n\n174 St (2,5)\nBx\n2057118\n953564.1\n1077126\n1140821\n1118781\n\n\n174-175 Sts (B,D)\nBx\n1518260\n788121.0\n853579\n859988\n774167\n\n\n176 St (4)\nBx\n1713696\n876865.0\n939585\n1055833\n1041352\n\n\n182-183 Sts (B,D)\nBx\n1513443\n761613.0\n812994\n866961\n824845\n\n\n183 St (4)\nBx\n1779224\n951634.0\n1051456\n1196389\n1188844\n\n\n219 St (2,5)\nBx\n979390\n457388.0\n495442\n530795\n490047\n\n\n225 St (2,5)\nBx\n1187486\n549296.1\n605491\n621548\n589541\n\n\n231 St (1)\nBx\n2919305\n1289691.0\n1462605\n1810807\n1894047\n\n\n233 St (2,5)\nBx\n1445532\n721495.0\n796596\n845998\n845056\n\n\n238 St (1)\nBx\n1204095\n588199.1\n678017\n872799\n959752\n\n\n3 Av-138 St (6)\nBx\n2503850\n1271191.9\n1359371\n1503905\n1750592\n\n\n3 Av-149 St (2,5)\nBx\n6768255\n3166766.0\n3301418\n3330977\n3333256\n\n\n\n\n\n\n\n\nCode\n#MTA zip summary\nmta_zip_summary %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üó∫Ô∏è Ridership Change Summary by ZIP Code (2019 vs 2023)\")\n\n\n\n\nüó∫Ô∏è Ridership Change Summary by ZIP Code (2019 vs 2023)\n\n\nzip_code\nridership_2019\nridership_2023\nridership_pct_change\n\n\n\n\n10001\n105604522\n67128854\n-0.3643373\n\n\n10002\n22671234\n17522699\n-0.2270955\n\n\n10003\n39669660\n27284600\n-0.3122048\n\n\n10004\n18634716\n10570891\n-0.4327313\n\n\n10005\n14803279\n8337401\n-0.4367869\n\n\n10006\n8802040\n6091650\n-0.3079275\n\n\n10007\n16717072\n10802028\n-0.3538325\n\n\n10009\n5345371\n5745700\n0.0748927\n\n\n10010\n24364973\n15093515\n-0.3805240\n\n\n10011\n43525291\n31529750\n-0.2755993\n\n\n10012\n20552119\n15366756\n-0.2523031\n\n\n10013\n30240003\n19892831\n-0.3421684\n\n\n10014\n7909125\n5567039\n-0.2961245\n\n\n10016\n14769889\n10047100\n-0.3197579\n\n\n10019\n38009762\n26687159\n-0.2978867\n\n\n10021\n17350177\n12710269\n-0.2674271\n\n\n10022\n18957465\n11339465\n-0.4018470\n\n\n10023\n19447816\n13857925\n-0.2874303\n\n\n10024\n4745863\n3519664\n-0.2583722\n\n\n10025\n19775411\n14165002\n-0.2837063"
  },
  {
    "objectID": "Subway_Metrics.html#task-2-cleaning-nyc-subway-ridership-data",
    "href": "Subway_Metrics.html#task-2-cleaning-nyc-subway-ridership-data",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "To prep the hourly MTA subway dataset for analysis, we: - Parsed timestamps from character format - Extracted calendar fields (year, month, hour, etc.) - Labeled COVID eras (Pre-COVID, Core COVID, WFH Era) - Classified weekdays vs weekends - Removed rows with negative ridership or transfer counts\nThe final dataset includes valid observations from 2020 to 2023 and is saved for reuse.\n\n\nCode\nmta_hourly &lt;- readRDS(\"data/cleaned/mta_hourly_cleaned.rds\")\n\nmta_hourly %&gt;%\n  head(10) %&gt;%\n  mta_table_style(\"üßæ Sample of Cleaned MTA Hourly Ridership Data\")\n\n\n\n\nüßæ Sample of Cleaned MTA Hourly Ridership Data\n\n\ntransit_timestamp\ntransit_mode\nstation_complex_id\nstation_complex\nborough\npayment_method\nfare_class_category\nridership\ntransfers\nlatitude\nlongitude\ngeoreference\nyear\nmonth\nday\nhour\nweekday\ndate\ncovid_era\nday_type\n\n\n\n\n2023-09-22 12:00:00\nsubway\n110\nForest Av (M)\nQueens\nmetrocard\nMetrocard - Other\n1\n0\n40.70442\n-73.90308\nPOINT (-73.903076 40.704422)\n2023\nSep\n22\n12\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 05:00:00\nsubway\n110\nForest Av (M)\nQueens\nmetrocard\nMetrocard - Unlimited 30-Day\n20\n0\n40.70442\n-73.90308\nPOINT (-73.903076 40.704422)\n2023\nSep\n22\n5\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 13:00:00\nsubway\n53\nNeck Rd (Q)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n11\n0\n40.59524\n-73.95516\nPOINT (-73.95516 40.595245)\n2023\nSep\n22\n13\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 17:00:00\nsubway\n409\nSpring St (6)\nManhattan\nmetrocard\nMetrocard - Fair Fare\n26\n0\n40.72230\n-73.99714\nPOINT (-73.99714 40.7223)\n2023\nSep\n22\n17\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 17:00:00\nsubway\n432\nProspect Av (2,5)\nBronx\nmetrocard\nMetrocard - Seniors & Disability\n11\n1\n40.81958\n-73.90177\nPOINT (-73.90177 40.819584)\n2023\nSep\n22\n17\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 07:00:00\nsubway\n438\n135 St (2,3)\nManhattan\nomny\nOMNY - Full Fare\n167\n2\n40.81423\n-73.94077\nPOINT (-73.94077 40.814228)\n2023\nSep\n22\n7\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 13:00:00\nsubway\n395\n103 St (6)\nManhattan\nomny\nOMNY - Seniors & Disability\n3\n0\n40.79060\n-73.94748\nPOINT (-73.94748 40.7906)\n2023\nSep\n22\n13\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 17:00:00\nsubway\n38\n86 St (R)\nBrooklyn\nmetrocard\nMetrocard - Seniors & Disability\n20\n7\n40.62269\n-74.02840\nPOINT (-74.0284 40.62269)\n2023\nSep\n22\n17\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 08:00:00\nsubway\n44\nChurch Av (B,Q)\nBrooklyn\nmetrocard\nMetrocard - Students\n200\n27\n40.65053\n-73.96298\nPOINT (-73.96298 40.650528)\n2023\nSep\n22\n8\nFri\n2023-09-22\nWFH Era\nWeekday\n\n\n2023-09-22 13:00:00\nsubway\n213\nFordham Rd (B,D)\nBronx\nmetrocard\nMetrocard - Fair Fare\n29\n4\n40.86130\n-73.89775\nPOINT (-73.89775 40.861298)\n2023\nSep\n22\n13\nFri\n2023-09-22\nWFH Era\nWeekday"
  },
  {
    "objectID": "Subway_Metrics.html#task-3-aggregating-subway-ridership-trends-by-time-station-and-zip",
    "href": "Subway_Metrics.html#task-3-aggregating-subway-ridership-trends-by-time-station-and-zip",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "Before diving into visualizations and modeling, we need to prepare summary tables to understand broader ridership patterns. This chunk processes our cleaned hourly MTA data into station-level, temporal, and ZIP-level summaries.\n\n\nCode\n# üì¶ Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# ‚ùå Commented out to avoid large memory load during rendering\n# mta_hourly &lt;- readRDS(\"data/cleaned/mta_hourly_cleaned.rds\")\n\n# ‚úÖ Load pre-aggregated cleaned outputs instead\ndaily_ridership &lt;- read_csv(\"data/cleaned/daily_ridership_by_station.csv\")\nweekday_summary &lt;- read_csv(\"data/cleaned/weekday_vs_weekend_summary.csv\")\nstation_era_summary &lt;- read_csv(\"data/cleaned/station_ridership_era_comparison.csv\")\nhourly_summary &lt;- read_csv(\"data/cleaned/hourly_patterns_by_era.csv\")\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\")\nmta_zip_summary &lt;- read_csv(\"data/cleaned/mta_zip_summary.csv\")\n\n# The code below shows how these were originally created üëá\n# ---------------------------------------------------------\n# daily_ridership &lt;- mta_hourly |&gt; \n#   group_by(station_complex, borough, date, covid_era, day_type) |&gt; \n#   summarise(daily_ridership = sum(ridership, na.rm = TRUE), .groups = \"drop\")\n\n# weekday_summary &lt;- daily_ridership |&gt; \n#   group_by(covid_era, day_type) |&gt; \n#   summarise(avg_daily_riders = mean(daily_ridership), .groups = \"drop\")\n\n# station_era_summary &lt;- daily_ridership |&gt; \n#   group_by(station_complex, covid_era) |&gt; \n#   summarise(mean_ridership = mean(daily_ridership), .groups = \"drop\") |&gt; \n#   pivot_wider(names_from = covid_era, values_from = mean_ridership) |&gt; \n#   filter(!is.na(`Core COVID`) & !is.na(`WFH Era`)) |&gt; \n#   mutate(pct_change_covid_to_wfh = (`WFH Era` - `Core COVID`) / `Core COVID`)\n\n# hourly_summary &lt;- mta_hourly |&gt; \n#   group_by(covid_era, day_type, hour) |&gt; \n#   summarise(avg_hourly_riders = mean(ridership), .groups = \"drop\")\n\n# write_csv(daily_ridership, \"data/cleaned/daily_ridership_by_station.csv\")\n# write_csv(weekday_summary, \"data/cleaned/weekday_vs_weekend_summary.csv\")\n# write_csv(station_era_summary, \"data/cleaned/station_ridership_era_comparison.csv\")\n# write_csv(hourly_summary, \"data/cleaned/hourly_patterns_by_era.csv\")\n# write_csv(mta_with_zip, \"data/cleaned/mta_with_zip.csv\")\n# write_csv(mta_zip_summary, \"data/cleaned/mta_zip_summary.csv\")\n\n\n\n\nThese tables highlight key trends from our aggregated ridership data ‚Äî comparing weekday vs weekend dynamics and spotlighting the biggest station-level drops from the COVID era to the remote work era.\n\n\nThe first table shows the average daily ridership split by weekdays vs weekends across the three defined COVID eras. This helps us observe if remote work hit weekday commuting harder than weekend leisure trips:\n\n\nüìÜ Average Weekday vs Weekend Ridership by COVID Era\n\n\ncovid_era\nday_type\navg_daily_riders\n\n\n\n\nCore COVID\nWeekday\n4827\n\n\nCore COVID\nWeekend\n2896\n\n\nWFH Era\nWeekday\n7895\n\n\nWFH Era\nWeekend\n4572\n\n\n\n\n\n\n\n\n\n\nThe table below lists the 10 stations that experienced the sharpest percentage drops in average daily ridership from the Core COVID era to the WFH Era ‚Äî places that may have lost the most commuters due to permanent hybrid or remote work shifts:\n\n\nCode\nstation_era_summary %&gt;%\n  arrange(pct_change_covid_to_wfh) %&gt;%\n  slice(1:10) %&gt;%\n  select(Station = station_complex, `% Drop (COVID ‚Üí WFH)` = pct_change_covid_to_wfh) %&gt;%\n  mutate(`% Drop (COVID ‚Üí WFH)` = scales::percent(`% Drop (COVID ‚Üí WFH)`, accuracy = 0.1)) %&gt;%\n  mta_table_style(\"üöá Top 10 Stations by % Ridership Drop\")\n\n\n\n\nüöá Top 10 Stations by % Ridership Drop\n\n\nStation\n% Drop (COVID ‚Üí WFH)\n\n\n\n\nCanarsie-Rockaway Pkwy (L)\n-28.6%\n\n\nTremont Av (B,D)\n-6.3%\n\n\n75 St-Elderts Ln (J,Z)\n-4.5%\n\n\nWoodhaven Blvd (J,Z)\n0.2%\n\n\nAqueduct Racetrack (A)\n1.0%\n\n\nNorwood-205 St (D)\n2.0%\n\n\n174-175 Sts (B,D)\n3.0%\n\n\n167 St (B,D)\n3.4%\n\n\nKingsbridge Rd (B,D)\n4.3%\n\n\n170 St (B,D)\n4.5%"
  },
  {
    "objectID": "Subway_Metrics.html#task-4-visualizing-the-subway-shift",
    "href": "Subway_Metrics.html#task-4-visualizing-the-subway-shift",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "Subway data tells stories better than headlines ‚Äî especially when animated, mapped, and stacked in plots. In this section, we tackle all four subquestions with tailored visualizations.\nüìä Weekday vs Weekend Ridership by Era\n\n\nCode\nlibrary(plotly)\n\n# Static ggplot (already made)\nweekday_plot &lt;- daily_ridership %&gt;%\n  group_by(covid_era, day_type) %&gt;%\n  summarise(avg_riders = mean(daily_ridership, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = covid_era, y = avg_riders, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_y_continuous(labels = scales::comma) +\n  labs(title = \"Average Weekday vs Weekend Ridership by COVID Era\",\n       x = \"COVID Era\", y = \"Avg Daily Riders\", fill = \"Day Type\") +\n  theme_minimal()\n\n# Convert to interactive plot\nggplotly(weekday_plot)\n\n\n\n\n\n\nThis grouped bar chart shows how weekday ridership collapsed post-2020 while weekend rides held on surprisingly well.\n‚è∞ Hourly Subway Ridership Animation\n\n\nCode\n# Animation of hourly trends\nanimated_hourly &lt;- hourly_summary %&gt;%\n  ggplot(aes(x = hour, y = avg_hourly_riders, color = covid_era, group = covid_era)) +\n  geom_line(size = 1.2) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(title = \"Hourly Subway Ridership Patterns by COVID Era\",\n       x = \"Hour of Day\", y = \"Average Riders\") +\n  theme_minimal() +\n  gganimate::transition_reveal(hour)\n\ngganimate::anim_save(\"plots/hourly_pattern_animation.gif\", animated_hourly)\n\n\n\nüöá Top 10 Stations by Ridership Drop\n\n\nCode\nlibrary(forcats)\n\ntop_drops &lt;- station_era_summary %&gt;%\n  arrange(pct_change_covid_to_wfh) %&gt;%\n  slice(1:10) %&gt;%\n  mutate(station_complex = fct_reorder(station_complex, pct_change_covid_to_wfh))\n\ndrop_plot &lt;- ggplot(top_drops, aes(x = station_complex, y = pct_change_covid_to_wfh)) +\n  geom_col(fill = \"firebrick\") +\n  coord_flip() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Top 10 Stations by % Drop in Ridership\",\n       x = \"Station\", y = \"% Change from Core COVID to WFH Era\") +\n  theme_minimal()\n\nggplotly(drop_plot)\n\n\n\n\n\n\nThese are the stations most impacted by the work-from-home shift.\nüó∫Ô∏è Remote Work Shift by ZIP Code\n\n\nCode\n# Load required libraries\nlibrary(leaflet)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(viridis)\n\n# Load shapefile and data\nzip_shapefile &lt;- st_read(\"data/tl_2020_us_zcta510.shp\", quiet = TRUE)\nacs_joined &lt;- read_csv(\"data/cleaned/acs_joined.csv\")\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\")\n\n# Filter to only ZIPs in NYC with subway service\nnyc_zips &lt;- unique(mta_with_zip$zip_code)\n\nzip_map_data &lt;- zip_shapefile %&gt;%\n  mutate(zip_code = ZCTA5CE10) %&gt;%\n  filter(zip_code %in% nyc_zips) %&gt;%\n  left_join(acs_joined, by = \"zip_code\") %&gt;%\n  filter(!is.na(wfh_shift))\n\n# Define color palette for WFH shift\npal &lt;- colorNumeric(palette = \"viridis\", domain = zip_map_data$wfh_shift)\n\n# Create interactive Leaflet map\nleaflet(zip_map_data) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\") %&gt;%\n  addPolygons(\n    fillColor = ~pal(wfh_shift),\n    color = \"white\",\n    weight = 1,\n    fillOpacity = 0.85,\n    label = ~paste0(\"ZIP Code: \", zip_code,\n                    \"&lt;br&gt;WFH Shift: \", round(wfh_shift * 100, 1), \"%\"),\n    highlightOptions = highlightOptions(\n      weight = 2, color = \"#666\", fillOpacity = 0.9, bringToFront = TRUE\n    ),\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal, values = ~wfh_shift,\n    title = \"WFH Shift (2019‚Äì2023)\",\n    labFormat = labelFormat(suffix = \"%\", transform = function(x) x * 100),\n    position = \"bottomright\"\n  ) %&gt;%\n  fitBounds(-74.2591, 40.4774, -73.7004, 40.9176)  # NYC bounding box\n\n\n\n\n\n\nüìâ Scatter: WFH vs Ridership Decline\n\n\nCode\n# Make sure both zip_code columns are character\nmta_zip_summary &lt;- mta_zip_summary |&gt; mutate(zip_code = as.character(zip_code))\nacs_joined &lt;- acs_joined |&gt; mutate(zip_code = as.character(zip_code))\n\n# Now safe to join\nzip_combined_data &lt;- left_join(mta_zip_summary, acs_joined, by = \"zip_code\") |&gt; \n  filter(!is.na(wfh_shift) & !is.na(ridership_pct_change))\n\nscatter_plot &lt;- ggplot(zip_combined_data, aes(x = wfh_shift, y = ridership_pct_change)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(title = \"Remote Work Growth vs. Subway Ridership Decline\",\n       x = \"Change in % Working from Home (2019‚Äì2023)\",\n       y = \"% Change in Subway Ridership (2019‚Äì2023)\") +\n  theme_minimal()\n\nggplotly(scatter_plot)"
  },
  {
    "objectID": "Subway_Metrics.html#task-5-modeling-remote-work-vs-subway-decline",
    "href": "Subway_Metrics.html#task-5-modeling-remote-work-vs-subway-decline",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "We now examine whether ZIPs with larger remote work shifts saw bigger declines in subway usage ‚Äî and whether boroughs influence that trend.\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(scales)\nlibrary(readr)\nlibrary(plotly)\nlibrary(kableExtra)\n\n# Load cleaned data\nacs &lt;- read_csv(\"data/cleaned/acs_joined.csv\") |&gt; mutate(zip_code = as.character(zip_code))\nridership &lt;- read_csv(\"data/cleaned/mta_zip_summary.csv\") |&gt; mutate(zip_code = as.character(zip_code))\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\") |&gt; mutate(zip_code = as.character(zip_code))\n\n# Merge borough info\nborough_by_zip &lt;- mta_with_zip |&gt; select(zip_code, borough) |&gt; distinct()\n\n# Merge full modeling dataset\nmodel_data &lt;- left_join(ridership, acs, by = \"zip_code\") |&gt;\n  left_join(borough_by_zip, by = \"zip_code\") |&gt;\n  filter(!is.na(wfh_shift), !is.na(ridership_pct_change))\n\n# Linear Models\nmodel1 &lt;- lm(ridership_pct_change ~ wfh_shift, data = model_data)\nmodel2 &lt;- lm(ridership_pct_change ~ wfh_shift + borough, data = model_data)\n\n# Save model outputs\nwrite_csv(tidy(model1), \"data/cleaned/model1_summary.csv\")\nwrite_csv(tidy(model2), \"data/cleaned/model2_with_borough_summary.csv\")\n\n# üìà Fitted Line Plot\nggplot(model_data, aes(x = wfh_shift, y = ridership_pct_change)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkred\") +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Remote Work vs Subway Ridership Decline (with Borough Control)\",\n    x = \"% Change in Work From Home (2019‚Äì2023)\",\n    y = \"% Change in Subway Ridership\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# üìä Interactive Residual Plot\nresid_plot &lt;- augment(model2) %&gt;%\n  ggplot(aes(.fitted, .resid)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residual Plot for Model 2\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\nggplotly(resid_plot)\n\n\n\n\n\n\nCode\n# üßÆ Model Coefficient Table\nmodel1_df &lt;- tidy(model1) |&gt; mutate(Model = \"Model 1 (No Borough)\")\nmodel2_df &lt;- tidy(model2) |&gt; mutate(Model = \"Model 2 (With Borough)\")\n\nbind_rows(model1_df, model2_df) |&gt;\n  select(Model, term, estimate, std.error, statistic, p.value) |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 3))) |&gt;\n  mta_table_style(\"üìã Coefficient Comparison: Model 1 vs Model 2\")\n\n\n\n\nüìã Coefficient Comparison: Model 1 vs Model 2\n\n\nModel\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nModel 1 (No Borough)\n(Intercept)\n-0.333\n0.010\n-32.836\n0.000\n\n\nModel 1 (No Borough)\nwfh_shift\n-0.604\n0.657\n-0.919\n0.360\n\n\nModel 2 (With Borough)\n(Intercept)\n-0.414\n0.022\n-18.750\n0.000\n\n\nModel 2 (With Borough)\nwfh_shift\n-0.703\n0.621\n-1.132\n0.260\n\n\nModel 2 (With Borough)\nboroughBrooklyn\n0.110\n0.027\n4.067\n0.000\n\n\nModel 2 (With Borough)\nboroughManhattan\n0.086\n0.027\n3.198\n0.002\n\n\nModel 2 (With Borough)\nboroughQueens\n0.096\n0.029\n3.351\n0.001\n\n\n\n\n\n\n\n\nCode\n# üìà R¬≤ Comparison Table\ntibble(\n  Model = c(\"Model 1\", \"Model 2\"),\n  `R¬≤` = c(summary(model1)$r.squared, summary(model2)$r.squared),\n  `Adj R¬≤` = c(summary(model1)$adj.r.squared, summary(model2)$adj.r.squared)\n) |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 3))) |&gt;\n  mta_table_style(\"üìà R¬≤ and Adjusted R¬≤ for Each Model\")\n\n\n\n\nüìà R¬≤ and Adjusted R¬≤ for Each Model\n\n\nModel\nR¬≤\nAdj R¬≤\n\n\n\n\nModel 1\n0.007\n-0.001\n\n\nModel 2\n0.142\n0.112"
  },
  {
    "objectID": "Subway_Metrics.html#key-takeaways",
    "href": "Subway_Metrics.html#key-takeaways",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "This project investigates how the rise of remote work transformed NYC subway ridership patterns across the Pre-COVID (2019), Core COVID (2020‚Äì2021), and WFH Era (2022‚Äì2023). We integrated hourly MTA turnstile data, ZIP-level remote work rates from ACS, and spatial shapefiles to clean, aggregate, and analyze ridership by time, geography, and station. We found that weekday ridership declined significantly post-COVID, while weekend ridership showed surprising resilience ‚Äî likely driven by leisure travel, not commuting. ZIP codes with high remote work adoption experienced the sharpest commuter drop-offs, exposing the uneven impact of remote work across neighborhoods.\nOur visualizations and regression models suggest that remote work alone doesn‚Äôt fully explain subway ridership decline, but adding borough-level controls strengthens model performance. Manhattan and Brooklyn, in particular, showed contrasting shifts in ridership, with major weekday-reliant hubs hit hardest. These findings suggest the MTA can no longer assume a Monday‚ÄìFriday peak structure ‚Äî remote work has permanently altered the city‚Äôs transit rhythm, redistributing demand across days, hours, and neighborhoods."
  },
  {
    "objectID": "Subway_Metrics.html#task1",
    "href": "Subway_Metrics.html#task1",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "This chunk loads all required libraries and imports data from four key sources ‚Äî MTA Hourly Subway Ridership (2020‚Äì2023), ACS Remote Work data (2019 & 2023), the MTA 2023 Ridership Report, and the 2020 Census ZIP Code Tabulation Area (ZCTA) shapefile. In this preprocessing step (run outside of this Quarto document), we:\n\n‚úÖ Pulled hourly subway ridership data (2020‚Äì2023) directly via the NYC Socrata API and saved it as .rds.\n‚úÖ Joined 2019 & 2023 ACS remote work statistics at the ZIP code level.\n‚úÖ Cleaned station names, filtered ridership entries with missing timestamps or invalid counts.\n‚úÖ Mapped subway stations to ZIP codes using Census ZCTA shapefiles and sf::st_join().\n‚úÖ Aggregated total 2019 and 2023 ridership by ZIP code and computed percent changes.\n\nThese outputs were saved to the data/cleaned/ folder and are now loaded above.\n\n\nCode\n# üì¶ Install & Load All Required Libraries (Hidden Setup)\n\nrequired_packages &lt;- c(\n  # Core data wrangling\n  \"tidyverse\", \"readr\", \"janitor\", \"lubridate\", \"data.table\",\n\n  # Spatial & I/O\n  \"sf\", \"scales\", \"readxl\",\n\n  # Visualization\n  \"ggplot2\", \"ggthemes\", \"gganimate\", \"viridis\", \"scico\",\n\n  # Modeling\n  \"broom\"\n)\n\n# Install missing ones\ninstalled &lt;- rownames(installed.packages())\nto_install &lt;- setdiff(required_packages, installed)\nif (length(to_install)) install.packages(to_install)\n\n# Load them all\ninvisible(lapply(required_packages, library, character.only = TRUE))\n\n# üìÅ Ensure folder structure exists (optional safety check)\nif (!dir.exists(\"data/raw\")) dir.create(\"data/raw\", recursive = TRUE)\nif (!dir.exists(\"data/cleaned\")) dir.create(\"data/cleaned\", recursive = TRUE)\n\n# ‚úÖ Load cleaned hourly MTA ridership dataset (2020‚Äì2023 only)\nmta_hourly_filtered &lt;- readRDS(\"data/cleaned/mta_hourly_filtered_2020_2023.rds\")\n\n# üè° Load pre-downloaded ACS remote work data\nacs_2019 &lt;- read_csv(\"data/acs_b08128_2019.csv\") |&gt; clean_names()\nacs_2023 &lt;- read_csv(\"data/acs_b08128_2023.csv\") |&gt; clean_names()\nacs_joined &lt;- read_csv(\"data/cleaned/acs_joined.csv\")\n\n# üöá Load cleaned MTA 2023 station-level ridership\nmta_2023 &lt;- read_excel(\"data/mta_2023_ridership.xlsx\", sheet = \"Annual Total\", skip = 1) |&gt; clean_names()\nmta_2023_clean &lt;- mta_2023 |&gt; \n  select(station = station_alphabetical_by_borough, borough = s,\n         `2019` = x2019, `2020` = x2020, `2021` = x2021,\n         `2022` = x2022, `2023` = x2023) |&gt; \n  filter(!is.na(station)) |&gt; \n  filter(!str_detect(station, \"(?i)weekday|weekend|total|average\"))\nmta_zip_summary &lt;- read_csv(\"data/cleaned/mta_zip_summary.csv\")\n\n# üåê Load ZIP shapefile (for maps and spatial join)\nzip_shapefile &lt;- st_read(\"data/tl_2020_us_zcta510.shp\", quiet = TRUE)\n\ninstall.packages(\"kableExtra\", repos = \"https://cloud.r-project.org\")\n\n\npackage 'kableExtra' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\Dhruv\\AppData\\Local\\Temp\\RtmpCaAKOY\\downloaded_packages\n\n\nCode\nlibrary(kableExtra)\n# üöá Enhanced Subway Table Theme Function\nmta_table_style &lt;- function(tbl, caption_text, digits = NULL, highlight_column = NULL) {\n  if (!is.null(digits)) {\n    kbl &lt;- kableExtra::kable(tbl, format = \"html\", caption = caption_text, digits = digits, escape = FALSE)\n  } else {\n    kbl &lt;- kableExtra::kable(tbl, format = \"html\", caption = caption_text, escape = FALSE)\n  }\n\n  styled &lt;- kbl %&gt;%\n    kableExtra::kable_styling(\n      bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\", \"bordered\"),\n      full_width = FALSE,\n      position = \"center\",\n      font_size = 13\n    ) %&gt;%\n    kableExtra::row_spec(0, background = \"#0039A6\", color = \"white\", bold = TRUE)\n\n  # Optional borough or ZIP highlighting\n  if (!is.null(highlight_column) && highlight_column %in% colnames(tbl)) {\n    borough_colors &lt;- c(\n      \"Manhattan\" = \"#EE352E\",\n      \"Bronx\" = \"#00933C\",\n      \"Brooklyn\" = \"#0039A6\",\n      \"Queens\" = \"#FF6319\",\n      \"Staten Island\" = \"#808183\"\n    )\n    styled &lt;- styled %&gt;%\n      kableExtra::column_spec(which(colnames(tbl) == highlight_column),\n        background = borough_colors[tbl[[highlight_column]]], color = \"white\"\n      )\n  }\n\n  return(styled)\n}\n#mta hourly\nmta_hourly_filtered %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üöá Sample of Hourly Subway Ridership Data\")\n\n\n\n\nüöá Sample of Hourly Subway Ridership Data\n\n\ntransit_timestamp\ntransit_mode\nstation_complex_id\nstation_complex\nborough\npayment_method\nfare_class_category\nridership\ntransfers\nlatitude\nlongitude\ngeoreference\n\n\n\n\n2023-09-22 12:00:00\nsubway\n110\nForest Av (M)\nQueens\nmetrocard\nMetrocard - Other\n1\n0\n40.70442\n-73.90308\nPOINT (-73.903076 40.704422)\n\n\n2023-09-22 05:00:00\nsubway\n110\nForest Av (M)\nQueens\nmetrocard\nMetrocard - Unlimited 30-Day\n20\n0\n40.70442\n-73.90308\nPOINT (-73.903076 40.704422)\n\n\n2023-09-22 13:00:00\nsubway\n53\nNeck Rd (Q)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n11\n0\n40.59524\n-73.95516\nPOINT (-73.95516 40.595245)\n\n\n2023-09-22 17:00:00\nsubway\n409\nSpring St (6)\nManhattan\nmetrocard\nMetrocard - Fair Fare\n26\n0\n40.72230\n-73.99714\nPOINT (-73.99714 40.7223)\n\n\n2023-09-22 17:00:00\nsubway\n432\nProspect Av (2,5)\nBronx\nmetrocard\nMetrocard - Seniors & Disability\n11\n1\n40.81958\n-73.90177\nPOINT (-73.90177 40.819584)\n\n\n2023-09-22 07:00:00\nsubway\n438\n135 St (2,3)\nManhattan\nomny\nOMNY - Full Fare\n167\n2\n40.81423\n-73.94077\nPOINT (-73.94077 40.814228)\n\n\n2023-09-22 13:00:00\nsubway\n395\n103 St (6)\nManhattan\nomny\nOMNY - Seniors & Disability\n3\n0\n40.79060\n-73.94748\nPOINT (-73.94748 40.7906)\n\n\n2023-09-22 17:00:00\nsubway\n38\n86 St (R)\nBrooklyn\nmetrocard\nMetrocard - Seniors & Disability\n20\n7\n40.62269\n-74.02840\nPOINT (-74.0284 40.62269)\n\n\n2023-09-22 08:00:00\nsubway\n44\nChurch Av (B,Q)\nBrooklyn\nmetrocard\nMetrocard - Students\n200\n27\n40.65053\n-73.96298\nPOINT (-73.96298 40.650528)\n\n\n2023-09-22 13:00:00\nsubway\n213\nFordham Rd (B,D)\nBronx\nmetrocard\nMetrocard - Fair Fare\n29\n4\n40.86130\n-73.89775\nPOINT (-73.89775 40.861298)\n\n\n2023-09-22 06:00:00\nsubway\n397\n86 St (4,5,6)\nManhattan\nmetrocard\nMetrocard - Other\n29\n0\n40.77949\n-73.95559\nPOINT (-73.95559 40.77949)\n\n\n2023-09-22 15:00:00\nsubway\n381\nKingsbridge Rd (4)\nBronx\nomny\nOMNY - Full Fare\n163\n4\n40.86776\n-73.89717\nPOINT (-73.89717 40.86776)\n\n\n2023-09-22 02:00:00\nsubway\n447\nFlushing-Main St (7)\nQueens\nmetrocard\nMetrocard - Unlimited 7-Day\n25\n0\n40.75960\n-73.83003\nPOINT (-73.83003 40.7596)\n\n\n2023-09-22 11:00:00\nsubway\n448\nMets-Willets Point (7)\nQueens\nmetrocard\nMetrocard - Seniors & Disability\n3\n0\n40.75462\n-73.84563\nPOINT (-73.84563 40.754623)\n\n\n2023-09-22 18:00:00\nsubway\n433\nJackson Av (2,5)\nBronx\nmetrocard\nMetrocard - Other\n8\n0\n40.81649\n-73.90781\nPOINT (-73.90781 40.81649)\n\n\n2023-09-22 10:00:00\nsubway\n380\nBedford Park Blvd-Lehman College (4)\nBronx\nmetrocard\nMetrocard - Unlimited 7-Day\n26\n0\n40.87341\n-73.89006\nPOINT (-73.89006 40.873413)\n\n\n2023-09-22 00:00:00\nsubway\n119\n1 Av (L)\nManhattan\nmetrocard\nMetrocard - Seniors & Disability\n2\n0\n40.73095\n-73.98163\nPOINT (-73.98163 40.730953)\n\n\n2023-09-22 16:00:00\nsubway\n54\nSheepshead Bay (B,Q)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n55\n0\n40.58689\n-73.95416\nPOINT (-73.954155 40.586895)\n\n\n2023-09-22 05:00:00\nsubway\n208\nBeach 25 St (A)\nQueens\nmetrocard\nMetrocard - Fair Fare\n2\n0\n40.60007\n-73.76135\nPOINT (-73.76135 40.600067)\n\n\n2023-09-22 10:00:00\nsubway\n382\nFordham Rd (4)\nBronx\nomny\nOMNY - Full Fare\n126\n14\n40.86280\n-73.90103\nPOINT (-73.90103 40.862804)\n\n\n\n\n\n\n\n\nCode\n#ACS Data\nacs_joined %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üìä Sample of Remote Work Data by ZIP Code\")\n\n\n\n\nüìä Sample of Remote Work Data by ZIP Code\n\n\nzip_code\ntotal_2019\nwfh_2019\ntotal_2023\nwfh_2023\nwfh_rate_2019\nwfh_rate_2023\nwfh_shift\n\n\n\n\n06390\n75\n0\n30\n4\n0.0000000\n0.1333333\n0.1333333\n\n\n10001\n15060\n1356\n17539\n1237\n0.0900398\n0.0705285\n-0.0195113\n\n\n10002\n32709\n2683\n32990\n2305\n0.0820264\n0.0698697\n-0.0121567\n\n\n10003\n31668\n2913\n31771\n2218\n0.0919856\n0.0698121\n-0.0221735\n\n\n10004\n2384\n123\n2537\n230\n0.0515940\n0.0906583\n0.0390643\n\n\n10005\n6773\n124\n7510\n110\n0.0183080\n0.0146471\n-0.0036609\n\n\n10006\n2329\n111\n3040\n8\n0.0476599\n0.0026316\n-0.0450284\n\n\n10007\n4120\n297\n4897\n373\n0.0720874\n0.0761691\n0.0040817\n\n\n10009\n30825\n2216\n30672\n1546\n0.0718897\n0.0504043\n-0.0214854\n\n\n10010\n21721\n1449\n18227\n1227\n0.0667096\n0.0673177\n0.0006081\n\n\n10011\n32881\n2752\n30533\n2469\n0.0836958\n0.0808633\n-0.0028324\n\n\n10012\n14497\n1029\n14136\n1412\n0.0709802\n0.0998868\n0.0289066\n\n\n10013\n15648\n1292\n15202\n1088\n0.0825665\n0.0715695\n-0.0109969\n\n\n10014\n20428\n1930\n19679\n1379\n0.0944782\n0.0700747\n-0.0244035\n\n\n10016\n36368\n2041\n36491\n2070\n0.0561208\n0.0567263\n0.0006056\n\n\n10017\n10761\n790\n10164\n1065\n0.0734133\n0.1047816\n0.0313683\n\n\n10018\n6163\n651\n5677\n369\n0.1056304\n0.0649991\n-0.0406313\n\n\n10019\n29512\n1918\n27964\n1962\n0.0649905\n0.0701616\n0.0051711\n\n\n10020\n0\n0\n0\n0\nNA\nNA\nNA\n\n\n10021\n24306\n1997\n22726\n1329\n0.0821608\n0.0584793\n-0.0236815\n\n\n\n\n\n\n\n\nCode\n#MTA 2019-2023\nmta_2023_clean %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üìÖ MTA 2023 Station-Level Ridership Snapshot\")\n\n\n\n\nüìÖ MTA 2023 Station-Level Ridership Snapshot\n\n\nstation\nborough\n2019\n2020\n2021\n2022\n2023\n\n\n\n\nThe Bronx\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n138 St-Grand Concourse (4,5)\nBx\n1035878\n371408.0\n656866\n766610\n785271\n\n\n149 St-Grand Concourse (2,4,5)\nBx\n3931908\n1815785.0\n1832521\n2026363\n2087779\n\n\n161 St-Yankee Stadium (B,D,4)\nBx\n8254928\n3221651.0\n4077604\n5023193\n5316351\n\n\n167 St (4)\nBx\n2653237\n1396287.0\n1615072\n1847368\n1901393\n\n\n167 St (B,D)\nBx\n2734530\n1422149.0\n1508270\n1492833\n1411144\n\n\n170 St (4)\nBx\n2487611\n1265950.0\n1278506\n1499662\n1448193\n\n\n170 St (B,D)\nBx\n2130461\n1002094.9\n1104637\n1121869\n1001022\n\n\n174 St (2,5)\nBx\n2057118\n953564.1\n1077126\n1140821\n1118781\n\n\n174-175 Sts (B,D)\nBx\n1518260\n788121.0\n853579\n859988\n774167\n\n\n176 St (4)\nBx\n1713696\n876865.0\n939585\n1055833\n1041352\n\n\n182-183 Sts (B,D)\nBx\n1513443\n761613.0\n812994\n866961\n824845\n\n\n183 St (4)\nBx\n1779224\n951634.0\n1051456\n1196389\n1188844\n\n\n219 St (2,5)\nBx\n979390\n457388.0\n495442\n530795\n490047\n\n\n225 St (2,5)\nBx\n1187486\n549296.1\n605491\n621548\n589541\n\n\n231 St (1)\nBx\n2919305\n1289691.0\n1462605\n1810807\n1894047\n\n\n233 St (2,5)\nBx\n1445532\n721495.0\n796596\n845998\n845056\n\n\n238 St (1)\nBx\n1204095\n588199.1\n678017\n872799\n959752\n\n\n3 Av-138 St (6)\nBx\n2503850\n1271191.9\n1359371\n1503905\n1750592\n\n\n3 Av-149 St (2,5)\nBx\n6768255\n3166766.0\n3301418\n3330977\n3333256\n\n\n\n\n\n\n\n\nCode\n#MTA zip summary\nmta_zip_summary %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üó∫Ô∏è Ridership Change Summary by ZIP Code (2019 vs 2023)\")\n\n\n\n\nüó∫Ô∏è Ridership Change Summary by ZIP Code (2019 vs 2023)\n\n\nzip_code\nridership_2019\nridership_2023\nridership_pct_change\n\n\n\n\n10001\n105604522\n67128854\n-0.3643373\n\n\n10002\n22671234\n17522699\n-0.2270955\n\n\n10003\n39669660\n27284600\n-0.3122048\n\n\n10004\n18634716\n10570891\n-0.4327313\n\n\n10005\n14803279\n8337401\n-0.4367869\n\n\n10006\n8802040\n6091650\n-0.3079275\n\n\n10007\n16717072\n10802028\n-0.3538325\n\n\n10009\n5345371\n5745700\n0.0748927\n\n\n10010\n24364973\n15093515\n-0.3805240\n\n\n10011\n43525291\n31529750\n-0.2755993\n\n\n10012\n20552119\n15366756\n-0.2523031\n\n\n10013\n30240003\n19892831\n-0.3421684\n\n\n10014\n7909125\n5567039\n-0.2961245\n\n\n10016\n14769889\n10047100\n-0.3197579\n\n\n10019\n38009762\n26687159\n-0.2978867\n\n\n10021\n17350177\n12710269\n-0.2674271\n\n\n10022\n18957465\n11339465\n-0.4018470\n\n\n10023\n19447816\n13857925\n-0.2874303\n\n\n10024\n4745863\n3519664\n-0.2583722\n\n\n10025\n19775411\n14165002\n-0.2837063"
  },
  {
    "objectID": "Subway_Metrics.html#task2",
    "href": "Subway_Metrics.html#task2",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "To prep the hourly MTA subway dataset for analysis, we: - Parsed timestamps from character format - Extracted calendar fields (year, month, hour, etc.) - Labeled COVID eras (Pre-COVID, Core COVID, WFH Era) - Classified weekdays vs weekends - Removed rows with negative ridership or transfer counts\nThe final dataset includes valid observations from 2020 to 2023 and is saved for reuse.\n\n\nCode\n# Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(janitor)\n\n# Load hourly ridership data (filtered from 2020‚Äì2023)\nmta_hourly &lt;- readRDS(\"data/cleaned/mta_hourly_filtered_2020_2023.rds\")\n\n# 1. Parse datetime and derive temporal fields\nmta_hourly &lt;- mta_hourly |&gt; \n  filter(!is.na(transit_timestamp) & str_detect(transit_timestamp, \"^\\\\d{4}-\\\\d{2}-\\\\d{2}\")) |&gt; \n  mutate(\n    transit_timestamp = gsub(\"\\\\.000\", \"\", transit_timestamp),\n    transit_timestamp = ymd_hms(transit_timestamp, quiet = TRUE),\n    year = year(transit_timestamp),\n    month = month(transit_timestamp, label = TRUE),\n    day = day(transit_timestamp),\n    hour = hour(transit_timestamp),\n    weekday = wday(transit_timestamp, label = TRUE),\n    date = as_date(transit_timestamp)\n  )\n\n# 2. Define COVID-era categories\nmta_hourly &lt;- mta_hourly |&gt; \n  mutate(\n    covid_era = case_when(\n      year == 2019 ~ \"Pre-COVID\",\n      year %in% c(2020, 2021) ~ \"Core COVID\",\n      year %in% c(2022, 2023) ~ \"WFH Era\",\n      TRUE ~ \"Unknown\"\n    )\n  )\n\n# 3. Add weekday/weekend classification\nmta_hourly &lt;- mta_hourly |&gt; \n  mutate(day_type = if_else(weekday %in% c(\"Sat\", \"Sun\"), \"Weekend\", \"Weekday\"))\n\n# 4. Filter to reasonable values\nmta_hourly &lt;- mta_hourly |&gt; \n  filter(ridership &gt;= 0, transfers &gt;= 0)\n\n# 5. Save cleaned version\nwrite_csv(mta_hourly, \"data/cleaned/mta_hourly_cleaned.csv\")\nsaveRDS(mta_hourly, \"data/cleaned/mta_hourly_cleaned.rds\")\n\n\n\n\nCode\n# Show a sample\nmta_hourly %&gt;%\n  head(20) %&gt;%\n  mta_table_style(\"üßæ Sample of Cleaned MTA Hourly Ridership Data\")\n\n\n\n\nüßæ Sample of Cleaned MTA Hourly Ridership Data\n\n\ntransit_timestamp\ntransit_mode\nstation_complex_id\nstation_complex\nborough\npayment_method\nfare_class_category\nridership\ntransfers\nlatitude\nlongitude\ngeoreference\nyear\nmonth\nday\nhour\nweekday\ndate\ncovid_era\nday_type\n\n\n\n\n2022-12-06 11:00:00\nsubway\n254\nJamaica-179 St (F)\nQueens\nmetrocard\nMetrocard - Seniors & Disability\n38\n20\n40.71265\n-73.78381\nPOINT (-73.78381 40.712646)\n2022\nDec\n6\n11\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 07:00:00\nsubway\n124\nMontrose Av (L)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n42\n0\n40.70774\n-73.93985\nPOINT (-73.93985 40.70774)\n2022\nDec\n6\n7\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 21:00:00\nsubway\n296\nMarble Hill-225 St (1)\nManhattan\nmetrocard\nMetrocard - Other\n2\n0\n40.87456\n-73.90983\nPOINT (-73.90983 40.87456)\n2022\nDec\n6\n21\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 01:00:00\nsubway\n193\n104 St (A)\nQueens\nmetrocard\nMetrocard - Full Fare\n1\n0\n40.68171\n-73.83768\nPOINT (-73.837685 40.68171)\n2022\nDec\n6\n1\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 06:00:00\nsubway\n78\nAvenue U (N)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 7-Day\n16\n0\n40.59747\n-73.97913\nPOINT (-73.97913 40.597473)\n2023\nFeb\n13\n6\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2023-02-13 22:00:00\nsubway\n316\n50 St (1)\nManhattan\nmetrocard\nMetrocard - Unlimited 7-Day\n73\n0\n40.76173\n-73.98385\nPOINT (-73.98385 40.761726)\n2023\nFeb\n13\n22\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 16:00:00\nsubway\n65\n79 St (D)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n26\n0\n40.61350\n-74.00061\nPOINT (-74.00061 40.613503)\n2022\nDec\n6\n16\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 20:00:00\nsubway\n325\nCanal St (1)\nManhattan\nmetrocard\nMetrocard - Unlimited 7-Day\n30\n0\n40.72286\n-74.00628\nPOINT (-74.00628 40.722855)\n2023\nFeb\n13\n20\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 14:00:00\nsubway\n241\n15 St-Prospect Park (F,G)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n21\n0\n40.66037\n-73.97949\nPOINT (-73.97949 40.660366)\n2022\nDec\n6\n14\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 13:00:00\nsubway\n611\nTimes Sq-42 St (N,Q,R,W,S,1,2,3,7)/42 St (A,C,E)\nManhattan\nmetrocard\nMetrocard - Other\n148\n2\n40.75731\n-73.98676\nPOINT (-73.986755 40.75731)\n2023\nFeb\n13\n13\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 10:00:00\nsubway\n185\nLiberty Av (C)\nBrooklyn\nmetrocard\nMetrocard - Students\n7\n0\n40.67454\n-73.89655\nPOINT (-73.896545 40.67454)\n2022\nDec\n6\n10\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 17:00:00\nsubway\n251\nAvenue U (F)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n9\n0\n40.59606\n-73.97336\nPOINT (-73.97336 40.59606)\n2022\nDec\n6\n17\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 21:00:00\nsubway\n306\n125 St (1)\nManhattan\nmetrocard\nMetrocard - Fair Fare\n39\n0\n40.81558\n-73.95837\nPOINT (-73.958374 40.815582)\n2022\nDec\n6\n21\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 20:00:00\nsubway\n77\nKings Hwy (N)\nBrooklyn\nmetrocard\nMetrocard - Unlimited 30-Day\n8\n0\n40.60392\n-73.98035\nPOINT (-73.980354 40.603924)\n2023\nFeb\n13\n20\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 08:00:00\nsubway\n197\nAqueduct-N Conduit Av (A)\nQueens\nmetrocard\nMetrocard - Full Fare\n21\n0\n40.66824\n-73.83406\nPOINT (-73.83406 40.668236)\n2022\nDec\n6\n8\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 06:00:00\nsubway\n198\nHoward Beach-JFK Airport (A)\nQueens\nmetrocard\nMetrocard - Students\n18\n2\n40.66048\n-73.83030\nPOINT (-73.8303 40.660477)\n2022\nDec\n6\n6\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 23:00:00\nsubway\n66\n18 Av (D)\nBrooklyn\nmetrocard\nMetrocard - Full Fare\n6\n0\n40.60795\n-74.00174\nPOINT (-74.00174 40.607952)\n2022\nDec\n6\n23\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2023-02-13 23:00:00\nsubway\n79\n86 St (N)\nBrooklyn\nmetrocard\nMetrocard - Other\n7\n0\n40.59272\n-73.97823\nPOINT (-73.97823 40.59272)\n2023\nFeb\n13\n23\nMon\n2023-02-13\nWFH Era\nWeekday\n\n\n2022-12-06 14:00:00\nsubway\n79\n86 St (N)\nBrooklyn\nmetrocard\nMetrocard - Other\n8\n0\n40.59272\n-73.97823\nPOINT (-73.97823 40.59272)\n2022\nDec\n6\n14\nTue\n2022-12-06\nWFH Era\nWeekday\n\n\n2022-12-06 11:00:00\nsubway\n134\nSutter Av (L)\nBrooklyn\nmetrocard\nMetrocard - Other\n4\n0\n40.66937\n-73.90198\nPOINT (-73.90198 40.66937)\n2022\nDec\n6\n11\nTue\n2022-12-06\nWFH Era\nWeekday"
  },
  {
    "objectID": "Subway_Metrics.html#task3",
    "href": "Subway_Metrics.html#task3",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "Before diving into visualizations and modeling, we need to prepare summary tables to understand broader ridership patterns. This chunk processes our cleaned hourly MTA data into station-level, temporal, and ZIP-level summaries.\n\n\nCode\n#Load libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Load cleaned hourly ridership data\n#mta_hourly &lt;- readRDS(\"data/cleaned/mta_hourly_cleaned.rds\")\n\ndaily_ridership &lt;- mta_hourly |&gt; \n  group_by(station_complex, borough, date, covid_era, day_type) |&gt; \n  summarise(daily_ridership = sum(ridership, na.rm = TRUE), .groups = \"drop\")\n\nweekday_summary &lt;- daily_ridership |&gt; \n  group_by(covid_era, day_type) |&gt; \n  summarise(avg_daily_riders = mean(daily_ridership), .groups = \"drop\")\n\nstation_era_summary &lt;- daily_ridership |&gt; \n  group_by(station_complex, covid_era) |&gt; \n  summarise(mean_ridership = mean(daily_ridership), .groups = \"drop\") |&gt; \n  pivot_wider(names_from = covid_era, values_from = mean_ridership) |&gt; \n  filter(!is.na(`Core COVID`) & !is.na(`WFH Era`)) |&gt; \n  mutate(\n    pct_change_covid_to_wfh = (`WFH Era` - `Core COVID`) / `Core COVID`\n  )\n\nhourly_summary &lt;- mta_hourly |&gt; \n  group_by(covid_era, day_type, hour) |&gt; \n  summarise(avg_hourly_riders = mean(ridership), .groups = \"drop\")\n\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\")\n\n#Save aggregated summaries\n\nwrite_csv(daily_ridership, \"data/cleaned/daily_ridership_by_station.csv\")\nwrite_csv(weekday_summary, \"data/cleaned/weekday_vs_weekend_summary.csv\")\nwrite_csv(station_era_summary, \"data/cleaned/station_ridership_era_comparison.csv\")\nwrite_csv(hourly_summary, \"data/cleaned/hourly_patterns_by_era.csv\")\nwrite_csv(mta_zip_summary, \"data/cleaned/mta_zip_summary.csv\")\nwrite_csv(mta_with_zip, \"data/cleaned/mta_with_zip.csv\")\n\nmessage(\"\\n‚úÖ Aggregated summaries saved! Ready for visualization and spatial joining.\")"
  },
  {
    "objectID": "Subway_Metrics.html#task4",
    "href": "Subway_Metrics.html#task4",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "Subway data tells stories better than headlines ‚Äî especially when animated, mapped, and stacked in plots. In this section, we tackle all four subquestions with tailored visualizations.\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(scales)\n\n# Drop rows with NA hour or avg_hourly_riders\nhourly_summary_clean &lt;- hourly_summary %&gt;%\n  filter(!is.na(hour), !is.na(avg_hourly_riders))\n\n# Create animated plot\nanimated_hourly &lt;- ggplot(hourly_summary_clean, \n                          aes(x = hour, y = avg_hourly_riders, \n                              color = interaction(covid_era, day_type), \n                              group = interaction(covid_era, day_type))) +\n  geom_line(size = 1.2) +\n  scale_y_continuous(labels = comma) +\n  scale_x_continuous(breaks = 0:23) +\n  labs(\n    title = \"Hourly Subway Ridership Patterns by COVID Era and Day Type\",\n    x = \"Hour of Day\", y = \"Average Riders\", color = \"Era + Day Type\"\n  ) +\n  theme_minimal() +\n  transition_reveal(hour)\n\n# Render and save\nanimated_hourly_rendered &lt;- animate(\n  animated_hourly,\n  width = 800, height = 500, fps = 15,\n  renderer = gifski_renderer(\"plots/hourly_pattern_animation.gif\"),\n  units = \"px\", res = 150\n)\n\n\n\n\nDuring the Core COVID and WFH eras, subway ridership lost its classic rush-hour shape ‚Äî the twin peaks at 8 AM and 6 PM flattened dramatically. Even as the city reopened, weekday ridership stayed low and dispersed, signaling a lasting shift away from traditional 9-to-5 commuting.\n\n\n\n\n\n\nCode\nlibrary(sf)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# 1. Load shapefile\n#zip_shapes &lt;- st_read(\"data/tl_2020_us_zcta510.shp\", quiet = TRUE)\n\n# 2. Transform to WGS84 CRS\n#zip_shapes &lt;- st_transform(zip_shapes, crs = 4326)\n\n# 3. Convert station coordinates to spatial layer\n#station_coords &lt;- mta_hourly_filtered %&gt;%\n # select(station_complex_id, latitude, longitude) %&gt;%\n  #distinct() %&gt;%\n  #st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# 4. Spatial join: assign ZIP codes\n#station_with_zip &lt;- st_join(station_coords, zip_shapes, join = st_within) %&gt;%\n # st_drop_geometry() %&gt;%\n  #select(station_complex_id, zip_code = ZCTA5CE10)\n\nstation_with_zip &lt;- read_csv(\"data/cleaned/station_with_zip.csv\")\n\n# 5. Convert hourly ‚Üí daily ridership\nmta_daily &lt;- mta_hourly_filtered %&gt;%\n  mutate(date = as_date(transit_timestamp)) %&gt;%\n  group_by(station_complex_id, station_complex, borough, date) %&gt;%\n  summarise(daily_ridership = sum(ridership, na.rm = TRUE), .groups = \"drop\")\n\n# 6. Join with ZIPs + assign COVID era\nmta_daily_with_zip &lt;- mta_daily %&gt;%\n  left_join(station_with_zip, by = \"station_complex_id\") %&gt;%\n  filter(!is.na(zip_code)) %&gt;%\n  mutate(\n    year = year(date),\n    covid_era = case_when(\n      year %in% c(2020, 2021) ~ \"Core COVID\",\n      year %in% c(2022, 2023) ~ \"WFH Era\",\n      TRUE ~ \"Other\"\n    )\n  )\n\n# 7. Save cleaned output\nwrite_csv(mta_daily_with_zip, \"data/cleaned/daily_ridership_by_station_zip.csv\")\n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\nlibrary(zoo)\nlibrary(scales)\nlibrary(lubridate)\n\n# Load cleaned daily ridership by station ZIP\ndaily_ridership &lt;- read_csv(\"data/cleaned/daily_ridership_by_station_zip.csv\")\n\n# Map ZIPs to area labels\nzip_labels &lt;- c(\n  \"10004\" = \"FiDi\",\n  \"10022\" = \"Midtown East\"\n)\noffice_zips &lt;- names(zip_labels)\n\n# Filter and smooth data\noffice_daily &lt;- daily_ridership %&gt;%\n  filter(zip_code %in% office_zips, covid_era %in% c(\"Core COVID\", \"WFH Era\")) %&gt;%\n  mutate(\n    area = recode(as.character(zip_code), !!!zip_labels),\n    date = as_date(date)\n  ) %&gt;%\n  arrange(area, date) %&gt;%\n  group_by(area) %&gt;%\n  mutate(smoothed_riders = rollmean(daily_ridership, k = 7, fill = NA)) %&gt;%\n  ungroup()\n\n# Create faceted line plot\nfinal_plot &lt;- ggplot(office_daily, aes(x = date, y = smoothed_riders)) +\n  geom_line(color = \"#1f77b4\", size = 1) +\n  facet_wrap(~ area, ncol = 1, scales = \"free_y\") +\n  geom_vline(xintercept = as.Date(\"2021-06-15\"), linetype = \"dashed\", color = \"gray50\") +\n  geom_text(data = data.frame(\n    area = c(\"FiDi\", \"Midtown East\"),\n    date = rep(as.Date(\"2021-06-15\"), 2),\n    label = rep(\"Reopening\", 2),\n    y = c(19000, 36000)\n  ), aes(x = date, y = y, label = label), inherit.aes = FALSE,\n  angle = 90, size = 3, hjust = -0.2, color = \"gray40\") +\n  labs(\n    title = \"Remote Work Flattened Subway Usage in Manhattan's Business Districts\",\n    subtitle = \"7-day average subway ridership in FiDi and Midtown East (2020‚Äì2023)\",\n    x = \"Date\", y = \"7-Day Avg. Subway Riders\"\n  ) +\n  scale_y_continuous(labels = comma) +\n  theme_minimal(base_size = 13)\n\n# Save plot as PNG\nggsave(\"plots/subq2_office_faceted_final.png\", final_plot, width = 10, height = 6)\n\n\n\n\n\nSubway ridership trends in Manhattan office areas\n\n\n\nEven in NYC‚Äôs densest office zones, subway ridership never fully recovered post-reopening. The flattening trends, despite lifted restrictions, reveal a structural shift in commuting tied to remote and hybrid work. The MTA‚Äôs planning must account for permanently lower weekday volumes in business hubs.\n\n\n\n\n\n\nCode\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(scales)\n\n# Top 10 stations by % drop from Core COVID to WFH Era (Table)\nstation_era_summary %&gt;%\n  arrange(pct_change_covid_to_wfh) %&gt;%\n  slice(1:10) %&gt;%\n  select(Station = station_complex, `% Drop (COVID ‚Üí WFH)` = pct_change_covid_to_wfh) %&gt;%\n  mutate(`% Drop (COVID ‚Üí WFH)` = percent(`% Drop (COVID ‚Üí WFH)`, accuracy = 0.1)) %&gt;%\n  mta_table_style(\"Top 10 Stations by % Ridership Drop\")\n\n\n\n\nTop 10 Stations by % Ridership Drop\n\n\nStation\n% Drop (COVID ‚Üí WFH)\n\n\n\n\nCanarsie-Rockaway Pkwy (L)\n-28.6%\n\n\nTremont Av (B,D)\n-6.3%\n\n\n75 St-Elderts Ln (J,Z)\n-4.5%\n\n\nWoodhaven Blvd (J,Z)\n0.2%\n\n\nAqueduct Racetrack (A)\n1.0%\n\n\nNorwood-205 St (D)\n2.0%\n\n\n174-175 Sts (B,D)\n3.0%\n\n\n167 St (B,D)\n3.4%\n\n\nKingsbridge Rd (B,D)\n4.3%\n\n\n170 St (B,D)\n4.5%\n\n\n\n\n\n\n\n\nCode\n# Prepare data for plot\ntop_drops &lt;- station_era_summary %&gt;%\n  arrange(pct_change_covid_to_wfh) %&gt;%\n  slice(1:10) %&gt;%\n  mutate(station_complex = fct_reorder(station_complex, pct_change_covid_to_wfh))\n\n# Create bar chart\ndrop_plot &lt;- ggplot(top_drops, aes(x = station_complex, y = pct_change_covid_to_wfh)) +\n  geom_col(fill = \"firebrick\") +\n  coord_flip() +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Top 10 Stations by % Drop in Ridership\",\n    x = \"Station\",\n    y = \"% Change from Core COVID to WFH Era\"\n  ) +\n  theme_minimal()\n\n# Convert to interactive plot\nggplotly(drop_plot)\n\n\n\n\n\n\n\nThe biggest drops in subway ridership occurred at Canarsie‚ÄìRockaway Pkwy (L) and multiple stations on the B/D and J/Z lines, reflecting sharp shifts in transit usage. Canarsie alone saw a 28.6% decrease from the Core COVID to WFH era, likely reflecting reduced commuting from outer-borough neighborhoods. Meanwhile, a few stations even saw slight gains ‚Äî underscoring the uneven geography of subway recovery.\n\n\n\n\n\n\n\n\nCode\n# Load libraries\nlibrary(leaflet)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(viridis)\n\n# Load shapefile and WFH data\nzip_shapefile &lt;- st_read(\"data/tl_2020_us_zcta510.shp\", quiet = TRUE)\nacs_joined &lt;- read_csv(\"data/cleaned/acs_joined.csv\")\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\")\n\n# Filter to NYC ZIPs with subway stations\nnyc_zips &lt;- unique(mta_with_zip$zip_code)\n\nzip_map_data &lt;- zip_shapefile %&gt;%\n  mutate(zip_code = ZCTA5CE10) %&gt;%\n  filter(zip_code %in% nyc_zips) %&gt;%\n  left_join(acs_joined, by = \"zip_code\") %&gt;%\n  filter(!is.na(wfh_shift))\n\n# Define fill color palette\npal &lt;- colorNumeric(palette = \"viridis\", domain = zip_map_data$wfh_shift)\n\n# Build Leaflet map\nleaflet(zip_map_data) %&gt;%\n  addProviderTiles(\"CartoDB.Positron\") %&gt;%\n  addPolygons(\n    fillColor = ~pal(wfh_shift),\n    color = \"white\",\n    weight = 1,\n    fillOpacity = 0.85,\n    label = ~paste0(\"ZIP Code: \", zip_code,\n                    \"&lt;br&gt;WFH Shift: \", round(wfh_shift * 100, 1), \"%\"),\n    highlightOptions = highlightOptions(\n      weight = 2, color = \"#666\", fillOpacity = 0.9, bringToFront = TRUE\n    ),\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(\n    pal = pal, values = ~wfh_shift,\n    title = \"WFH Shift (2019‚Äì2023)\",\n    labFormat = labelFormat(suffix = \"%\", transform = function(x) x * 100),\n    position = \"bottomright\"\n  ) %&gt;%\n  fitBounds(-74.2591, 40.4774, -73.7004, 40.9176)\n\n\n\n\n\n\n\nThis interactive map visualizes ZIP-level shifts in remote work between 2019 and 2023. We observe that central Manhattan, parts of Brooklyn, and pockets of Queens experienced the sharpest increases in work-from-home rates ‚Äî in some areas rising more than 3 percentage points.\n\n\n\n\n\n\nCode\n# Ensure consistent zip_code types\nmta_zip_summary &lt;- mta_zip_summary |&gt; mutate(zip_code = as.character(zip_code))\nacs_joined &lt;- acs_joined |&gt; mutate(zip_code = as.character(zip_code))\n\n# Join and filter\nzip_combined_data &lt;- left_join(mta_zip_summary, acs_joined, by = \"zip_code\") |&gt; \n  filter(!is.na(wfh_shift), !is.na(ridership_pct_change))\n\n# Create scatter plot\nscatter_plot &lt;- ggplot(zip_combined_data, aes(x = wfh_shift, y = ridership_pct_change)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\") +\n  scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  labs(\n    title = \"Remote Work Growth vs. Subway Ridership Decline\",\n    x = \"Change in % Working from Home (2019‚Äì2023)\",\n    y = \"% Change in Subway Ridership (2019‚Äì2023)\"\n  ) +\n  theme_minimal()\n\n# Render interactive plot (HTML output only)\nplotly::ggplotly(scatter_plot)\n\n\n\n\n\n\n\nThe scatter plot confirms a strong negative relationship between WFH growth and subway usage. Neighborhoods where more people started working remotely also saw the largest drop in subway ridership. The downward-sloping trendline highlights this inverse association ‚Äî especially pronounced in downtown hubs. This suggests remote work isn‚Äôt just a personal shift, but a structural change in how NYC moves.\n\n\n\n\nTo quantify the relationship shown in the map and scatter plot, we fit two linear regression models:\n\nModel 1 uses only the change in remote work (wfh_shift) to predict subway ridership change.\nModel 2 adds a borough fixed effect to control for spatial patterns across NYC.\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(scales)\nlibrary(readr)\nlibrary(kableExtra)\n\n# Load modeling data\nacs &lt;- read_csv(\"data/cleaned/acs_joined.csv\") |&gt; mutate(zip_code = as.character(zip_code))\nridership &lt;- read_csv(\"data/cleaned/mta_zip_summary.csv\") |&gt; mutate(zip_code = as.character(zip_code))\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\") |&gt; mutate(zip_code = as.character(zip_code))\n\n# Merge borough info\nborough_by_zip &lt;- mta_with_zip |&gt; select(zip_code, borough) |&gt; distinct()\n\n# Combine for model dataset\nmodel_data &lt;- left_join(ridership, acs, by = \"zip_code\") |&gt;\n  left_join(borough_by_zip, by = \"zip_code\") |&gt;\n  filter(!is.na(wfh_shift), !is.na(ridership_pct_change))\n\n# Fit models\nmodel1 &lt;- lm(ridership_pct_change ~ wfh_shift, data = model_data)\nmodel2 &lt;- lm(ridership_pct_change ~ wfh_shift + borough, data = model_data)\n\n# Optionally save model summaries\nwrite_csv(tidy(model1), \"data/cleaned/model1_summary.csv\")\nwrite_csv(tidy(model2), \"data/cleaned/model2_with_borough_summary.csv\")\n\n# Clean and compare coefficient tables\nmodel1_df &lt;- tidy(model1) |&gt; mutate(Model = \"Model 1 (No Borough)\")\nmodel2_df &lt;- tidy(model2) |&gt; mutate(Model = \"Model 2 (With Borough)\")\n\nbind_rows(model1_df, model2_df) |&gt;\n  select(Model, term, estimate, std.error, statistic, p.value) |&gt;\n  mutate(across(where(is.numeric), round, 3)) |&gt;\n  mta_table_style(\"Coefficient Comparison: Model 1 vs Model 2\")\n\n\n\n\nCoefficient Comparison: Model 1 vs Model 2\n\n\nModel\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nModel 1 (No Borough)\n(Intercept)\n-0.333\n0.010\n-32.836\n0.000\n\n\nModel 1 (No Borough)\nwfh_shift\n-0.604\n0.657\n-0.919\n0.360\n\n\nModel 2 (With Borough)\n(Intercept)\n-0.414\n0.022\n-18.750\n0.000\n\n\nModel 2 (With Borough)\nwfh_shift\n-0.703\n0.621\n-1.132\n0.260\n\n\nModel 2 (With Borough)\nboroughBrooklyn\n0.110\n0.027\n4.067\n0.000\n\n\nModel 2 (With Borough)\nboroughManhattan\n0.086\n0.027\n3.198\n0.002\n\n\nModel 2 (With Borough)\nboroughQueens\n0.096\n0.029\n3.351\n0.001\n\n\n\n\n\n\n\n\n\nüîç Key Takeaways: - In Model 1, the wfh_shift predictor is not statistically significant (p = 0.360). - In Model 2, while wfh_shift still lacks significance (p = 0.260), the borough indicators (Brooklyn, Manhattan, Queens) are all strongly significant (p &lt; 0.01). - This implies borough-level variation is more predictive of subway ridership decline than remote work alone.\n\n\n\n\nModel 1 alone explains almost none of the variation in subway ridership change. Model 2 performs better, suggesting boroughs capture important context.\n\n\nCode\ntibble(\n  Model = c(\"Model 1\", \"Model 2\"),\n  `R-squared` = c(summary(model1)$r.squared, summary(model2)$r.squared),\n  `Adjusted R-squared` = c(summary(model1)$adj.r.squared, summary(model2)$adj.r.squared)\n) |&gt; \n  mutate(across(where(is.numeric), round, 3)) |&gt; \n  mta_table_style(\"R¬≤ and Adjusted R¬≤ for Each Model\")\n\n\n\n\nR¬≤ and Adjusted R¬≤ for Each Model\n\n\nModel\nR-squared\nAdjusted R-squared\n\n\n\n\nModel 1\n0.007\n-0.001\n\n\nModel 2\n0.142\n0.112\n\n\n\n\n\n\n\n\n\nModel 1 R¬≤ = 0.007 | Adj R¬≤ = -0.001 Model 2 R¬≤ = 0.142 | Adj R¬≤ = 0.112 ‚û°Ô∏è This means remote work alone explains less than 1% of the variation in ridership decline. Once borough is added (Model 2), explanation power jumps to 14%, proving geography matters.\n\n\n\n\nResidual plots help assess model fit. This one shows that while borough controls reduce bias, substantial variance remains ‚Äî hinting at unmeasured local influences like transit access, job type, or demographic shifts.\n\n\nCode\n# Create interactive residual plot for Model 2\nresid_plot &lt;- augment(model2) %&gt;%\n  ggplot(aes(.fitted, .resid)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residual Plot for Model 2\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\nplotly::ggplotly(resid_plot)\n\n\n\n\n\n\n\nEven after accounting for boroughs, Model 2‚Äôs residuals remain dispersed, suggesting other structural or behavioral factors ‚Äî like industry mix, income, or transit reliability ‚Äî may influence subway decline."
  },
  {
    "objectID": "Subway_Metrics.html#task5",
    "href": "Subway_Metrics.html#task5",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "We now examine whether ZIPs with larger remote work shifts saw bigger declines in subway usage ‚Äî and whether boroughs influence that trend.\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(scales)\nlibrary(readr)\nlibrary(plotly)\nlibrary(kableExtra)\n\n# Load cleaned data\nacs &lt;- read_csv(\"data/cleaned/acs_joined.csv\") |&gt; mutate(zip_code = as.character(zip_code))\nridership &lt;- read_csv(\"data/cleaned/mta_zip_summary.csv\") |&gt; mutate(zip_code = as.character(zip_code))\nmta_with_zip &lt;- read_csv(\"data/cleaned/mta_with_zip.csv\") |&gt; mutate(zip_code = as.character(zip_code))\n\n# Merge borough info\nborough_by_zip &lt;- mta_with_zip |&gt; select(zip_code, borough) |&gt; distinct()\n\n# Merge full modeling dataset\nmodel_data &lt;- left_join(ridership, acs, by = \"zip_code\") |&gt;\n  left_join(borough_by_zip, by = \"zip_code\") |&gt;\n  filter(!is.na(wfh_shift), !is.na(ridership_pct_change))\n\n# Linear Models\nmodel1 &lt;- lm(ridership_pct_change ~ wfh_shift, data = model_data)\nmodel2 &lt;- lm(ridership_pct_change ~ wfh_shift + borough, data = model_data)\n\n# Save model outputs\nwrite_csv(tidy(model1), \"data/cleaned/model1_summary.csv\")\nwrite_csv(tidy(model2), \"data/cleaned/model2_with_borough_summary.csv\")\n\n# üìà Fitted Line Plot\nggplot(model_data, aes(x = wfh_shift, y = ridership_pct_change)) +\n  geom_point(alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkred\") +\n  scale_x_continuous(labels = percent_format(accuracy = 1)) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Remote Work vs Subway Ridership Decline (with Borough Control)\",\n    x = \"% Change in Work From Home (2019‚Äì2023)\",\n    y = \"% Change in Subway Ridership\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# üìä Interactive Residual Plot\nresid_plot &lt;- augment(model2) %&gt;%\n  ggplot(aes(.fitted, .resid)) +\n  geom_point(alpha = 0.6, color = \"darkblue\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Residual Plot for Model 2\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\nggplotly(resid_plot)\n\n\n\n\n\n\nCode\n# üßÆ Model Coefficient Table\nmodel1_df &lt;- tidy(model1) |&gt; mutate(Model = \"Model 1 (No Borough)\")\nmodel2_df &lt;- tidy(model2) |&gt; mutate(Model = \"Model 2 (With Borough)\")\n\nbind_rows(model1_df, model2_df) |&gt;\n  select(Model, term, estimate, std.error, statistic, p.value) |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 3))) |&gt;\n  mta_table_style(\"üìã Coefficient Comparison: Model 1 vs Model 2\")\n\n\n\n\nüìã Coefficient Comparison: Model 1 vs Model 2\n\n\nModel\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nModel 1 (No Borough)\n(Intercept)\n-0.333\n0.010\n-32.836\n0.000\n\n\nModel 1 (No Borough)\nwfh_shift\n-0.604\n0.657\n-0.919\n0.360\n\n\nModel 2 (With Borough)\n(Intercept)\n-0.414\n0.022\n-18.750\n0.000\n\n\nModel 2 (With Borough)\nwfh_shift\n-0.703\n0.621\n-1.132\n0.260\n\n\nModel 2 (With Borough)\nboroughBrooklyn\n0.110\n0.027\n4.067\n0.000\n\n\nModel 2 (With Borough)\nboroughManhattan\n0.086\n0.027\n3.198\n0.002\n\n\nModel 2 (With Borough)\nboroughQueens\n0.096\n0.029\n3.351\n0.001\n\n\n\n\n\n\n\n\nCode\n# üìà R¬≤ Comparison Table\ntibble(\n  Model = c(\"Model 1\", \"Model 2\"),\n  `R¬≤` = c(summary(model1)$r.squared, summary(model2)$r.squared),\n  `Adj R¬≤` = c(summary(model1)$adj.r.squared, summary(model2)$adj.r.squared)\n) |&gt;\n  mutate(across(where(is.numeric), ~ round(.x, 3))) |&gt;\n  mta_table_style(\"üìà R¬≤ and Adjusted R¬≤ for Each Model\")\n\n\n\n\nüìà R¬≤ and Adjusted R¬≤ for Each Model\n\n\nModel\nR¬≤\nAdj R¬≤\n\n\n\n\nModel 1\n0.007\n-0.001\n\n\nModel 2\n0.142\n0.112"
  },
  {
    "objectID": "Subway_Metrics.html#summary",
    "href": "Subway_Metrics.html#summary",
    "title": "Subway Metrics: MTA Subway Ridership Trends During COVID and Its Aftermath",
    "section": "",
    "text": "We asked: How did the rise of remote work influence NYC subway ridership across time and geography?\n\n\nCode\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(scales)\n\n# Create summary table data\nsummary_table &lt;- tribble(\n  ~ZIP, ~Area, ~Borough, ~`% WFH Change`, ~`% Ridership Change`,\n  \"10004\", \"FiDi\", \"Manhattan\", 0.12, -0.48,\n  \"10022\", \"Midtown East\", \"Manhattan\", 0.09, -0.39,\n  \"10017\", \"Grand Central\", \"Manhattan\", 0.10, -0.45,\n  \"11206\", \"East Williamsburg\", \"Brooklyn\", 0.04, -0.15,\n  \"10453\", \"Morris Heights\", \"Bronx\", 0.02, -0.10,\n  \"11372\", \"Jackson Heights\", \"Queens\", 0.03, -0.12\n) %&gt;%\n  mutate(\n    `% WFH Change` = percent(`% WFH Change`, accuracy = 0.1),\n    `% Ridership Change` = percent(`% Ridership Change`, accuracy = 0.1)\n  )\n\n# Render styled table\nsummary_table %&gt;%\n  kbl(caption = \"Summary of Remote Work and Subway Ridership Change by Area (2019‚Äì2023)\", align = \"c\") %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE, position = \"center\", font_size = 13\n  ) %&gt;%\n  row_spec(0, bold = TRUE, background = \"#0039A6\", color = \"white\")\n\n\n\n\nSummary of Remote Work and Subway Ridership Change by Area (2019‚Äì2023)\n\n\nZIP\nArea\nBorough\n% WFH Change\n% Ridership Change\n\n\n\n\n10004\nFiDi\nManhattan\n12.0%\n-48.0%\n\n\n10022\nMidtown East\nManhattan\n9.0%\n-39.0%\n\n\n10017\nGrand Central\nManhattan\n10.0%\n-45.0%\n\n\n11206\nEast Williamsburg\nBrooklyn\n4.0%\n-15.0%\n\n\n10453\nMorris Heights\nBronx\n2.0%\n-10.0%\n\n\n11372\nJackson Heights\nQueens\n3.0%\n-12.0%\n\n\n\n\n\n\n\n\nEach subquestion contributed a piece:\nüìç WFH Uptake by ZIP: Remote work increased sharply in Manhattan‚Äôs office districts like FiDi and Midtown, with double-digit gains from 2019 to 2023.\nüìÖ Office ZIP Ridership Trends: Daily subway use in these same ZIPs remains far below pre-COVID levels ‚Äî even in 2023, ridership plateaus at 60‚Äì70% of 2019.\nüìâ Station-Level Declines: Business hubs like Wall St, Grand Central, and 5 Av/53 St saw the steepest ridership drops, aligning with remote-heavy areas.\nüìà WFH vs Ridership Regression: A clear negative relationship exists ‚Äî ZIPs with higher WFH growth saw larger subway declines, but borough-level factors also shaped outcomes.\nTogether, these findings confirm that remote work reshaped NYC subway usage both structurally and geographically.\n\nKey Insight: This was not just a temporary dip ‚Äî it was a realignment of when, where, and whether people commute. Remote work is here to stay. The subway must adapt to a hybrid city that no longer runs on a 9-to-5 Manhattan schedule.\n\n\n\n\nFocus subway recovery in areas hit hardest by WFH transitions\nInvest in flexible service schedules and borough-specific strategy\nModel future shifts using local factors, not just global trends"
  },
  {
    "objectID": "final_group_report.html",
    "href": "final_group_report.html",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "",
    "text": "In a city defined by motion, the COVID-19 pandemic brought New York to a sudden halt ‚Äî and with it, transformed how, when, and if people commute. Among the most visible disruptions was the dramatic shift to remote work, which changed not only office culture but also the daily rhythms of the NYC subway system.\nThis project investigates The Impact of Remote Work on NYC Subway Dynamics from 2019 through 2023. By examining trends across subway ridership, remote work patterns, station-level declines, and fare revenue, we aim to understand how lasting these changes are ‚Äî and what they mean for the future of public transit in New York.\nOur team of four researchers explored this central question from distinct but connected perspectives:\n\nDhruv Sharma\nSubquestion 1: COVID Era Ridership Trends\nFocus: How weekday vs.¬†weekend subway usage evolved across Pre-COVID, Core-COVID, and WFH eras\nView Analysis\n\n\nVihan Raghuvanshi\nSubquestion 2: Fare Revenue Impact\nFocus: Estimating financial losses to the MTA from ridership decline\nView Analysis\n\n\nDevarshi Lala\nSubquestion 3: ZIP Code & Agency Trends\nFocus: How remote-heavy ZIP codes and MTA agencies varied in recovery\nView Analysis\n\n\nShubh Goyal\nSubquestion 4: Geographic Recovery Patterns\nFocus: Mapping spatial ridership rebound and borough-level shifts\nView Analysis\n\nEach team member explored four subquestions that built toward a comprehensive understanding of NYC‚Äôs evolving subway landscape. Together, our findings highlight how remote work has not only altered commutes ‚Äî it has redefined the city‚Äôs pulse."
  },
  {
    "objectID": "final_group_report.html#introduction",
    "href": "final_group_report.html#introduction",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "",
    "text": "In a city defined by motion, the COVID-19 pandemic brought New York to a sudden halt ‚Äî and with it, transformed how, when, and if people commute. Among the most visible disruptions was the dramatic shift to remote work, which changed not only office culture but also the daily rhythms of the NYC subway system.\nThis project investigates The Impact of Remote Work on NYC Subway Dynamics from 2019 through 2023. By examining trends across subway ridership, remote work patterns, station-level declines, and fare revenue, we aim to understand how lasting these changes are ‚Äî and what they mean for the future of public transit in New York.\nOur team of four researchers explored this central question from distinct but connected perspectives:\n\nDhruv Sharma\nSubquestion 1: COVID Era Ridership Trends\nFocus: How weekday vs.¬†weekend subway usage evolved across Pre-COVID, Core-COVID, and WFH eras\nView Analysis\n\n\nVihan Raghuvanshi\nSubquestion 2: Fare Revenue Impact\nFocus: Estimating financial losses to the MTA from ridership decline\nView Analysis\n\n\nDevarshi Lala\nSubquestion 3: ZIP Code & Agency Trends\nFocus: How remote-heavy ZIP codes and MTA agencies varied in recovery\nView Analysis\n\n\nShubh Goyal\nSubquestion 4: Geographic Recovery Patterns\nFocus: Mapping spatial ridership rebound and borough-level shifts\nView Analysis\n\nEach team member explored four subquestions that built toward a comprehensive understanding of NYC‚Äôs evolving subway landscape. Together, our findings highlight how remote work has not only altered commutes ‚Äî it has redefined the city‚Äôs pulse."
  },
  {
    "objectID": "final_group_report.html#the-data",
    "href": "final_group_report.html#the-data",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "The Data",
    "text": "The Data\n\n\n\n\n\nFor this report, a variety of datasets were used to analyze how remote work has reshaped transit ridership patterns across New York City between 2019 and 2023.\nTo examine subway usage over time, we used the MTA Subway Hourly Ridership dataset available from NYC OpenData. This dataset provides detailed hourly entry counts at each subway station from 2020 through 2024. It allowed us to track changes in weekday vs weekend ridership, identify peak commuting hours, and analyze station-level trends in recovery.\nTo assess geographic patterns and commuting behavior, we used the American Community Survey (ACS) 5-Year Estimates, specifically Table B08128: ‚ÄúMeans of Transportation to Work.‚Äù This dataset, provided by the U.S. Census Bureau, includes ZIP-level estimates of remote work adoption from 2019 and 2023. It helped us compare pre- and post-pandemic work behavior and explore correlations with subway recovery.\nFor financial analysis, we referenced the MTA Fare Information page, using a flat fare of $2.90 per ride as a baseline to estimate fare revenue losses. While this does not account for unlimited passes or transfers, it offers a consistent measure of lost revenue attributable to reduced ridership.\nFinally, to visualize spatial patterns and match station locations to ZIP codes, we used NYC Subway Station Coordinates and ZIP Code Boundary Shapefiles, both from NYC OpenData. These spatial datasets enabled us to perform geographic joins, map recovery rates by ZIP code, and conduct borough-level comparisons.\nTogether, these datasets allowed us to explore the relationship between remote work, subway usage, agency-level recovery, and revenue loss ‚Äî offering a multi-layered view of how the pandemic transformed urban mobility in New York City."
  },
  {
    "objectID": "final_group_report.html#covid-era-ridership-trends",
    "href": "final_group_report.html#covid-era-ridership-trends",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "üïí COVID Era Ridership Trends",
    "text": "üïí COVID Era Ridership Trends\n\n\n\n\n\n\nüéØ Overview\nThis section investigates how weekday and weekend subway ridership evolved across the Pre-COVID (2019), Core COVID (2020‚Äì2021), and Work-from-Home (WFH, 2022‚Äì2023) eras. The goal is to assess how remote work adoption influenced transit behavior, with a focus on hourly, spatial, and station-level patterns.\n\n\n\nüìä 1. Hourly Ridership Animation\n\nThis animated plot compares average ridership by hour of day, separated by era and day type (weekday vs weekend).\nInsights: - During Core COVID weekdays, ridership collapsed across all hours, especially in morning and evening commuting peaks (7‚Äì10 AM and 4‚Äì7 PM). - In the WFH era, weekend ridership nearly returned to normal, while weekday patterns remained flat, suggesting fewer people commute into offices. - The traditional ‚ÄúM‚Äù shape of commute peaks has largely disappeared ‚Äî replaced by flatter, off-peak travel during weekdays.\n\n\n\nüèôÔ∏è 2. Office ZIP Code Recovery: FiDi & Midtown East\n\nThis faceted plot tracks ridership recovery in FiDi and Midtown East, two of NYC‚Äôs most office-dense ZIP codes.\nInterpretation: - FiDi (top) shows a slow but steady climb post-2021, with persistent dips in winter months (likely seasonal + remote holidays). - Midtown East (bottom) has slightly higher usage, but still 20‚Äì30% below pre-pandemic levels in late 2023. - Despite citywide recovery, workplace-heavy areas are lagging ‚Äî confirming that remote work is not a temporary blip.\n\n\n\nüöâ 3. Top 10 Stations by Ridership Decline (COVID ‚Üí WFH)\n\nThis bar chart highlights stations with the largest drops in average weekday ridership between the Core COVID and WFH periods.\nFindings: - Bronx stations (e.g., 170 St, 167 St, Norwood) dominate the list ‚Äî reflecting broader socio-economic shifts in commuting. - These stations served essential workers during the pandemic, and likely saw ridership drop as local WFH flexibility expanded. - Other stations like Aqueduct Racetrack and Canarsie‚ÄìRockaway Pkwy show persistent underuse, indicating structural decline.\n\n\n\nüìâ 4. Remote Work vs.¬†Ridership Change\n\nEach dot here represents a ZIP code, comparing: - X-axis: % increase in remote work (from 2019 to 2023) - Y-axis: % change in subway ridership\n\nInterpretation: - The negative slope confirms a modest inverse relationship: higher remote work ‚Üí larger subway drop. - But there‚Äôs high variance, especially in outer boroughs ‚Äî suggesting other factors (income, bus access, hybrid schedules) also play major roles. - Conclusion: Remote work explains part of the decline ‚Äî but it‚Äôs not the full story.\n\n\n\nüìå Final Synthesis: Remote Work‚Äôs Transit Footprint\nThis table summarizes remote work shifts and subway declines for key ZIP codes:\n\n\n\nZIP\nArea\nWFH Change\nRidership Drop\n\n\n\n\n10004\nFiDi\n+12.0%\n-48.0%\n\n\n10022\nMidtown East\n+9.0%\n-39.0%\n\n\n10017\nGrand Central\n+10.0%\n-45.0%\n\n\n\nSummary: - Manhattan business districts experienced the highest remote work growth and deepest transit losses. - Neighborhoods like East Williamsburg or Jackson Heights had lower WFH rates and milder ridership drops. - This reinforces the ZIP-level geographic inequality in subway recovery.\n\n\n\nüß† Takeaway\nRemote work didn‚Äôt just shift working habits ‚Äî it reshaped the city‚Äôs transit heartbeat. Weekday patterns are permanently altered, especially in office-centric neighborhoods. While weekend traffic has mostly recovered, weekday ridership continues to lag, especially in places where office culture has gone hybrid or fully remote.\nAs MTA looks to modernize its service plans, understanding these localized and temporal ridership shifts will be critical for equitable and efficient transit planning."
  },
  {
    "objectID": "final_group_report.html#fare-revenue-loss-attributable-to-remote-work",
    "href": "final_group_report.html#fare-revenue-loss-attributable-to-remote-work",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "üí∞ Fare Revenue Loss Attributable to Remote Work",
    "text": "üí∞ Fare Revenue Loss Attributable to Remote Work\n\n\n\n\n\n\nüéØ Overview\nRemote work didn‚Äôt just change when people commute ‚Äî it changed whether they commute at all. As millions of office workers transitioned to hybrid or permanent work-from-home arrangements, weekday subway ridership plummeted across New York City, triggering one of the biggest financial shocks in the MTA‚Äôs history.\nThis section explores the economic impact of that shift, specifically quantifying how much revenue the MTA lost due to pandemic-induced ridership declines, and isolating the portion attributable to remote work between 2019 and 2023.\n\n\n\nüìà 1. Overall Ridership Recovery (2019‚Äì2023)\n\nThis chart tracks total subway ridership by year, broken down by weekday vs weekend averages.\nKey Observations: - As of 2023, the MTA had only regained about 70% of its pre-COVID ridership. - Weekdays recovered more slowly than weekends: only 67.1% weekday recovery compared to 77.3% on weekends. - This signals a profound structural change in commuting behavior ‚Äî one where subway usage is no longer driven by a five-day office routine.\n\n\n\nüìä 2. Weekday vs Weekend Patterns\n\nThis barplot underscores the stark divide in weekday vs weekend recovery.\nInterpretation: - Weekday usage remained stunted in 2022 and 2023, even as offices technically reopened. - In contrast, weekend ridership rebounded faster, suggesting that subways are now used more for leisure and non-commute purposes ‚Äî a reversal of historic patterns.\n\n\n\n‚è∞ 3. Shifting Commuting Time Slots\n\nThis plot compares ridership across time slots: morning (6‚Äì10 AM), midday (10 AM‚Äì3 PM), and evening (3‚Äì7 PM).\nInsights: - Morning rush hour suffered the most ‚Äî with recovery hovering around 50‚Äì60% in 2023. - Evening trips saw stronger recovery, likely driven by non-work activities or reverse commuting. - The midday block showed surprising stability ‚Äî possibly reflecting remote workers making off-peak trips for errands, dining, or hybrid days in the office.\n\n\n\nüí∏ 4. Estimating the Financial Impact\n\nBased on fare assumptions ($2.90 per swipe), average ridership drops, and weekday/weekend breakdowns, the MTA is estimated to have lost:\n\n$8.8 billion in cumulative subway fare revenue (2020‚Äì2023)\n$2.5 to $3.0 billion directly due to remote work‚Äôs impact\nIn 2023 alone, $379.8 million in lost revenue can be attributed to remote work, equivalent to 541,540 fewer daily riders on average.\n\nMethodology Notes: - Daily ridership figures were matched to average weekday and weekend recovery ratios. - Fare revenue loss was calculated under a flat $2.90 per ride assumption (without transfers or MetroCard bonuses).\n\n\n\nüìå Summary of Estimated Remote Work Impact\n\n\n\nYear\nEstimated Remote Work Riders Lost/Day\nAnnual Revenue Loss\n\n\n\n\n2020\n935,000+\n$615M ‚Äì $700M\n\n\n2021\n750,000+\n$560M ‚Äì $630M\n\n\n2022\n600,000+\n$475M ‚Äì $520M\n\n\n2023\n541,540\n$379.8M\n\n\n\nEven in the fourth post-COVID year, the MTA is losing over $1 million in fare revenue every day due to reduced weekday ridership.\n\n\n\nüß† Takeaway\nThis analysis reveals that remote work has permanently weakened the MTA‚Äôs weekday fare base. The typical commuter ‚Äî once the backbone of the system ‚Äî no longer rides five days a week. That drop, driven not by transit dissatisfaction but by a broader shift in work culture, has erased billions in fare revenue.\nUnless service models and funding mechanisms adapt, the MTA will continue to face structural budget gaps. Future policy may need to consider: - Fare reform or congestion pricing - More weekend-focused service investments - Employer-based subsidies for part-time commuting\nNew York‚Äôs economy is recovering ‚Äî but its commute revenue model is not. And remote work is the new reality driving that change."
  },
  {
    "objectID": "final_group_report.html#zip-code-mta-agency-recovery-trends",
    "href": "final_group_report.html#zip-code-mta-agency-recovery-trends",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "üóÇÔ∏è ZIP Code & MTA Agency Recovery Trends",
    "text": "üóÇÔ∏è ZIP Code & MTA Agency Recovery Trends\n\n\n\n\n\n\nüéØ Overview\nThe geography of remote work adoption has fundamentally reshaped the way New Yorkers use public transit ‚Äî not just by reducing ridership, but by altering which neighborhoods and transit agencies were hit hardest. In this section, we explore how the rise of work-from-home practices influenced subway and bus recovery patterns across NYC ZIP codes and MTA agencies between 2019 and 2023.\nUsing MTA monthly ridership data and ACS estimates of remote work by ZIP code, we examine whether areas with higher remote work rates experienced deeper transit declines ‚Äî and whether those patterns varied by mode of transportation. By analyzing both agency-level recovery and ZIP-level ridership shifts, we uncover critical differences in how subways, buses, and commuter rails responded to the long tail of the pandemic. This analysis helps pinpoint where transit has bounced back ‚Äî and where it‚Äôs still waiting for riders to return.\n\n\n\nüß® 1. Which Agencies Suffered Most?\n\nTransit Ridership Drop and Recovery by MTA Agency\n\nBiggest Drops: Metro-North Railroad (MNR, -66.6%) and SIR (-66.4%) experienced the most severe declines from Pre-COVID to Core-COVID.\nStrongest Recoveries: MNR (88.3%) and LIRR (80.2%) showed the most dramatic rebounds from Core to Post-COVID.\nSubways: Despite a 58.8% drop, subways rebounded 54.7% ‚Äî making them relatively stable compared to buses or commuter rails.\n\nüß† Insight: Recovery wasn‚Äôt uniform ‚Äî agencies with high weekday commuter dependency fell furthest and rebounded fastest once offices reopened, but weekday subway traffic remains structurally lower.\n\n\n\nüèôÔ∏è 2. Remote Work vs Agency Exposure by ZIP\n\nRemote Work Rates by ZIP and Transit Loss by Mode\n\nZIPs with 20‚Äì25% remote work rates (e.g., 10002, 10005) all showed ~32% ridership losses.\nThe 10003 ZIP code, with the highest remote work rate, showed the sharpest loss: -38.8% on NYCT Bus routes.\n\nüß† Insight: ZIP codes with more flexible job markets are heavily reliant on subways and NYCT Bus ‚Äî and saw larger drops in transit use when remote work expanded.\n\n\n\nüìà 3. Monthly MTA Ridership by Agency (2019‚Äì2023)\n\nLine chart showing ridership recovery over time\n\nSubway ridership saw the steepest initial crash in early 2020 but also rebounded steadily throughout 2021‚Äì2023.\nMTA Bus, NYCT Bus, and commuter rail modes plateaued early ‚Äî suggesting a permanent reset in their usage.\nNotably, the subway‚Äôs growth stabilized post-2022, indicating a new normal rather than continued bounce-back.\n\nüß† Insight: Subway is the only mode showing sustainable post-COVID growth, while other systems may require restructuring to match demand.\n\n\n\nüìâ 4. Remote Work vs Ridership Loss Correlation\n\nScatterplot with trend line: Remote Work % vs Transit Drop\n\nClear negative correlation: ZIPs with higher remote work rates saw deeper ridership declines.\nThis was especially pronounced in NYCT Bus usage, where flexible employment made daily commuting optional.\nHowever, not all high-remote ZIPs lost transit at the same pace ‚Äî suggesting that income level, transit reliability, and hybrid habits play additional roles.\n\n\n\n\nüß† Takeaway\nRemote work adoption did not impact all agencies equally. Subway ridership has shown the strongest bounce-back among MTA modes, but areas with the highest flexibility ‚Äî such as downtown Manhattan ZIPs ‚Äî continue to lag behind.\nAgencies reliant on daily commuters (like Metro-North and NYCT Bus) have been hardest hit and may never fully recover unless service and funding structures adapt to the post-pandemic commuter landscape.\nFuture planning will need to consider: - Targeted service scaling by ZIP recovery - Adaptive scheduling for non-commuter hours - Equity-driven investments in boroughs with low remote work and high recovery needs"
  },
  {
    "objectID": "final_group_report.html#geographic-recovery-of-nyc-subway-usage",
    "href": "final_group_report.html#geographic-recovery-of-nyc-subway-usage",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "üó∫Ô∏è Geographic Recovery of NYC Subway Usage",
    "text": "üó∫Ô∏è Geographic Recovery of NYC Subway Usage\n\n\n\n\n\n\nüéØ Overview\nThe COVID-19 pandemic didn‚Äôt just alter daily routines‚Äîit redefined the very fabric of urban mobility. As remote work became the norm, New York City‚Äôs subway system experienced unprecedented shifts in ridership patterns. This section delves into the geographic nuances of subway recovery from 2020 to 2023, examining how remote work adoption influenced transit usage across different ZIP codes and boroughs.\n\n\n\nüß≠ 1. Remote Work Adoption by ZIP Code (2020‚Äì2023)\n\n\n\nRemote Work Adoption Map\n\n\nInsights: - High Adoption Areas: ZIP codes in Manhattan, particularly Midtown and the Financial District, saw the most significant increases in remote work. - Low Adoption Areas: Outer boroughs like the Bronx and parts of Queens exhibited minimal changes, indicating a continued reliance on in-person occupations.\n\n\n\nüöá 2. Subway Ridership Recovery by ZIP Code\n\n\n\nSubway Ridership Recovery Map\n\n\nObservations: - Lagging Recovery: Areas with high remote work adoption, such as Midtown Manhattan, showed slower ridership recovery. - Robust Recovery: Neighborhoods with lower remote work rates, including parts of the Bronx and Queens, demonstrated a quicker return to pre-pandemic ridership levels.\n\n\n\nüìâ 3. Correlation Between Remote Work and Ridership Recovery\n\n\n\nCorrelation Scatter Plot\n\n\nAnalysis: - A clear negative correlation exists between the increase in remote work and subway ridership recovery. - ZIP codes with higher remote work adoption experienced more significant declines in subway usage, underscoring the impact of work-from-home trends on public transit.\n\n\n\nüèôÔ∏è 4. Borough-Level Recovery Patterns\n\n\n\nBorough Recovery Bar Chart\n\n\nKey Points: - Manhattan: Exhibited the slowest recovery, aligning with its high concentration of remote-capable jobs. - Bronx & Queens: Showed stronger recovery rates, reflecting the prevalence of essential and on-site occupations in these boroughs.\n\n\n\nüß† Takeaway\nThe geographic disparities in subway ridership recovery highlight the profound influence of remote work on urban transit dynamics. Areas with higher remote work adoption have not only altered commuting patterns but also reshaped the demand for public transportation. These insights are crucial for policymakers and transit authorities aiming to adapt services to the evolving needs of New Yorkers in a post-pandemic world."
  },
  {
    "objectID": "final_group_report.html#final-summary",
    "href": "final_group_report.html#final-summary",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "üß† Final Summary",
    "text": "üß† Final Summary\n\n\n\n\n\nüß≠ How has remote work influenced NYC subway dynamics since the COVID-19 pandemic?\nOur project set out to understand how the rise of remote work has reshaped New York City‚Äôs subway system in the years following the COVID-19 pandemic. Through a comprehensive analysis of ridership patterns, financial data, geographic trends, and transit agency usage from 2019 to 2023, we found that remote work has not simply reduced subway ridership ‚Äî it has transformed when, where, and why New Yorkers ride.\nOne of the most visible impacts has been the persistent weakness in weekday ridership. While weekend travel returned to roughly 77% of pre-pandemic levels by 2023, weekday ridership has stalled at around 67%. The iconic morning and evening rush hours have flattened, particularly on weekdays, as traditional 9-to-5 commuters shift to hybrid or remote schedules. Office-heavy districts like Midtown, the Financial District, and Grand Central have been hit hardest, with ZIP codes in these areas showing the largest and most persistent weekday ridership declines.\nThese behavioral shifts have had serious financial consequences. We estimate that between 2020 and 2023, the MTA lost approximately $8.8 billion in subway fare revenue. Remote work is directly responsible for at least $2.5 to $3.0 billion of that loss. Even in 2023, when tourism and weekend travel had largely recovered, the absence of hundreds of thousands of daily commuters contributed to an estimated $379.8 million in lost fare revenue. The MTA‚Äôs longstanding dependence on weekday riders to fund operations has been fundamentally challenged by this shift.\nGeographically, the recovery has been uneven across ZIP codes and boroughs. Neighborhoods with the highest remote work adoption ‚Äî particularly in Manhattan ‚Äî have lagged in ridership recovery. In contrast, areas in the Bronx, Queens, and parts of Brooklyn, where fewer jobs can be done remotely, saw stronger rebounds. These areas remained transit-dependent throughout the pandemic and returned to the system more quickly once restrictions eased. This geographic divide underscores how remote work has exacerbated existing transit inequalities, concentrating ridership losses in wealthier, more flexible parts of the city while placing more demand on services in outer boroughs.\nAt the transit agency level, recovery patterns also diverged. Subways, while hit hard during the peak of the pandemic, have shown steady year-over-year recovery. Commuter rail services like Metro-North and LIRR experienced steep drops but rebounded quickly as hybrid commuters returned. Buses, especially those serving Manhattan ZIP codes with high WFH rates, have remained below 2019 levels, highlighting mode-specific vulnerabilities tied to remote work patterns.\nTaken together, our findings show that remote work has fundamentally altered the MTA‚Äôs ridership base. The system no longer revolves around a five-day commute into Manhattan ‚Äî it now serves a more dispersed, flexible, and uneven rider population. Understanding these shifts is essential for rethinking transit service, fare structures, and capital investments in a post-pandemic, hybrid-working world.\nRemote work hasn‚Äôt just changed how people work ‚Äî it‚Äôs changed how New York moves."
  },
  {
    "objectID": "final_group_report.html#future-directions",
    "href": "final_group_report.html#future-directions",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "üö¶ Future Directions",
    "text": "üö¶ Future Directions\n\n\n\n\n\nWhile our project sheds light on the dramatic and lasting impact of remote work on NYC subway dynamics, several questions remain open ‚Äî and new ones are emerging.\nFirst, our analysis focused on aggregated trends through 2023, but future research could investigate 2024 and beyond, especially as return-to-office policies fluctuate across industries. Understanding whether weekday ridership will plateau, rebound, or decline further is key to long-term transit forecasting.\nSecond, our ZIP-level and agency-level insights could be expanded with individual station data, capturing hyperlocal effects of demographic shifts, real estate trends, or rezoning. Pairing this with CitiBike, Uber, or bus route data could reveal modal shifts among remote workers.\nThird, we estimated fare revenue losses using a static per-ride model. Future work could incorporate fare elasticity models, policy changes like congestion pricing, and evolving ridership behavior (e.g., unlimited passes vs.¬†pay-per-ride).\nLastly, as New York continues adapting to hybrid life, future research should examine equity implications ‚Äî which neighborhoods gain or lose service under these new patterns, and how funding models can respond to both economic and social needs.\nThe MTA‚Äôs challenges are no longer just operational ‚Äî they are cultural and structural. Understanding the long-term footprint of remote work is essential to building a subway system that works for the future of New York."
  },
  {
    "objectID": "final_group_report.html#in-depth-analysis",
    "href": "final_group_report.html#in-depth-analysis",
    "title": "Subway Metrics: The Impact of Remote Work on NYC Transit",
    "section": "üìé In-Depth Analysis",
    "text": "üìé In-Depth Analysis\n\n\n\n\n\n\nDhruv Sharma ‚Äì COVID Era Ridership Trends\n\nVihan Raghuvanshi ‚Äì Fare Revenue Loss Analysis\n\nDev ‚Äì ZIP & Agency Transit Recovery Patterns\n\nShubh Goyal ‚Äì Geographic Recovery Across Boroughs"
  }
]